{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "global-nudist"
      },
      "source": [
        "# Linear Regression Model\n",
        "### Load data and test train split"
      ],
      "id": "global-nudist"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKOZC6Gyp6HQ",
        "outputId": "dc56bf30-129b-40a1-d48a-74e52199538a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "jKOZC6Gyp6HQ"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "distant-found"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.get_option(\"display.max_columns\")\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_csv('/content/drive/My Drive/4ML3/Project/train.csv')\n",
        "dataset_val = dataset.values\n",
        "\n",
        "x = dataset_val[:,0:(dataset_val.shape[1])-1]\n",
        "y = dataset_val[:,-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=69420)"
      ],
      "id": "distant-found"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "answering-device"
      },
      "source": [
        "### Normalize Data"
      ],
      "id": "answering-device"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "measured-planning"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_x = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "scaler_x.fit(X_train)\n",
        "scaler_y.fit(y_train)\n",
        "\n",
        "X_train_stand = scaler_x.transform(X_train)\n",
        "X_test_stand = scaler_x.transform(X_test)\n",
        "y_train_stand = scaler_y.transform(y_train)\n",
        "y_test_stand = scaler_y.transform(y_test)\n",
        "\n",
        "# method to inverse-normalize the data back to critical temperatures in Kelvin\n",
        "def inverse_normalize(value, y_or_x):\n",
        "    new_val = np.array(value).reshape(-1,1)\n",
        "    \n",
        "    if y_or_x == \"x\":\n",
        "        scaler = scaler_x\n",
        "    else:\n",
        "        scaler = scaler_y\n",
        "    new_val = scaler.inverse_transform(new_val)\n",
        "    \n",
        "    return new_val"
      ],
      "id": "measured-planning"
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_normalize(0.125, \"y\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzQSXmsSorq-",
        "outputId": "c11b46e5-bb88-4c06-d304-ae004f148b0f"
      },
      "id": "nzQSXmsSorq-",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[38.75327462]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "internal-tonight"
      },
      "source": [
        "### Apply Linear Regression"
      ],
      "id": "internal-tonight"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dress-emission",
        "outputId": "10eb8d50-1c1b-494b-bd6a-67424cb53254"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Error: [[43.39874336]]\n",
            "Testing Error: [[43.62880409]]\n",
            "LR Intercept: [-1.47857841e-13]\n",
            "time: 106 ms (started: 2021-12-07 19:09:57 +00:00)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train_stand, y_train_stand)\n",
        "\n",
        "# compute errors\n",
        "y_train_pred = lin_reg.predict(X_train_stand)\n",
        "y_test_pred = lin_reg.predict(X_test_stand)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "training_error = np.array(MSE(y_train_stand,y_train_pred)).reshape(-1,1)\n",
        "testing_error = np.array(MSE(y_test_stand,y_test_pred)).reshape(-1,1)\n",
        "print(\"Training Error: {}\".format(scaler_y.inverse_transform(training_error)))\n",
        "print(\"Testing Error: {}\".format(scaler_y.inverse_transform(testing_error)))\n",
        "\n",
        "# Show the weights\n",
        "print(\"LR Intercept: {}\".format(lin_reg.intercept_))\n",
        "#print(\"LR Model Weights: {}\".format(lin_reg.coef_))\n",
        "col_labels = columns=list(dataset.columns)[:-1]\n",
        "df = pd.DataFrame(data=lin_reg.coef_, columns=col_labels)"
      ],
      "id": "dress-emission"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "violent-destination",
        "outputId": "b93f8182-7355-4ca1-d92f-1b787ddb05a3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>number_of_elements</th>\n",
              "      <th>mean_atomic_mass</th>\n",
              "      <th>wtd_mean_atomic_mass</th>\n",
              "      <th>gmean_atomic_mass</th>\n",
              "      <th>wtd_gmean_atomic_mass</th>\n",
              "      <th>entropy_atomic_mass</th>\n",
              "      <th>wtd_entropy_atomic_mass</th>\n",
              "      <th>range_atomic_mass</th>\n",
              "      <th>wtd_range_atomic_mass</th>\n",
              "      <th>std_atomic_mass</th>\n",
              "      <th>wtd_std_atomic_mass</th>\n",
              "      <th>mean_fie</th>\n",
              "      <th>wtd_mean_fie</th>\n",
              "      <th>gmean_fie</th>\n",
              "      <th>wtd_gmean_fie</th>\n",
              "      <th>entropy_fie</th>\n",
              "      <th>wtd_entropy_fie</th>\n",
              "      <th>range_fie</th>\n",
              "      <th>wtd_range_fie</th>\n",
              "      <th>std_fie</th>\n",
              "      <th>wtd_std_fie</th>\n",
              "      <th>mean_atomic_radius</th>\n",
              "      <th>wtd_mean_atomic_radius</th>\n",
              "      <th>gmean_atomic_radius</th>\n",
              "      <th>wtd_gmean_atomic_radius</th>\n",
              "      <th>entropy_atomic_radius</th>\n",
              "      <th>wtd_entropy_atomic_radius</th>\n",
              "      <th>range_atomic_radius</th>\n",
              "      <th>wtd_range_atomic_radius</th>\n",
              "      <th>std_atomic_radius</th>\n",
              "      <th>wtd_std_atomic_radius</th>\n",
              "      <th>mean_Density</th>\n",
              "      <th>wtd_mean_Density</th>\n",
              "      <th>gmean_Density</th>\n",
              "      <th>wtd_gmean_Density</th>\n",
              "      <th>entropy_Density</th>\n",
              "      <th>wtd_entropy_Density</th>\n",
              "      <th>range_Density</th>\n",
              "      <th>wtd_range_Density</th>\n",
              "      <th>std_Density</th>\n",
              "      <th>...</th>\n",
              "      <th>mean_ElectronAffinity</th>\n",
              "      <th>wtd_mean_ElectronAffinity</th>\n",
              "      <th>gmean_ElectronAffinity</th>\n",
              "      <th>wtd_gmean_ElectronAffinity</th>\n",
              "      <th>entropy_ElectronAffinity</th>\n",
              "      <th>wtd_entropy_ElectronAffinity</th>\n",
              "      <th>range_ElectronAffinity</th>\n",
              "      <th>wtd_range_ElectronAffinity</th>\n",
              "      <th>std_ElectronAffinity</th>\n",
              "      <th>wtd_std_ElectronAffinity</th>\n",
              "      <th>mean_FusionHeat</th>\n",
              "      <th>wtd_mean_FusionHeat</th>\n",
              "      <th>gmean_FusionHeat</th>\n",
              "      <th>wtd_gmean_FusionHeat</th>\n",
              "      <th>entropy_FusionHeat</th>\n",
              "      <th>wtd_entropy_FusionHeat</th>\n",
              "      <th>range_FusionHeat</th>\n",
              "      <th>wtd_range_FusionHeat</th>\n",
              "      <th>std_FusionHeat</th>\n",
              "      <th>wtd_std_FusionHeat</th>\n",
              "      <th>mean_ThermalConductivity</th>\n",
              "      <th>wtd_mean_ThermalConductivity</th>\n",
              "      <th>gmean_ThermalConductivity</th>\n",
              "      <th>wtd_gmean_ThermalConductivity</th>\n",
              "      <th>entropy_ThermalConductivity</th>\n",
              "      <th>wtd_entropy_ThermalConductivity</th>\n",
              "      <th>range_ThermalConductivity</th>\n",
              "      <th>wtd_range_ThermalConductivity</th>\n",
              "      <th>std_ThermalConductivity</th>\n",
              "      <th>wtd_std_ThermalConductivity</th>\n",
              "      <th>mean_Valence</th>\n",
              "      <th>wtd_mean_Valence</th>\n",
              "      <th>gmean_Valence</th>\n",
              "      <th>wtd_gmean_Valence</th>\n",
              "      <th>entropy_Valence</th>\n",
              "      <th>wtd_entropy_Valence</th>\n",
              "      <th>range_Valence</th>\n",
              "      <th>wtd_range_Valence</th>\n",
              "      <th>std_Valence</th>\n",
              "      <th>wtd_std_Valence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.112698</td>\n",
              "      <td>0.739433</td>\n",
              "      <td>-0.852727</td>\n",
              "      <td>-0.457711</td>\n",
              "      <td>0.654996</td>\n",
              "      <td>-0.387973</td>\n",
              "      <td>0.03124</td>\n",
              "      <td>0.3345</td>\n",
              "      <td>0.021192</td>\n",
              "      <td>-0.317538</td>\n",
              "      <td>0.040885</td>\n",
              "      <td>0.422258</td>\n",
              "      <td>-1.008366</td>\n",
              "      <td>-0.370683</td>\n",
              "      <td>0.928834</td>\n",
              "      <td>-1.476906</td>\n",
              "      <td>0.461826</td>\n",
              "      <td>0.655031</td>\n",
              "      <td>0.155118</td>\n",
              "      <td>-0.70293</td>\n",
              "      <td>-0.009304</td>\n",
              "      <td>-0.250073</td>\n",
              "      <td>2.798798</td>\n",
              "      <td>0.049219</td>\n",
              "      <td>-3.051188</td>\n",
              "      <td>0.925677</td>\n",
              "      <td>0.543201</td>\n",
              "      <td>0.32491</td>\n",
              "      <td>-0.090564</td>\n",
              "      <td>-0.184098</td>\n",
              "      <td>-0.283415</td>\n",
              "      <td>-0.404009</td>\n",
              "      <td>-0.061742</td>\n",
              "      <td>0.099986</td>\n",
              "      <td>0.359415</td>\n",
              "      <td>0.158349</td>\n",
              "      <td>-0.163694</td>\n",
              "      <td>-0.179728</td>\n",
              "      <td>-0.010257</td>\n",
              "      <td>0.261514</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.129693</td>\n",
              "      <td>0.556396</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>-0.606301</td>\n",
              "      <td>0.021876</td>\n",
              "      <td>-0.178381</td>\n",
              "      <td>-0.601911</td>\n",
              "      <td>-0.119497</td>\n",
              "      <td>0.791908</td>\n",
              "      <td>-0.360805</td>\n",
              "      <td>0.529067</td>\n",
              "      <td>-0.751941</td>\n",
              "      <td>-0.426537</td>\n",
              "      <td>0.574279</td>\n",
              "      <td>-0.192746</td>\n",
              "      <td>0.257857</td>\n",
              "      <td>-0.226834</td>\n",
              "      <td>0.189451</td>\n",
              "      <td>-0.11944</td>\n",
              "      <td>0.145218</td>\n",
              "      <td>-0.059941</td>\n",
              "      <td>0.68421</td>\n",
              "      <td>-0.07331</td>\n",
              "      <td>-0.374954</td>\n",
              "      <td>0.116419</td>\n",
              "      <td>0.011595</td>\n",
              "      <td>-0.419373</td>\n",
              "      <td>-0.283312</td>\n",
              "      <td>0.503461</td>\n",
              "      <td>-0.01029</td>\n",
              "      <td>-0.384191</td>\n",
              "      <td>0.722082</td>\n",
              "      <td>0.53669</td>\n",
              "      <td>-0.863814</td>\n",
              "      <td>0.943916</td>\n",
              "      <td>-0.814803</td>\n",
              "      <td>0.139676</td>\n",
              "      <td>-0.026485</td>\n",
              "      <td>0.116761</td>\n",
              "      <td>-0.317241</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   number_of_elements  mean_atomic_mass  ...  std_Valence  wtd_std_Valence\n",
              "0           -0.112698          0.739433  ...     0.116761        -0.317241\n",
              "\n",
              "[1 rows x 81 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df # Linear regression coefficients for each feature"
      ],
      "id": "violent-destination"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tFs_uouTYuV"
      },
      "source": [
        "### Test Time Taken\n"
      ],
      "id": "1tFs_uouTYuV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "vkxCgRzlTcGO",
        "outputId": "7746852b-a063-47bf-c9b6-f0a2f3a9ceba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autotime extension is already loaded. To reload it, use:\n",
            "  %reload_ext autotime\n",
            "Training Error: [[43.39874336]]\n",
            "Testing Error: [[43.62880409]]\n",
            "LR Intercept: [-1.47857841e-13]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>number_of_elements</th>\n",
              "      <th>mean_atomic_mass</th>\n",
              "      <th>wtd_mean_atomic_mass</th>\n",
              "      <th>gmean_atomic_mass</th>\n",
              "      <th>wtd_gmean_atomic_mass</th>\n",
              "      <th>entropy_atomic_mass</th>\n",
              "      <th>wtd_entropy_atomic_mass</th>\n",
              "      <th>range_atomic_mass</th>\n",
              "      <th>wtd_range_atomic_mass</th>\n",
              "      <th>std_atomic_mass</th>\n",
              "      <th>wtd_std_atomic_mass</th>\n",
              "      <th>mean_fie</th>\n",
              "      <th>wtd_mean_fie</th>\n",
              "      <th>gmean_fie</th>\n",
              "      <th>wtd_gmean_fie</th>\n",
              "      <th>entropy_fie</th>\n",
              "      <th>wtd_entropy_fie</th>\n",
              "      <th>range_fie</th>\n",
              "      <th>wtd_range_fie</th>\n",
              "      <th>std_fie</th>\n",
              "      <th>wtd_std_fie</th>\n",
              "      <th>mean_atomic_radius</th>\n",
              "      <th>wtd_mean_atomic_radius</th>\n",
              "      <th>gmean_atomic_radius</th>\n",
              "      <th>wtd_gmean_atomic_radius</th>\n",
              "      <th>entropy_atomic_radius</th>\n",
              "      <th>wtd_entropy_atomic_radius</th>\n",
              "      <th>range_atomic_radius</th>\n",
              "      <th>wtd_range_atomic_radius</th>\n",
              "      <th>std_atomic_radius</th>\n",
              "      <th>wtd_std_atomic_radius</th>\n",
              "      <th>mean_Density</th>\n",
              "      <th>wtd_mean_Density</th>\n",
              "      <th>gmean_Density</th>\n",
              "      <th>wtd_gmean_Density</th>\n",
              "      <th>entropy_Density</th>\n",
              "      <th>wtd_entropy_Density</th>\n",
              "      <th>range_Density</th>\n",
              "      <th>wtd_range_Density</th>\n",
              "      <th>std_Density</th>\n",
              "      <th>...</th>\n",
              "      <th>mean_ElectronAffinity</th>\n",
              "      <th>wtd_mean_ElectronAffinity</th>\n",
              "      <th>gmean_ElectronAffinity</th>\n",
              "      <th>wtd_gmean_ElectronAffinity</th>\n",
              "      <th>entropy_ElectronAffinity</th>\n",
              "      <th>wtd_entropy_ElectronAffinity</th>\n",
              "      <th>range_ElectronAffinity</th>\n",
              "      <th>wtd_range_ElectronAffinity</th>\n",
              "      <th>std_ElectronAffinity</th>\n",
              "      <th>wtd_std_ElectronAffinity</th>\n",
              "      <th>mean_FusionHeat</th>\n",
              "      <th>wtd_mean_FusionHeat</th>\n",
              "      <th>gmean_FusionHeat</th>\n",
              "      <th>wtd_gmean_FusionHeat</th>\n",
              "      <th>entropy_FusionHeat</th>\n",
              "      <th>wtd_entropy_FusionHeat</th>\n",
              "      <th>range_FusionHeat</th>\n",
              "      <th>wtd_range_FusionHeat</th>\n",
              "      <th>std_FusionHeat</th>\n",
              "      <th>wtd_std_FusionHeat</th>\n",
              "      <th>mean_ThermalConductivity</th>\n",
              "      <th>wtd_mean_ThermalConductivity</th>\n",
              "      <th>gmean_ThermalConductivity</th>\n",
              "      <th>wtd_gmean_ThermalConductivity</th>\n",
              "      <th>entropy_ThermalConductivity</th>\n",
              "      <th>wtd_entropy_ThermalConductivity</th>\n",
              "      <th>range_ThermalConductivity</th>\n",
              "      <th>wtd_range_ThermalConductivity</th>\n",
              "      <th>std_ThermalConductivity</th>\n",
              "      <th>wtd_std_ThermalConductivity</th>\n",
              "      <th>mean_Valence</th>\n",
              "      <th>wtd_mean_Valence</th>\n",
              "      <th>gmean_Valence</th>\n",
              "      <th>wtd_gmean_Valence</th>\n",
              "      <th>entropy_Valence</th>\n",
              "      <th>wtd_entropy_Valence</th>\n",
              "      <th>range_Valence</th>\n",
              "      <th>wtd_range_Valence</th>\n",
              "      <th>std_Valence</th>\n",
              "      <th>wtd_std_Valence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.112698</td>\n",
              "      <td>0.739433</td>\n",
              "      <td>-0.852727</td>\n",
              "      <td>-0.457711</td>\n",
              "      <td>0.654996</td>\n",
              "      <td>-0.387973</td>\n",
              "      <td>0.03124</td>\n",
              "      <td>0.3345</td>\n",
              "      <td>0.021192</td>\n",
              "      <td>-0.317538</td>\n",
              "      <td>0.040885</td>\n",
              "      <td>0.422258</td>\n",
              "      <td>-1.008366</td>\n",
              "      <td>-0.370683</td>\n",
              "      <td>0.928834</td>\n",
              "      <td>-1.476906</td>\n",
              "      <td>0.461826</td>\n",
              "      <td>0.655031</td>\n",
              "      <td>0.155118</td>\n",
              "      <td>-0.70293</td>\n",
              "      <td>-0.009304</td>\n",
              "      <td>-0.250073</td>\n",
              "      <td>2.798798</td>\n",
              "      <td>0.049219</td>\n",
              "      <td>-3.051188</td>\n",
              "      <td>0.925677</td>\n",
              "      <td>0.543201</td>\n",
              "      <td>0.32491</td>\n",
              "      <td>-0.090564</td>\n",
              "      <td>-0.184098</td>\n",
              "      <td>-0.283415</td>\n",
              "      <td>-0.404009</td>\n",
              "      <td>-0.061742</td>\n",
              "      <td>0.099986</td>\n",
              "      <td>0.359415</td>\n",
              "      <td>0.158349</td>\n",
              "      <td>-0.163694</td>\n",
              "      <td>-0.179728</td>\n",
              "      <td>-0.010257</td>\n",
              "      <td>0.261514</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.129693</td>\n",
              "      <td>0.556396</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>-0.606301</td>\n",
              "      <td>0.021876</td>\n",
              "      <td>-0.178381</td>\n",
              "      <td>-0.601911</td>\n",
              "      <td>-0.119497</td>\n",
              "      <td>0.791908</td>\n",
              "      <td>-0.360805</td>\n",
              "      <td>0.529067</td>\n",
              "      <td>-0.751941</td>\n",
              "      <td>-0.426537</td>\n",
              "      <td>0.574279</td>\n",
              "      <td>-0.192746</td>\n",
              "      <td>0.257857</td>\n",
              "      <td>-0.226834</td>\n",
              "      <td>0.189451</td>\n",
              "      <td>-0.11944</td>\n",
              "      <td>0.145218</td>\n",
              "      <td>-0.059941</td>\n",
              "      <td>0.68421</td>\n",
              "      <td>-0.07331</td>\n",
              "      <td>-0.374954</td>\n",
              "      <td>0.116419</td>\n",
              "      <td>0.011595</td>\n",
              "      <td>-0.419373</td>\n",
              "      <td>-0.283312</td>\n",
              "      <td>0.503461</td>\n",
              "      <td>-0.01029</td>\n",
              "      <td>-0.384191</td>\n",
              "      <td>0.722082</td>\n",
              "      <td>0.53669</td>\n",
              "      <td>-0.863814</td>\n",
              "      <td>0.943916</td>\n",
              "      <td>-0.814803</td>\n",
              "      <td>0.139676</td>\n",
              "      <td>-0.026485</td>\n",
              "      <td>0.116761</td>\n",
              "      <td>-0.317241</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   number_of_elements  mean_atomic_mass  ...  std_Valence  wtd_std_Valence\n",
              "0           -0.112698          0.739433  ...     0.116761        -0.317241\n",
              "\n",
              "[1 rows x 81 columns]"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 718 ms (started: 2021-12-07 19:11:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "%load_ext autotime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.get_option(\"display.max_columns\")\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_csv('/content/drive/My Drive/4ML3/Project/train.csv')\n",
        "dataset_val = dataset.values\n",
        "\n",
        "x = dataset_val[:,0:(dataset_val.shape[1])-1]\n",
        "y = dataset_val[:,-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=69420)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_x = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "scaler_x.fit(X_train)\n",
        "scaler_y.fit(y_train)\n",
        "\n",
        "X_train_stand = scaler_x.transform(X_train)\n",
        "X_test_stand = scaler_x.transform(X_test)\n",
        "y_train_stand = scaler_y.transform(y_train)\n",
        "y_test_stand = scaler_y.transform(y_test)\n",
        "\n",
        "# method to un-normalize the data back to critical temperatures in Kelvin\n",
        "def inverse_normalize(value, y_or_x):\n",
        "    new_val = np.array(value).reshape(-1,1)\n",
        "    \n",
        "    if y_or_x == \"x\":\n",
        "        scaler = scaler_x\n",
        "    else:\n",
        "        scaler = scaler_y\n",
        "    new_val = scaler.inverse_transform(new_val)\n",
        "    \n",
        "    return new_val\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train_stand, y_train_stand)\n",
        "\n",
        "# compute errors\n",
        "y_train_pred = lin_reg.predict(X_train_stand)\n",
        "y_test_pred = lin_reg.predict(X_test_stand)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "training_error = np.array(MSE(y_train_stand,y_train_pred)).reshape(-1,1)\n",
        "testing_error = np.array(MSE(y_test_stand,y_test_pred)).reshape(-1,1)\n",
        "print(\"Training Error: {}\".format(scaler_y.inverse_transform(training_error)))\n",
        "print(\"Testing Error: {}\".format(scaler_y.inverse_transform(testing_error)))\n",
        "\n",
        "# Show the weights\n",
        "print(\"LR Intercept: {}\".format(lin_reg.intercept_))\n",
        "#print(\"LR Model Weights: {}\".format(lin_reg.coef_))\n",
        "col_labels = columns=list(dataset.columns)[:-1]\n",
        "df = pd.DataFrame(data=lin_reg.coef_, columns=col_labels)\n",
        "\n",
        "df"
      ],
      "id": "vkxCgRzlTcGO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZqCAxOxVAnk"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "4ZqCAxOxVAnk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xp-jofIUn8G"
      },
      "source": [
        "## L2 Regularization varying Penalties"
      ],
      "id": "0Xp-jofIUn8G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgLtAXjlUqIp",
        "outputId": "30119d1d-74cb-4817-8132-c5e6d5c95719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Penalty: 0.005\n",
            "Current C_vals: 100.0\n",
            "Training Error: [[43.39874476]]\n",
            "Testing Error: [[43.62873436]]\n",
            "\n",
            "Current Penalty: 0.1\n",
            "Current C_vals: 5.0\n",
            "Training Error: [[43.39920069]]\n",
            "Testing Error: [[43.62802383]]\n",
            "\n",
            "Current Penalty: 1\n",
            "Current C_vals: 0.5\n",
            "Training Error: [[43.41270283]]\n",
            "Testing Error: [[43.63674645]]\n",
            "\n",
            "Current Penalty: 5\n",
            "Current C_vals: 0.1\n",
            "Training Error: [[43.45354381]]\n",
            "Testing Error: [[43.66766257]]\n",
            "\n",
            "Current Penalty: 10\n",
            "Current C_vals: 0.05\n",
            "Training Error: [[43.48888924]]\n",
            "Testing Error: [[43.69332737]]\n",
            "\n",
            "Current Penalty: 20\n",
            "Current C_vals: 0.025\n",
            "Training Error: [[43.55201153]]\n",
            "Testing Error: [[43.74363448]]\n",
            "\n",
            "Current Penalty: 40\n",
            "Current C_vals: 0.0125\n",
            "Training Error: [[43.66168707]]\n",
            "Testing Error: [[43.84142151]]\n",
            "\n",
            "Current Penalty: 60\n",
            "Current C_vals: 0.008333333333333333\n",
            "Training Error: [[43.75232721]]\n",
            "Testing Error: [[43.9277828]]\n",
            "\n",
            "Current Penalty: 100\n",
            "Current C_vals: 0.005\n",
            "Training Error: [[43.89359717]]\n",
            "Testing Error: [[44.06751075]]\n",
            "\n",
            "Current Penalty: 200\n",
            "Current C_vals: 0.0025\n",
            "Training Error: [[44.12405442]]\n",
            "Testing Error: [[44.30029128]]\n",
            "\n",
            "Current Penalty: 400\n",
            "Current C_vals: 0.00125\n",
            "Training Error: [[44.38899906]]\n",
            "Testing Error: [[44.56523846]]\n",
            "\n",
            "Current Penalty: 800\n",
            "Current C_vals: 0.000625\n",
            "Training Error: [[44.70330454]]\n",
            "Testing Error: [[44.87059038]]\n",
            "\n",
            "Current Penalty: 1600\n",
            "Current C_vals: 0.0003125\n",
            "Training Error: [[45.12025502]]\n",
            "Testing Error: [[45.27006706]]\n",
            "\n",
            "Current Penalty: 2000\n",
            "Current C_vals: 0.00025\n",
            "Training Error: [[45.28712822]]\n",
            "Testing Error: [[45.43041485]]\n",
            "\n",
            "Current Penalty: 2500\n",
            "Current C_vals: 0.0002\n",
            "Training Error: [[45.47221122]]\n",
            "Testing Error: [[45.60886454]]\n",
            "\n",
            "Current Penalty: 3000\n",
            "Current C_vals: 0.00016666666666666666\n",
            "Training Error: [[45.63695112]]\n",
            "Testing Error: [[45.76818155]]\n",
            "\n",
            "Current Penalty: 4000\n",
            "Current C_vals: 0.000125\n",
            "Training Error: [[45.92006119]]\n",
            "Testing Error: [[46.04276333]]\n",
            "\n",
            "Current Penalty: 5000\n",
            "Current C_vals: 0.0001\n",
            "Training Error: [[46.15678731]]\n",
            "Testing Error: [[46.27282921]]\n",
            "\n",
            "Current Penalty: 6000\n",
            "Current C_vals: 8.333333333333333e-05\n",
            "Training Error: [[46.35935067]]\n",
            "Testing Error: [[46.46983025]]\n",
            "\n",
            "Current Penalty: 7000\n",
            "Current C_vals: 7.142857142857143e-05\n",
            "Training Error: [[46.53588491]]\n",
            "Testing Error: [[46.64151089]]\n",
            "\n",
            "Training Error: [[46.53588491]]\n",
            "Testing Error: [[46.64151089]]\n",
            "time: 978 ms (started: 2021-12-07 19:43:44 +00:00)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Ridge # uses L2 regularization by default with C = 1 (C = 1/alpha)\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "penalties = [0.005,0.1, 1, 5, 10, 20, 40, 60, 100, 200, 400, 800, 1600, 2000, 2500, 3000, 4000, 5000, 6000, 7000]\n",
        "C_vals = []\n",
        "for penalty in penalties:\n",
        "  C_vals.append(1/(2*penalty))\n",
        "\n",
        "training_errors = []\n",
        "testing_errors = []\n",
        "\n",
        "for i in range(len(penalties)):\n",
        "\n",
        "  # define model\n",
        "  print(\"Current Penalty: {}\".format(penalties[i]))\n",
        "  print(\"Current C_vals: {}\".format(C_vals[i]))\n",
        "\n",
        "  model = Ridge(alpha=penalties[i])\n",
        "  model.fit(X_train_stand,y_train_stand)\n",
        "\n",
        "  # compute errors\n",
        "  y_train_pred = model.predict(X_train_stand)\n",
        "  y_test_pred = model.predict(X_test_stand)\n",
        "\n",
        "  # Compute traditional MSE error\n",
        "  training_error = inverse_normalize(MSE(y_train_stand,y_train_pred), \"y\")\n",
        "  testing_error = inverse_normalize(MSE(y_test_stand,y_test_pred), \"y\")\n",
        "  print(\"Training Error: {}\".format(training_error))\n",
        "  print(\"Testing Error: {}\\n\".format(testing_error))\n",
        "\n",
        "  #append scores and store\n",
        "  training_errors.append(training_error)\n",
        "  testing_errors.append(testing_error)\n",
        "\n",
        "print(\"Training Error: {}\".format(training_error))\n",
        "print(\"Testing Error: {}\".format(testing_error))\n",
        "\n"
      ],
      "id": "vgLtAXjlUqIp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs9UQ6ZGab7i",
        "outputId": "6a9b1731-8b65-4d46-dbf9-42585a12e345"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The optimal C value: 5.0\n",
            "Testing Error: [43.62802383]\n",
            "time: 3.81 ms (started: 2021-12-07 19:46:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "print(\"The optimal C value: {}\\nTesting Error: {}\".format(C_vals[list(testing_errors).index(min(testing_errors))], testing_errors[list(testing_errors).index(min(testing_errors))]))"
      ],
      "id": "Gs9UQ6ZGab7i"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cpP6kt3Zxkh"
      },
      "source": [
        "### Plots"
      ],
      "id": "-cpP6kt3Zxkh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "UimtP6mCWTBB",
        "outputId": "460b6f11-a9e3-41bc-e996-98c46f53331e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 204 ms (started: 2021-12-07 19:44:27 +00:00)\n"
          ]
        }
      ],
      "source": [
        "testing_errors = (np.array(testing_errors)).reshape(-1,1)\n",
        "training_errors = (np.array(training_errors)).reshape(-1,1)\n",
        "plt.plot(C_vals, testing_errors)\n",
        "plt.plot(C_vals, training_errors)\n",
        "plt.title(\"Training and Testing MSE vs L2 Penalties\")\n",
        "plt.xlabel(\"L2 C Hyperparameter\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.legend([\"train\", \"test\"])\n",
        "plt.show()"
      ],
      "id": "UimtP6mCWTBB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "FQVpYQAiZU_S",
        "outputId": "7ef0e7ec-1553-4954-a586-0297ec4ca431"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 218 ms (started: 2021-12-07 19:44:36 +00:00)\n"
          ]
        }
      ],
      "source": [
        "testing_errors = (np.array(testing_errors)).reshape(-1,1)\n",
        "training_errors = (np.array(training_errors)).reshape(-1,1)\n",
        "plt.plot(penalties, testing_errors)\n",
        "plt.plot(penalties, training_errors)\n",
        "plt.title(\"Training and Testing MSE vs L2 Penalties\")\n",
        "plt.xlabel(\"L2 Penalties\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.legend([\"train\", \"test\"])\n",
        "plt.show()"
      ],
      "id": "FQVpYQAiZU_S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98LmSz8IZG5l",
        "outputId": "83d80545-bd4a-445e-a81b-eb7d919d2c0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[5000.0,\n",
              " 500.0,\n",
              " 50.0,\n",
              " 5.0,\n",
              " 0.5,\n",
              " 0.1,\n",
              " 0.05,\n",
              " 0.025,\n",
              " 0.0125,\n",
              " 0.008333333333333333,\n",
              " 0.005,\n",
              " 0.0025,\n",
              " 0.00125,\n",
              " 0.000625,\n",
              " 0.0003125,\n",
              " 0.00025,\n",
              " 0.0002,\n",
              " 0.00016666666666666666,\n",
              " 0.000125,\n",
              " 0.0001,\n",
              " 8.333333333333333e-05,\n",
              " 7.142857142857143e-05]"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 3.46 ms (started: 2021-12-07 19:35:05 +00:00)\n"
          ]
        }
      ],
      "source": [
        "C_vals"
      ],
      "id": "98LmSz8IZG5l"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "detailed-arabic"
      },
      "source": [
        "# Polynomial Regression"
      ],
      "id": "detailed-arabic"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-YuSrJowEl-"
      },
      "source": [
        "##Testing Amount of Data"
      ],
      "id": "1-YuSrJowEl-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TScmzvFcxeyB"
      },
      "source": [
        "####Power 2 - Pipeline"
      ],
      "id": "TScmzvFcxeyB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-ELnMoLtDZv",
        "outputId": "c6b6de68-ea43-4329-cdac-4cee38ed78a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Size: 10\n",
            "Training Error: [[26.14285714]]\n",
            "Testing Error: [[38.99599297]]\n",
            "Sample Size: 100\n",
            "Training Error: [[57.79279963]]\n",
            "Testing Error: [[6.82395997e+24]]\n",
            "Sample Size: 10000\n",
            "Training Error: [[59.93417609]]\n",
            "Testing Error: [[1.69978986e+14]]\n",
            "Sample Size: 20000\n",
            "Training Error: [[38.62382991]]\n",
            "Testing Error: [[55.50263848]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# method to un-normalize the data back to critical temperatures in Kelvin\n",
        "def inverse_normalize(value, y_or_x):\n",
        "    new_val = np.array(value).reshape(-1,1)\n",
        "    \n",
        "    if y_or_x == \"x\":\n",
        "        scaler = scaler_x\n",
        "    else:\n",
        "        scaler = scaler_y\n",
        "    new_val = scaler.inverse_transform(new_val)\n",
        "    \n",
        "    return new_val\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "sample_size = [10, 100, 500, 1000, 5000, 10000, 12000, 14000, 18000, 20000]\n",
        "\n",
        "p2_training_error = []\n",
        "p2_testing_error = []\n",
        "\n",
        "for i in range(len(sample_size)):\n",
        "\n",
        "  print(\"Sample Size: {}\".format(sample_size[i]))\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  pd.get_option(\"display.max_columns\")\n",
        "  import matplotlib.pyplot as plt\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  dataset = pd.read_csv('/content/drive/My Drive/4ML3/Project/train.csv')\n",
        "  dataset_val = dataset.values\n",
        "\n",
        "  x = dataset_val[0:sample_size[i],0:(dataset_val.shape[1])-1]\n",
        "  y = dataset_val[0:sample_size[i],-1]\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=69420)\n",
        "\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  scaler_x = StandardScaler()\n",
        "  scaler_y = StandardScaler()\n",
        "\n",
        "  y_train = y_train.reshape(-1,1)\n",
        "  y_test = y_test.reshape(-1,1)\n",
        "\n",
        "  scaler_x.fit(X_train)\n",
        "  scaler_y.fit(y_train)\n",
        "\n",
        "  X_train_stand = scaler_x.transform(X_train)\n",
        "  X_test_stand = scaler_x.transform(X_test)\n",
        "  y_train_stand = scaler_y.transform(y_train)\n",
        "  y_test_stand = scaler_y.transform(y_test)\n",
        "\n",
        "  # Initialize polynomial features for regression\n",
        "  qr = LinearRegression()\n",
        "  quadratic = PolynomialFeatures(degree = 2)\n",
        "  poly_reg_quad = make_pipeline(quadratic, qr)\n",
        "  poly_reg_quad.fit(X_train_stand,y_train_stand)\n",
        "  y_quad_eval_train = poly_reg_quad.predict(X_train_stand)\n",
        "  y_quad_eval_test = poly_reg_quad.predict(X_test_stand)\n",
        "  # Power 2 MSE\n",
        "  training_error = MSE(y_train_stand,y_quad_eval_train)\n",
        "  testing_error = MSE(y_test_stand,y_quad_eval_test)\n",
        "  print(\"Training Error: {}\".format(inverse_normalize(training_error, 'y')))\n",
        "  print(\"Testing Error: {}\".format(inverse_normalize(testing_error, 'y')))\n",
        "\n",
        "  p2_training_error.append(inverse_normalize(training_error, 'y'))\n",
        "  p2_testing_error.append(inverse_normalize(testing_error, 'y'))"
      ],
      "id": "9-ELnMoLtDZv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_C_BwGSyjI5"
      },
      "source": [
        "####Power 4 - Pipeline"
      ],
      "id": "o_C_BwGSyjI5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "applicable-obligation",
        "outputId": "8dcc38c4-df7e-48ad-8aaf-e24e8b31bbc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Size: 10\n",
            "Training Error: 6.169379817160378e-26\n",
            "Testing Error: 0.22339699059337514\n",
            "Sample Size: 20\n",
            "Training Error: 3.771649666389034e-27\n",
            "Testing Error: 97263.5734968643\n",
            "Sample Size: 40\n",
            "Training Error: 0.07978162007331813\n",
            "Testing Error: 5.644268731126022e+21\n",
            "Sample Size: 80\n",
            "Training Error: 0.3562381981187128\n",
            "Testing Error: 3.906963416960303e+20\n",
            "Sample Size: 100\n",
            "Training Error: 0.39914739080882805\n",
            "Testing Error: 1.0185012679309477e+22\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# method to un-normalize the data back to critical temperatures in Kelvin\n",
        "def inverse_normalize(value, y_or_x):\n",
        "    new_val = np.array(value).reshape(-1,1)\n",
        "    \n",
        "    if y_or_x == \"x\":\n",
        "        scaler = scaler_x\n",
        "    else:\n",
        "        scaler = scaler_y\n",
        "    new_val = scaler.inverse_transform(new_val)\n",
        "    \n",
        "    return new_val\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "sample_size = [10, 20, 40, 80, 100]\n",
        "\n",
        "p4_training_error = []\n",
        "p4_testing_error = []\n",
        "\n",
        "for i in range(len(sample_size)):\n",
        "\n",
        "  print(\"Sample Size: {}\".format(sample_size[i]))\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  pd.get_option(\"display.max_columns\")\n",
        "  import matplotlib.pyplot as plt\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  dataset = pd.read_csv('/content/drive/My Drive/4ML3/Project/train.csv')\n",
        "  dataset_val = dataset.values\n",
        "\n",
        "  x = dataset_val[0:sample_size[i],0:(dataset_val.shape[1])-1]\n",
        "  y = dataset_val[0:sample_size[i],-1]\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=69420)\n",
        "\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  scaler_x = StandardScaler()\n",
        "  scaler_y = StandardScaler()\n",
        "\n",
        "  y_train = y_train.reshape(-1,1)\n",
        "  y_test = y_test.reshape(-1,1)\n",
        "\n",
        "  scaler_x.fit(X_train)\n",
        "  scaler_y.fit(y_train)\n",
        "\n",
        "  X_train_stand = scaler_x.transform(X_train)\n",
        "  X_test_stand = scaler_x.transform(X_test)\n",
        "  y_train_stand = scaler_y.transform(y_train)\n",
        "  y_test_stand = scaler_y.transform(y_test)\n",
        "\n",
        "  # Initialize polynomial features for regression\n",
        "  # Power 4 Model Setup\n",
        "  p4r = LinearRegression()\n",
        "  power4 = PolynomialFeatures(degree = 4)\n",
        "  poly_reg_four = make_pipeline(power4, p4r)\n",
        "  poly_reg_four.fit(X_train_stand,y_train_stand)\n",
        "  y_p4_eval_train = poly_reg_four.predict(X_train_stand)\n",
        "  y_p4_eval_test = poly_reg_four.predict(X_test_stand)\n",
        "  # Power 4 MSE\n",
        "  training_error = MSE(y_train_stand,y_p4_eval_train)\n",
        "  testing_error = MSE(y_test_stand,y_p4_eval_test)\n",
        "  #print(\"Training Error: {}\".format(inverse_normalize(training_error, 'y')))\n",
        "  #print(\"Testing Error: {}\".format(inverse_normalize(testing_error, 'y')))\n",
        "\n",
        "  print(\"Training Error: {}\".format(training_error))\n",
        "  print(\"Testing Error: {}\".format(testing_error))\n",
        "\n",
        "  p4_training_error.append(inverse_normalize(training_error, 'y'))\n",
        "  p4_testing_error.append(inverse_normalize(testing_error, 'y'))"
      ],
      "id": "applicable-obligation"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qhxgwz6m2R-8"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "Qhxgwz6m2R-8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMv7Mngp6uCD"
      },
      "source": [
        "### Determining Relation of Samples to Polynomial Transformed Features\n",
        "\n"
      ],
      "id": "QMv7Mngp6uCD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjTz4YWt7At2"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "QjTz4YWt7At2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8lKnw1g7Buh"
      },
      "source": [
        "#### Power 2 No Pipeline"
      ],
      "id": "q8lKnw1g7Buh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLe3h8xa6zsX",
        "outputId": "19e8c7fa-7fd5-4adc-a499-1b638c887576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Sizes to Iterate through: [25, 50, 100, 200, 400, 800, 1600, 3200, 6400, 12800, 16000, 20000, 21263]\n",
            "Polynomial Level = 2\n",
            "Sample Size = 25\n",
            "Size of transformed x: (17, 3403)\n",
            "Training Error: [[29.86301674]]\n",
            "Testing Error: [[1.73002902e+19]]\n",
            "\n",
            "Polynomial Level = 2\n",
            "Sample Size = 50\n",
            "Size of transformed x: (35, 3403)\n",
            "Training Error: [[50.57553674]]\n",
            "Testing Error: [[3.83583492e+17]]\n",
            "\n",
            "Polynomial Level = 2\n",
            "Sample Size = 100\n",
            "Size of transformed x: (70, 3403)\n",
            "Training Error: [[57.79279963]]\n",
            "Testing Error: [[6.82395997e+24]]\n",
            "\n",
            "Polynomial Level = 2\n",
            "Sample Size = 200\n",
            "Size of transformed x: (140, 3403)\n",
            "Training Error: [[65.90891252]]\n",
            "Testing Error: [[6.5301658e+24]]\n",
            "\n",
            "Polynomial Level = 2\n",
            "Sample Size = 400\n",
            "Size of transformed x: (280, 3403)\n",
            "Training Error: [[68.59315298]]\n",
            "Testing Error: [[6.19286635e+20]]\n",
            "\n",
            "Polynomial Level = 2\n",
            "Sample Size = 800\n",
            "Size of transformed x: (560, 3403)\n",
            "Training Error: [[68.27780581]]\n",
            "Testing Error: [[4.25230847e+25]]\n",
            "\n",
            "Polynomial Level = 2\n",
            "Sample Size = 1600\n",
            "Size of transformed x: (1120, 3403)\n",
            "Training Error: [[66.08917383]]\n",
            "Testing Error: [[3.90753333e+24]]\n",
            "\n",
            "Polynomial Level = 2\n",
            "Sample Size = 3200\n",
            "Size of transformed x: (2240, 3403)\n",
            "Training Error: [[62.07622951]]\n",
            "Testing Error: [[2.34751655e+21]]\n",
            "\n",
            "Polynomial Level = 2\n",
            "Sample Size = 6400\n",
            "Size of transformed x: (4480, 3403)\n",
            "Training Error: [[62.72923351]]\n",
            "Testing Error: [[1.77642136e+22]]\n",
            "\n",
            "Polynomial Level = 2\n",
            "Sample Size = 12800\n",
            "Size of transformed x: (8960, 3403)\n",
            "Training Error: [[50.82914695]]\n",
            "Testing Error: [[13887.36090211]]\n",
            "\n",
            "Polynomial Level = 2\n",
            "Sample Size = 16000\n",
            "Size of transformed x: (11200, 3403)\n",
            "Training Error: [[43.13153267]]\n",
            "Testing Error: [[198.48341108]]\n",
            "\n",
            "Polynomial Level = 2\n",
            "Sample Size = 20000\n",
            "Size of transformed x: (14000, 3403)\n",
            "Training Error: [[38.62382991]]\n",
            "Testing Error: [[55.50263848]]\n",
            "\n",
            "Polynomial Level = 2\n",
            "Sample Size = 21263\n",
            "Size of transformed x: (14884, 3403)\n",
            "Training Error: [[37.45607206]]\n",
            "Testing Error: [[77.8619406]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "poly_level = 2\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "import math\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# method to un-normalize the data back to critical temperatures in Kelvin\n",
        "def inverse_normalize(value, y_or_x):\n",
        "    new_val = np.array(value).reshape(-1,1)\n",
        "    \n",
        "    if y_or_x == \"x\":\n",
        "        scaler = scaler_x\n",
        "    else:\n",
        "        scaler = scaler_y\n",
        "    new_val = scaler.inverse_transform(new_val)\n",
        "    \n",
        "    return new_val\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.get_option(\"display.max_columns\")\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_csv('/content/drive/My Drive/4ML3/Project/train.csv')\n",
        "dataset_val = dataset.values\n",
        "\n",
        "x = dataset_val[:,0:(dataset_val.shape[1])-1]\n",
        "y = dataset_val[:,-1]\n",
        "\n",
        "\n",
        "sample_sizes = [25, 50, 100, 200, 400, 800, 1600, 3200, 6400, 12800, 16000, 20000, len(x)]\n",
        "print(\"Sample Sizes to Iterate through: {}\".format(sample_sizes))\n",
        "\n",
        "training_errors = []\n",
        "testing_errors = []\n",
        "training_errors_log = []\n",
        "testing_errors_log = []\n",
        "\n",
        "\n",
        "for i in range(len(sample_sizes)):\n",
        "\n",
        "  print(\"Polynomial Level = {}\\nSample Size = {}\".format(poly_level, sample_sizes[i]))\n",
        "\n",
        "  x = dataset_val[0:sample_sizes[i],0:(dataset_val.shape[1])-1]\n",
        "  y = dataset_val[0:sample_sizes[i],-1]\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=69420)\n",
        "\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  scaler_x = StandardScaler()\n",
        "  scaler_y = StandardScaler()\n",
        "\n",
        "  y_train = y_train.reshape(-1,1)\n",
        "  y_test = y_test.reshape(-1,1)\n",
        "\n",
        "  scaler_x.fit(X_train)\n",
        "  scaler_y.fit(y_train)\n",
        "\n",
        "  X_train_stand = scaler_x.transform(X_train)\n",
        "  X_test_stand = scaler_x.transform(X_test)\n",
        "  y_train_stand = scaler_y.transform(y_train)\n",
        "  y_test_stand = scaler_y.transform(y_test)\n",
        "  quadratic_reg = LinearRegression()\n",
        "  quadratic = PolynomialFeatures(degree = poly_level)\n",
        "  x_quad = quadratic.fit_transform(X_train_stand)\n",
        "\n",
        "  print(\"Size of transformed x: {}\".format(x_quad.shape))\n",
        "\n",
        "  quadratic_reg.fit(x_quad, y_train_stand)\n",
        "\n",
        "  x_quad_train_stand = quadratic.fit_transform(X_train_stand)\n",
        "  x_quad_test_stand = quadratic.fit_transform(X_test_stand)\n",
        "\n",
        "  y_pred_quad_train = quadratic_reg.predict(x_quad_train_stand)\n",
        "  y_pred_quad_test = quadratic_reg.predict(x_quad_test_stand)\n",
        "\n",
        "  training_error = inverse_normalize(MSE(y_train_stand,y_pred_quad_train), \"y\")\n",
        "  testing_error = inverse_normalize(MSE(y_test_stand,y_pred_quad_test), \"y\")\n",
        "\n",
        "  training_errors.append(training_error)\n",
        "  testing_errors.append(testing_error)\n",
        "\n",
        "  training_errors_log.append(math.log(training_error, 10))\n",
        "  testing_errors_log.append(math.log(testing_error, 10))\n",
        "\n",
        "\n",
        "  print(\"Training Error: {}\".format(training_error))\n",
        "  print(\"Testing Error: {}\\n\".format(testing_error))"
      ],
      "id": "TLe3h8xa6zsX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHnhT6QhBUzy"
      },
      "outputs": [],
      "source": [
        "training_errors = (np.array(training_errors)).reshape(-1,1)"
      ],
      "id": "FHnhT6QhBUzy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ul7ObfQBBoeh",
        "outputId": "921e8b4f-713b-441a-fb2e-e030416d0a09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[29.86301674],\n",
              "       [50.57553674],\n",
              "       [57.79279963],\n",
              "       [65.90891252],\n",
              "       [68.59315298],\n",
              "       [68.27780581],\n",
              "       [66.08917383],\n",
              "       [62.07622951],\n",
              "       [62.72923351],\n",
              "       [50.82914695],\n",
              "       [43.13153267],\n",
              "       [38.62382991],\n",
              "       [37.45607206]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_errors"
      ],
      "id": "ul7ObfQBBoeh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Xl8iJtwm_jVO",
        "outputId": "51a5b68e-7806-46b7-a8b3-05c847b93f8b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "training_errors = (np.array(training_errors)).reshape(-1,1)\n",
        "testing_errors = (np.array(testing_errors)).reshape(-1,1)\n",
        "\n",
        "plt.plot(sample_sizes, training_errors)\n",
        "plt.title(\"Training MSE vs Training Sample Size\")\n",
        "plt.xlabel(\"Training Sample Size\")\n",
        "plt.ylabel(\"Training MSE\")\n",
        "# plt.legend([\"train\", \"validation\"])\n",
        "plt.show()"
      ],
      "id": "Xl8iJtwm_jVO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "-oyDBZxX_zwo",
        "outputId": "85c50186-c0a5-4bcc-91c8-959ab3f9a34f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(sample_sizes, testing_errors_log)\n",
        "plt.title(\"Testing MSE vs Training Sample Size\")\n",
        "plt.xlabel(\"Training Sample Size\")\n",
        "plt.ylabel(\"log10 Testing MSE\")\n",
        "# plt.legend([\"train\", \"validation\"])\n",
        "plt.show()"
      ],
      "id": "-oyDBZxX_zwo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOanBeU383SS"
      },
      "source": [
        "### Power 4 No Pipeline"
      ],
      "id": "fOanBeU383SS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVjCFFSp87te"
      },
      "source": [
        ""
      ],
      "id": "FVjCFFSp87te"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mlh27FMK88F-",
        "outputId": "18dc0f8e-8ed4-400c-e8fe-2b1ebefae4f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Sizes to Iterate through: [25, 50, 100, 200]\n",
            "Polynomial Level = 4\n",
            "Sample Size = 25\n",
            "Size of transformed x: (17, 2024785)\n",
            "Training Error: [[29.99858461]]\n",
            "Testing Error: [[3.09800604e+18]]\n",
            "\n",
            "Polynomial Level = 4\n",
            "Sample Size = 50\n",
            "Size of transformed x: (35, 2024785)\n",
            "Training Error: [[50.68526502]]\n",
            "Testing Error: [[1.4951221e+18]]\n",
            "\n",
            "Polynomial Level = 4\n",
            "Sample Size = 100\n",
            "Size of transformed x: (70, 2024785)\n",
            "Training Error: [[69.31345433]]\n",
            "Testing Error: [[3.17427517e+23]]\n",
            "\n",
            "Polynomial Level = 4\n",
            "Sample Size = 200\n",
            "Size of transformed x: (140, 2024785)\n",
            "Training Error: [[68.27431284]]\n",
            "Testing Error: [[1.57988315e+25]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "poly_level = 4\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# method to un-normalize the data back to critical temperatures in Kelvin\n",
        "def inverse_normalize(value, y_or_x):\n",
        "    new_val = np.array(value).reshape(-1,1)\n",
        "    \n",
        "    if y_or_x == \"x\":\n",
        "        scaler = scaler_x\n",
        "    else:\n",
        "        scaler = scaler_y\n",
        "    new_val = scaler.inverse_transform(new_val)\n",
        "    \n",
        "    return new_val\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.get_option(\"display.max_columns\")\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_csv('/content/drive/My Drive/4ML3/Project/train.csv')\n",
        "dataset_val = dataset.values\n",
        "\n",
        "x = dataset_val[:,0:(dataset_val.shape[1])-1]\n",
        "y = dataset_val[:,-1]\n",
        "\n",
        "training_errors = []\n",
        "testing_errors = []\n",
        "training_errors_log = []\n",
        "testing_errors_log = []\n",
        "\n",
        "\n",
        "sample_sizes = [25, 50, 100, 200]\n",
        "print(\"Sample Sizes to Iterate through: {}\".format(sample_sizes))\n",
        "\n",
        "\n",
        "for i in range(len(sample_sizes)):\n",
        "\n",
        "  print(\"Polynomial Level = {}\\nSample Size = {}\".format(poly_level, sample_sizes[i]))\n",
        "\n",
        "  x = dataset_val[0:sample_sizes[i],0:(dataset_val.shape[1])-1]\n",
        "  y = dataset_val[0:sample_sizes[i],-1]\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=69420)\n",
        "\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  scaler_x = StandardScaler()\n",
        "  scaler_y = StandardScaler()\n",
        "\n",
        "  y_train = y_train.reshape(-1,1)\n",
        "  y_test = y_test.reshape(-1,1)\n",
        "\n",
        "  scaler_x.fit(X_train)\n",
        "  scaler_y.fit(y_train)\n",
        "\n",
        "  X_train_stand = scaler_x.transform(X_train)\n",
        "  X_test_stand = scaler_x.transform(X_test)\n",
        "  y_train_stand = scaler_y.transform(y_train)\n",
        "  y_test_stand = scaler_y.transform(y_test)\n",
        "  quadratic_reg = LinearRegression()\n",
        "  quadratic = PolynomialFeatures(degree = poly_level)\n",
        "  x_quad = quadratic.fit_transform(X_train_stand)\n",
        "\n",
        "  print(\"Size of transformed x: {}\".format(x_quad.shape))\n",
        "\n",
        "  quadratic_reg.fit(x_quad, y_train_stand)\n",
        "\n",
        "  x_quad_train_stand = quadratic.fit_transform(X_train_stand)\n",
        "  x_quad_test_stand = quadratic.fit_transform(X_test_stand)\n",
        "\n",
        "  y_pred_quad_train = quadratic_reg.predict(x_quad_train_stand)\n",
        "  y_pred_quad_test = quadratic_reg.predict(x_quad_test_stand)\n",
        "\n",
        "  training_error = inverse_normalize(MSE(y_train_stand,y_pred_quad_train), \"y\")\n",
        "  testing_error = inverse_normalize(MSE(y_test_stand,y_pred_quad_test), \"y\")\n",
        "\n",
        "  training_errors.append(training_error)\n",
        "  testing_errors.append(testing_error)\n",
        "\n",
        "  training_errors_log.append(math.log(training_error, 10))\n",
        "  testing_errors_log.append(math.log(testing_error, 10))\n",
        "\n",
        "  print(\"Training Error: {}\".format(training_error))\n",
        "  print(\"Testing Error: {}\\n\".format(testing_error))"
      ],
      "id": "Mlh27FMK88F-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "y0s1nHM4DXka",
        "outputId": "31c14fc2-8e74-4c14-8405-217ea449d6d0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwcdZ3/8dd7Jhc5SEgyhEBiwh0FJcAQYEUOUUQ8EI8AQgAB8Vg8fq4Hrvtb0cX9ea1Rd10RuXJwX3KKIIeK5pqYEG7DkSEJOYaQ+5zMfH5/VE3S05mZzExS093T7+fj0Y+pqq7j09U9n6r6VtWnFBGYmVn5qCh0AGZm1rWc+M3MyowTv5lZmXHiNzMrM078ZmZlxonfzKzMOPF3M5J+L+nC3T2udYykqyX93909bqmS9KSkSzOYb7dfd5mICL8K/ALW5bwagY05/ecVOr5OfJ6TgQDuyRt+RDr8yZxhZwJzgTXAm8DjwP7pe1cC9XnrZ1WGcT+Xs5wGYFNO/78Wer128jNdArwIrAWWAQ8BAwoQx5PApaX8GbrTq8fu2XzYroiI/k3dkhaQ/IP8MX88ST0iYmtXxrYL6oDjJQ2JiBXpsAuBfzSNIOkgYDLwcZKE3x84jSTpNrktIs7vioAj4rCc2J4EpkbEtfnjlcr3IOkk4D+B0yNijqTBwEcKHFaHdIfPUIzc1FPEJJ0saZGkb0laCtwgaS9JD0iqk7Qy7R6RM822Q2pJF0l6StJP03Ffk/TBTo67v6Q/S1or6Y+SfiVpahvhbwF+B5yTTl8JnA3clDPOWOC1iHgsEmsj4q6IeL0T6+r3ki7PG/a0pI8rMVHScklrJD0j6fAOzHu0pJB0iaTXSTZSSLpD0lJJq9N1k7vhuFHSVWl30/f4L2kMSyR9ppPjDpF0f/o5Zkm6StJTrYR+DDAtIuYARMRbETEpItam8/qQpDnpvBZKurKFz/yZ9L2Vkj4v6RhJ8yStkvQ/OeNfJOmvkv4nXR8vSjq1jXV6saQX0vn+QdKoTn6G3HV3v6R1Oa9GSRel742R9KiktyS9JGl8a7GVAyf+4rcPMBgYBVxG8p3dkPa/jaRZ6H9anRqOBV4ChgI/Bq6TpE6MezMwExhC0gQzoR2xTwYuSLs/ADwLvJHz/t+BMWlSPkVS//wZdMAtwLlNPZLeQbKOHiQ5ijgROAQYCIwHVrQwj505CXg7yWcB+D1wMLA3yWe5qZXpIPkeBwL7kTRd/ErSXp0Y91fA+nScC9NXa2YAH5D0PUnvltQ77/31JN/PIOBDwBckfSxvnGPTz3g28HPgO8D7gMOA8Ur2yHPHfYXk9/Nd4G4le+jNSDoT+FeSI70q4C8k319nPsM2EfGRiOifHkF/ClgKPCapH/AoyW94b5Kdkf9NfyPlqdBtTX41fwELgPel3SeT7Dn3aWP8scDKnP4nSdtSgYuAl3Pe60vSxr5PR8Yl2cBsBfrmvD+VpCmkpZhOBhal3fOBQ4FbgfOAS2nexn8ccDtJ09Am4Eagf/relennX5XzeqKVZQ4gSWSj0v4fANen3e8laWI6Dqho5/eQu25Gp+vigDbGH5SOMzDtvxG4Kmd9bAR65Iy/HDiuI+MClSTnPA7Nee8q4Kk24vogcH+67tYBPwMqWxn358DEvM+8X877K4Czc/rvAr6a8/t5A1DO+zOBCS2sz98Dl+SMVwFsaPruOvIZctddzviHpOvshLT/bOAveeP8BvhuV/9/F8vLe/zFry4iNjX1SOor6TeSaiWtAf4MDEqbUlqytKkjIjakna3tWbc27r7AWznDABa2M/4pwOXAKcA9+W9GxPSIGB8RVcB7SPbMv5Mzyu0RMSjndUpLC4nk0P9B0qYlkr3/m9L3Hic5KvoVsFzSNZL2bGf8ubZ9ZkmVkn4o6ZX0e1iQvjW0lWlXRPPzAhto/XtobdwqoAfN132b30NE/D4iPkJy1HgmSYJuat47VtITSpoNVwOfbyH+ZTndG1voz/0MiyPNqqlakt9OvlHAL9LmolXAW4BIjnA69BnySRoI3Av8W0Q0NYGNAo5tWl66zPNIdmrKkhN/8csvn/ovJHvQx0bEniSJEpJ/nKwsAQZL6pszbGQ7p50CfBF4KG/DsYOImAXcDbS7/T3PLcC5ko4H+gBP5Mz7lxFxNPAOkj3Cb3Ri/rnfxadJktD7SJplRqfDs/we6kiOvEbkDGvX9xARjRHxGMn5iab1ezNwHzAyIgYCV7Nr8e+X14z4Npo37TVZCHwub4O+R0T8rROfYRtJFSSf6YmIuCZveX/KW17/iPhCRz9gd+HEX3oGkOxprUrbT7+b9QIjohaoAa6U1CtNrO26siIiXiNpG/9O/nuSTpD0WUl7p/1jgI8C0zsZ6kMke3ffJ7kaqDGd7zHp3m1PkuagTSSXze6KAcBmkuaPviRXnmQqIhpINoxXpkd+Y9h+DmUHks6UdI6SCwIkaRzJd9G0fgeQHMltSt/79C6GuDfwZUk9JX2K5HzIQy2MdzXwbaUnwyUNTMfvzGfI9QOgH/CVvOEPAIdImpDG1jP9Tby9cx+z9Dnxl56fA3uQXPM+HXi4i5Z7HnA8SaK7CriNJPHtVEQ8FREt7fmtIkn0z0haR/JZ7iE5sdzk7LwrNdY1bShaWM5mksT4PpI9vyZ7Ar8FVpI0P6wAftKe2NswOZ3XYuB5Or+x6qjLSY4wlpIcTd1C69/DSuCzJOdZ1pCcl/lJRDSdhP4i8H1Ja4F/JznXsitmkJwIfpMkCX8ytl/Ku01E3AP8CLg1bSZ7lqQdvzOfIde5JOdCVub8Vs5LmwFPI2kGfINk3f0IaPVEcXen5k1yZu0j6TbgxYjI/IjDWifpRyQn6wt6B3Z62eSlEXFCIeOw9vEev7VLemh8oKQKSaeTtG//rtBxlZv0evR35TR7XEILJ83N2uI7d6299iFpRhkCLAK+EOlNNdalBpA07+xLcoXNf5FcxWLWbm7qMTMrM27qMTMrMyXR1DN06NAYPXp0ocMwMysps2fPfjO9ObKZkkj8o0ePpqamptBhmJmVFEm1LQ13U4+ZWZlx4jczKzOZJX5Jh0qam/NaI+mrkgandbHnp39bK01rZmYZyCzxR8RLETE2IsYCR5NUGLwHuAJ4LCIOBh5L+83MrIt0VVPPqcArabGvM4FJ6fBJQP6DH8zMLENdlfjPYfsTdoZFxJK0eykwrKUJJF0mqUZSTV1dXVfEaGZWFjJP/JJ6kVRgvCP/vfShDS3eOhwR10REdURUV1XtcBmqmZl1Uldcx/9B4O8R0fTknmWShkfEEknDSR6RZpaZLVsbuWXm66zZWE/vnhX0qqygd89KeveooFePCnr3SLqb9W8bb/v7vSorqKjI8jkrZl2jKxL/uTR/kPJ9JA+I/mH61wWmLDONjcHX73ia+55u6XEAHderMncDkWxAtm8gmm9Ito3To7LNjcr28Zr39+lZQa/Kyh3mXemNj+2iTBN/+nT79wOfyxn8Q+B2SZeQPMhifJYxWPmKCL5733Pc9/QbfPP0Q/nsew5g89ZGtmxtZPPWBjbXNzbvz+ve9qpvYEtD47bxN29tSMdrmj4dv76RTfWNrN5Yz+b6xpxpts97a+OuF0XsUaHmG5X8DUmzDVELRzM9KnY48mlpw9PWRqpHhWj+lEUrJZkm/ohYT1LGN3fYCpKrfMwy9bNH/8GU6bV87qQD+OLJBwHQs7KioM9d2tqQbBC2bTjqG9nS0MCmFjZC+Rup3A3J9g1P841QU//69VtzpsndqCXDdlWFaP3opsXmtOZNavlHQC0fLeVNn390VVnhjU8nlUStHrOOuvYvr/Lfj7/MOceM5IrTxxQ6nG16VFbQo7KCvr0KF0NjYyQbhJ1uVFo5GqrfvgHJ36jkbrRWb6xv4ehq+/u7Q0sblfyNUFtNcL2abYi2Hw3t+F7+hmv7Bq4Uz/s48Vu3c0fNQq568AXOeOc+/OCsd3qvME9FhehTUUmfnpVAz4LEEBHUN0TeRiVvw7PTo5uWN1y5G6l1m7fy1vrW570bWt7oWam2NypNR0CdPLp5134DGdJ/9x6mOvFbt/KH55byrbvm8Z6DhzLx7LE+EVqkJNGrR3KuYkAB49ja0I6jm1aOhvI3JFsaWj5vtLm+kTUbt+64kUvPHdU3tL31mXTxOE46ZPde0u7Eb93G315+ky/dPIcjRg7i6vOPpnePykKHZEWuqemtXwHP+zQ0BltavLAg6T6wqv9uX6YTv3ULTy9cxWcn17D/0H7ccNEx9Ovtn7aVhsoKsUevSvbo1XVNby7LbCVv/rK1XHTDTAb378XkS8YxqJBnTs1KgBO/lbSFb21gwnUz6VFZwdRLjmXYnn0KHZJZ0XPit5JVt3YzE66bwYYtW5lyyThGDelX6JDMSoIbQq0krd5Yz4XXz2TZms1MvXQcY/bZs9AhmZUM7/Fbydm4pYFLJ81i/vK1XD3haI4eNbjQIZmVFO/xW0mpb2jkizfNpqZ2Jf997pG7/fpms3LgPX4rGY2Nwb/c/jRPvFTHDz72Tj78rn0LHZJZSXLit5KQW2nzW6eP4dPHvq3QIZmVLCd+Kwm5lTa/cPKBhQ7HrKQ58VvRK9ZKm2alyonfiporbZrtfpkmfkmDJN0p6UVJL0g6XtKVkhZLmpu+zsgyBitdrrRplo2sL+f8BfBwRHxSUi+gL/ABYGJE/DTjZVsJc6VNs+xklvglDQROBC4CiIgtwBYfqtvOuNKmWbaybOrZH6gDbpA0R9K16cPXAS6XNE/S9ZL2amliSZdJqpFUU1dXl2GYVkxeXu5Km2ZZyzLx9wCOAn4dEUcC64ErgF8DBwJjgSXAf7U0cURcExHVEVFdVeW7M8vBopUbOP9aV9o0y1qWiX8RsCgiZqT9dwJHRcSyiGiIiEbgt8C4DGOwEpFU2pzJhi1bmXyxK22aZSmzxB8RS4GFkg5NB50KPC9peM5oZwHPZhWDlYamSptLV2/ihs8cw9uHu9KmWZayPmv2JeCm9IqeV4HPAL+UNBYIYAHwuYxjsCKWW2nz2guPcaVNsy6QaeKPiLlAdd7gCVku00qHK22aFYbv3LWCcKVNs8Jx4rcul1tp85unH+pKm2ZdzInfutzEpkqbJx7AF05ypU2zrubEb13q2r+8yi8ff5mzq0dyxQfHuOiaWQE48VuXuXP2Iq568AU+ePg+/OfHXWnTrFCc+K1LNFXaPOGgofz8HFfaNCskJ37LXFOlzXfuN5DfTHClTbNCc+K3TOVW2rzxM660aVYMnPgtM660aVacnPgtE660aVa8nPhtt3OlTbPi5gZX263WbEoqbS5ZvZGbLj3WlTbNipD3+G232bilgUtvrGH+8rX8ZkK1K22aFSnv8dtu0VRpc1btW660aVbkvMdvu6yxMfj6Ha60aVYqnPhtl0QEV97/HPfOdaVNs1KRaeKXNEjSnZJelPSCpOMlDZb0qKT56d+9sozBsjXx0X8weZorbZqVkqz3+H8BPBwRY4AjgBeAK4DHIuJg4LG030rQdU+95kqbZiUos8QvaSBwInAdQERsiYhVwJnApHS0ScDHsorBsnPn7EX8xwPPu9KmWQnKco9/f6AOuEHSHEnXSuoHDIuIJek4S4FhLU0s6TJJNZJq6urqMgzTOsqVNs1KW5aJvwdwFPDriDgSWE9es05EBBAtTRwR10REdURUV1X50sBi8bdXXGnTrNRlmfgXAYsiYkbafyfJhmCZpOEA6d/lGcZgu9G8Rav47KQaRg/tyw0XudKmWanKLPFHxFJgoaRD00GnAs8D9wEXpsMuBO7NKgbbfV5evpYLr5/JXv16MeWSY9mrnyttmpWqrHfZvgTcJKkX8CrwGZKNze2SLgFqgfEZx2C7KLfS5k2XutKmWanLNPFHxFyguoW3Ts1yubb7vLlue6XN2z53vCttmnUDbqS1Vq3ZVM8F17nSpll345IN1iJX2jTrvrzHbzuob2jkn2/+O7Nq3+KX57jSpll34z1+a6ap0ubjLy7nqo8dzkeOcKVNs+7Gid+2ya+0ed6xowodkpllwInftmmqtHmZK22adWtO/AY0r7T5bVfaNOvWnPjNlTbNyowTf5l7xJU2zcqOE38Z+9srb3L5La60aVZunPjL1LZKm0NcadOs3DjxlyFX2jQrb078ZWbRyg1MuG4mlRUVTL3ElTbNypETfxlpqrS5fvNWplwyjtFDXWnTrBy5YbdM5FbanHqJK22alTPv8ZeBTfXbK21eff7RVI92pU2zctZq4pc0Jqe7d957x7Vn5pIWSHpG0lxJNemwKyUtTofNlXRGZ4O3natvaOSLNyWVNn82fiwnH7p3oUMyswJra4//5pzuaXnv/W8HlnFKRIyNiNwncU1Mh42NiIc6MC/rAFfaNLOWtJX41Up3S/1WZHIrbX7jA660aWbbtZX4o5XulvrbmscjkmZLuixn+OWS5km6XtJeLU0o6TJJNZJq6urq2rk4azLxj/O3Vdr84smutGlm2ymi5RwuaTlwK8ne/dlpN2n/+IgYttOZS/tFxGJJewOPAl8CXgLeJNko/AcwPCIubms+1dXVUVNT075PZFz31Gv8xwPPc3b1SH74CRddMytXkmbnNbMDbV/O+Y2c7vys264sHBGL07/LJd0DjIuIP+cE9VvggfbMy9rnrrTS5umH7cMPzjrcSd/MdtBq4o+ISfnD0maZVdHaYULzcfsBFRGxNu0+Dfi+pOERsSQd7Szg2c6FbvkeeW4p37xrHu8+aAi/OHcsPSp9ta6Z7aityzn/vemSTkm9JT0OvAIsk/S+dsx7GPCUpKeBmcCDEfEw8OP0Es95wCnA/9nlT2HNKm1eM6HalTbNrFVtNfWcTdIGD3AhSdt+FXAIMAn4Y1szjohXgSNaGD6hU5Faq5oqbY4a7EqbZrZzbbUFbMlp0vkAcGtENETEC7jUQ9FwpU0z66i2Ev9mSYdLqiJpknkk572+2YZl7ZFfaXOfga60aWY719ae+1eBO0madyZGxGsAaYmFOV0Qm7WhqdLmus1buf1zx7vSppm1W1tX9UwHxrQw/CHAZRYKaM2mei683pU2zaxzWk38kr7W1oQR8bPdH47tTFOlzX8sW8tvL6h2pU0z67C2mnp+CswFfg9sxvV5Ci630uYvzznSlTbNrFPaSvxHAucCHwJmA7cAj7Xn5i3b/Vxp08x2l1av6omIpyPiiogYC1wHnAk8L+mjXRadAUmlze/lVNo8/zhX2jSzztvpPf3p5ZxHAu8EFgHLsw7Kmpv4x/lMmlbLZ9+zvyttmtkua+vk7sXAeKAPyWWd4yPCSb+LXf/Ua/zysfmMrx7Bv57xdhddM7Nd1lYb/7UkBdRqSe7cPS036USEm3wydtfsRXw/rbT5n2e5vLKZ7R5tJf5TuiwK24ErbZpZVtq6getPXRmIbTftlRVcfsscDnelTTPLgHcji8y8Rav47OSk0uaNrrRpZhlw4i8iTZU2B/Xt6UqbZpYZJ/4isW7zVi5wpU0z6wI7bUeQdD/Jg9FzrSZ57u5vImJTG9MuANYCDcDWiKiWNBi4DRgNLCC5THRlZ4LvTu75+yLeWL3JlTbNLHPt2eN/FVgH/DZ9rSFJ5oek/TtzSkSMzXnS+xUkpR8OBh5L+8taRDB5Wi3vGjGQcfu76JqZZas9Zw7/KSKOyem/X9KsiDhG0nOdWOaZwMlp9yTgSeBbnZhPtzHt1RXMX76On3zyXYUOxczKQHv2+PtLeltTT9rdP+3dspNpA3hE0mxJl6XDhkXEkrR7KclD2Xcg6TJJNZJq6urq2hFm6ZoyrZZBfXu68JqZdYn27PH/C/CUpFdISjPvD3xRUj+SPfa2nBARiyXtDTwq6cXcNyMiJLVY7TMirgGuAaiuru62FUGXrN7II88v49IT9qdPT1+vb2bZ22nij4iHJB3M9qdxvZRzQvfnO5l2cfp3uaR7gHHAMknDI2KJpOGUedG3W2a8TmOEK26aWZdp7+WcRwOHAUcA4yVdsLMJJPWTNKCpGziNpPbPfcCF6WgXAvd2NOjuYsvWRm6euZBTDt2bkYP9/Hoz6xrtuZxzCnAgydO4GtLBAUzeyaTDgHvSwmI9gJsj4mFJs4DbJV1CUgBufCdjL3kPP7c0eWj68d7bN7Ou0542/mrgHR198lZEvEpyhJA/fAVwakfm1V1NmbaAUUP6ctLBVYUOxczKSHuaep4F9sk6kHLzwpI1zFqwkvOPHUVFhcstm1nXac8e/1CSRy7OJHnoOuB6/Ltq8rRaeveo4FPVIwodipmVmfYk/iuzDqLcrN5Yz+/mLObMsfsyqK8LsZlZ12rP5Zyuy7+b3TV7ERvrG7jg+NGFDsXMylBbz9x9KiJOkLSW5kXaRHLv1Z6ZR9cNNTYGU6fXcuTbBnH4fgMLHY6ZlaG2nsB1Qvp3QNeF0/399ZU3efXN9Uw8e4cLnszMukS7Hu8kqZLkuvxt40fE61kF1Z1NnlbLkH69OOOdwwsdipmVqfbcwPUl4LvAMqAxHRyAS0l20OJVG3nshWV8/qQD/RxdMyuY9uzxfwU4NL3xynbBTdNrATjPdXnMrIDacwPXQpInbtku2Ly1gdtmLeTUtw9jv0F7FDocMytj7dnjfxV4UtKDNL+B62eZRdUNPfTMElas38IFrstjZgXWnsT/evrqlb6sEyZPq+WAof1494FDCx2KmZW59tzA9b2uCKQ7e3bxaua8vop///A7XJfHzAqurRu4fh4RX5V0P81v4AJcq6cjJk9bwB49K/nE0a7LY2aF19Ye/5T070+7IpDuatWGLdw79w0+ftQIBu7Rs9DhmJm1eefu7PSva/XsgjtqFrF5a6NP6ppZ0djp5ZySDpZ0p6TnJb3a9GrvAiRVSpoj6YG0/0ZJr0mam77G7soHKGaNjcHUGbUcM3ov3j7cpY3MrDi05zr+G4BfA1uBU0geuTi1A8v4CvBC3rBvRMTY9DW3A/MqKX+aX0ftig1McBVOMysi7Un8e0TEY4AiojYirgQ+1J6ZSxqRjntt50MsXVOm1TK0f29OP8wPMDOz4tGexL9ZUgUwX9Llks4C+rdz/j8Hvsn2Gj9NfiBpnqSJknq3NKGkyyTVSKqpq6tr5+KKx8K3NvDES8v59LiR9OrRntVsZtY12pORvgL0Bb4MHA2cD1y4s4kkfRhY3nSSOMe3gTHAMcBg4FstTR8R10REdURUV1WV3sPIp06vpULi08f6pK6ZFZc2b+BKyzGfHRFfB9YBn+nAvN8NfFTSGUAfYE9JUyPi/PT9zZJuAL7eibiL2qb6Bm6rWchp7xjGPgP7FDocM7NmWt3jl9QjIhqAEzoz44j4dkSMiIjRwDnA4xFxvqTh6fwFfAx4tjPzL2b3P/0GqzbUM8GXcJpZEWprj38mcBQwR9J9wB3A+qY3I+LuTi7zJklVJI9wnAt8vpPzKVpTptdy8N79Of6AIYUOxcxsB+0p0tYHWAG8l6R0g9K/7U78EfEk8GTa/d6OBllK5i5cxbxFq/n+mYeRHNSYmRWXthL/3pK+RtIU05Twm+xQu8cSk6ctoF+vSs46cr9Ch2Jm1qK2En8lyWWbLe22OvG34K31W3hg3hLOrh7JgD6uy2NmxamtxL8kIr7fZZF0A7fNWsiWrY0+qWtmRa2t6/jdQN0BDY3B1Om1HHfAYA4ZNqDQ4ZiZtaqtxH9ql0XRDTzx4nIWr9rIBa7LY2ZFrtXEHxFvdWUgpW7y9FqG7dmb979jWKFDMTNrk4vI7AavvbmeP/+jjk+PG0XPSq9SMytuzlK7wdTptfSoEOeOG1noUMzMdsqJfxdt3NLAHTULOf3wfdh7T9flMbPi58S/i+6du5g1m7b6pK6ZlQwn/l0QEUyeVsuYfQZwzOi9Ch2OmVm7OPHvgr+/vpLnl6xhwvGjXJfHzEqGE/8umDytlgG9e/Cxsa7LY2alw4m/k+rWbuahZ5bwiaNH0K93e4qcmpkVByf+Trpt1uvUN4Tr8phZyXHi74StDY3cNON1TjhoKAdWtfe582ZmxSHzxC+pUtIcSQ+k/ftLmiHpZUm3SeqVdQy72x9fWM6S1Zu8t29mJakr9vi/AryQ0/8jYGJEHASsBC7pghh2qynTF7DvwD6cOmbvQodiZtZhmSZ+SSOADwHXpv0ieYTjnekok0geuF4yXl6+jr++vILzjhtFD9flMbMSlHXm+jnwTaAx7R8CrIqIrWn/IqCkroWcOr2WXpUVnHOM6/KYWWnKLPFL+jCwPCJmd3L6yyTVSKqpq6vbzdF1zvrNW7lr9iI+9K7hDOnfu9DhmJl1SpZ7/O8GPippAXArSRPPL4BBkpoufB8BLG5p4oi4JiKqI6K6qqoqwzDb7545i1m7eatP6ppZScss8UfEtyNiRESMBs4BHo+I84AngE+mo10I3JtVDLtTRDBlWi2H77cnR44cVOhwzMw6rRBnJ78FfE3SyyRt/tcVIIYOm/naW7y0bC0XHDfadXnMrKR1Sa2BiHgSeDLtfhUY1xXL3Z0mT69l4B49+cgR+xY6FDOzXeLrEdth+ZpN/OHZpYyvHsEevSoLHY6Z2S5x4m+Hm2e+TkME5x/nk7pmVvqc+HeivqGRm2e8zkmHVDFqSL9Ch2Nmtsuc+HfiD88tZfnazVzgSzjNrJtw4t+JydNqGTl4D046xHV5zKx7cOJvw4tL1zDztbc4/9hRVFb4Ek4z6x6c+NswZVotvXtUML7adXnMrPtw4m/Fmk313DNnMR85Yl/26ldyjwwwM2uVE38r7p69iA1bGnxS18y6HSf+FkQEU6bXcsTIQbxrhOvymFn34sTfgr+9soJX6tZzgW/YMrNuyIm/BZOnLWBwv1586F3DCx2Kmdlu58Sf541VG3n0+WWMrx5Jn56uy2Nm3Y8Tf56bZ7xOAOcd+7ZCh2Jmlgkn/hybtzZw66zXOXXM3owc3LfQ4ZiZZcKJP8fDzy7lzXVbmHD86EKHYmaWGSf+HJOn1TJ6SF/ec9DQQodiZpaZzBK/pD6SZkp6WtJzkr6XDr9R0muS5qavsVnF0BHPvbGa2bUrOf+4UVS4Lo+ZdWNZPnpxM/DeiFgnqSfwlKTfp+99IyLuzJykpY4AAAyySURBVHDZHTZlWi19elbwqaNdl8fMurfMEn9EBLAu7e2ZviKr5e2K1Rvq+d3cxXxs7H4M7Nuz0OGYmWUq0zZ+SZWS5gLLgUcjYkb61g8kzZM0UVLvVqa9TFKNpJq6urosw+SO2QvZVN/IBNflMbMykGnij4iGiBgLjADGSToc+DYwBjgGGAx8q5Vpr4mI6oiorqqqyizGxsZg6vRajh61F4ftOzCz5ZiZFYsuuaonIlYBTwCnR8SSSGwGbgDGdUUMrfnLy2+yYMUGV+E0s7KR5VU9VZIGpd17AO8HXpQ0PB0m4GPAs1nF0B5Tpi1gaP9enH74PoUMw8ysy2R5Vc9wYJKkSpINzO0R8YCkxyVVAQLmAp/PMIY2LXxrA4+9uJx/PvkgevdwXR4zKw9ZXtUzDziyheHvzWqZHXXTjNcR8GnX5TGzMlK2d+5uqm/gtlmv8/53DGPfQXsUOhwzsy5Tton/wXlLWLmhngtcl8fMykzZJv7J02s5sKof/3TgkEKHYmbWpcoy8c9btIqnF65iwnGjSC4uMjMrH2WZ+CdPq6Vvr0o+fvSIQodiZtblyi7xr1y/hfuffoOzjtyPPfu4Lo+ZlZ+yS/y31yxk89ZGn9Q1s7JVVom/oTGYOqOWcfsP5tB9BhQ6HDOzgiirxP+nfyxn4VsbXZfHzMpaWSX+ydNq2XtAbz5wmOvymFn5KpvEX7tiPX/6Rx3njnsbPSvL5mObme2gbDLg1Om1VEquy2NmZa8sEv/GLQ3cXrOIDxy2D8P27FPocMzMCqosEv/9T7/B6o31frSimRllkPgjgsnTF3DIsP4cu//gQodjZlZw3T7xz1m4imcXr2HC8aNdl8fMjGwfvdhH0kxJT0t6TtL30uH7S5oh6WVJt0nqlVUMAFOm1dK/dw/OOnK/LBdjZlYystzj3wy8NyKOAMYCp0s6DvgRMDEiDgJWApdkFcCb6zbz4LwlfOKo/ejfO8unTJqZlY7MEn8k1qW9PdNXAO8F7kyHTyJ54Hombpu1kC0NjT6pa2aWI9M2fkmVkuYCy4FHgVeAVRGxNR1lEdBiG4ykyyTVSKqpq6vr1PKrBvRmfPUIDtrbdXnMzJpkmvgjoiEixgIjgHHAmA5Me01EVEdEdVVVVaeWP756JD/+5BGdmtbMrLvqkqt6ImIV8ARwPDBIUlOD+whgcVfEYGZmiSyv6qmSNCjt3gN4P/ACyQbgk+loFwL3ZhWDmZntKMtLXYYDkyRVkmxgbo+IByQ9D9wq6SpgDnBdhjGYmVmezBJ/RMwDjmxh+Ksk7f1mZlYA3f7OXTMza86J38yszDjxm5mVGSd+M7Myo4godAw7JakOqO3k5EOBN3djOFkrpXhLKVYorXhLKVYorXhLKVbYtXhHRcQOd8CWROLfFZJqIqK60HG0VynFW0qxQmnFW0qxQmnFW0qxQjbxuqnHzKzMOPGbmZWZckj81xQ6gA4qpXhLKVYorXhLKVYorXhLKVbIIN5u38ZvZmbNlcMev5mZ5XDiNzMrM90q8UsaKekJSc+nD3j/Sjr8SkmLJc1NX2cUOlYASQskPZPGVJMOGyzpUUnz0797FTpOAEmH5qy/uZLWSPpqsaxbSddLWi7p2ZxhLa5LJX4p6WVJ8yQdVSTx/kTSi2lM9+SUNR8taWPOOr66CGJt9XuX9O103b4k6QNdGWsb8d6WE+uC9MmAxbBuW8tZ2f52I6LbvEhKQR+Vdg8A/gG8A7gS+Hqh42sh3gXA0LxhPwauSLuvAH5U6DhbiLsSWAqMKpZ1C5wIHAU8u7N1CZwB/B4QcBwwo0jiPQ3okXb/KCfe0bnjFUmsLX7v6f/b00BvYH+Sx61WFjrevPf/C/j3Ilm3reWsTH+73WqPPyKWRMTf0+61JA9+afGZvkXsTJKH0EPGD6PfBacCr0REZ++m3u0i4s/AW3mDW1uXZwKTIzGd5Klww7sm0kRL8UbEI7H9edTTSZ5QV3CtrNvWnAncGhGbI+I14GW6uAx7W/FKEjAeuKUrY2pNGzkr099ut0r8uSSNJnkewIx00OXpodH1xdJ8AgTwiKTZki5Lhw2LiCVp91JgWGFCa9M5NP/HKcZ1C62vy/2AhTnjLaL4dhAuJtmza7K/pDmS/iTpPYUKKk9L33uxr9v3AMsiYn7OsKJYt3k5K9PfbrdM/JL6A3cBX42INcCvgQOBscASkkO9YnBCRBwFfBD4Z0kn5r4ZybFdUV1vK6kX8FHgjnRQsa7bZopxXbZG0neArcBN6aAlwNsi4kjga8DNkvYsVHypkvjeW3AuzXdaimLdtpCztsnit9vtEr+kniQr8KaIuBsgIpZFRENENAK/pUieABYRi9O/y4F7SOJa1nTolv5dXrgIW/RB4O8RsQyKd92mWluXi4GROeONSIcVnKSLgA8D56X/8KTNJivS7tkk7eaHFCxI2vzei3nd9gA+DtzWNKwY1m1LOYuMf7vdKvGn7XfXAS9ExM9yhue2gZ0FPJs/bVeT1E/SgKZukhN7zwL3kTyEHorzYfTN9piKcd3maG1d3gdckF4hcRywOuewumAknQ58E/hoRGzIGV6l5NnVSDoAOBh4tTBRboupte/9PuAcSb0l7U8S68yujq8V7wNejIhFTQMKvW5by1lk/dst1NnsLF7ACSSHRPOAuenrDGAK8Ew6/D5geBHEegDJ1Q9PA88B30mHDwEeA+YDfwQGFzrWnJj7ASuAgTnDimLdkmyMlgD1JO2el7S2LkmuiPgVyd7dM0B1kcT7Mkn7bdNv9+p03E+kv5G5wN+BjxRBrK1+78B30nX7EvDBYli36fAbgc/njVvoddtazsr0t+uSDWZmZaZbNfWYmdnOOfGbmZUZJ34zszLjxG9mVmac+M3MyowTv3U5SUNyqiEuzavy2Gsn01ZL+mU7lvG33RRrX0k3Kami+qykp9K7LDMjaV0Hx784jW9eGuOZ6fDvS3pfNlFaKfPlnFZQkq4E1kXET3OG9YjtxcoKStK3gaqI+FrafyiwICI2Z7jMdRHRro2LpBHAn0gqPK5ON0pVkRRIM2uR9/itKEi6UdLVkmYAP5Y0TtK0tHjW39KEi6STJT2Qdl+ZFgh7UtKrkr6cM791OeM/KelOJbXub0rvlkTSGemw2UpqnD/QQmjDybklPiJeakr6kn6XTvucthfZQ9I6JbX1n5P0x/SzNMX40XSciyTdmw6fL+m7rayXb0iale7Nf6+FUfYG1gLr0vjWNSX9dJ1+Mj1KajqiekZSpO8fKOnh9DP8RdKY9n1bVup6FDoAsxwjgH+KiAYlhbLeExFb0+aK/yS5yzLfGOAUklrmL0n6dUTU541zJHAY8AbwV+DdSh588xvgxIh4TVJrZXqvJ6mg+kmSOyknxfbKjhdHxFuS9gBmSborkrov/YDHI+Ibku4BrgLeT1JnfRLJna6Q1Lc5HNiQTv9gRNQ0LVjSaSQlBMaR3LF5n6QTIyk73ORpYBnwmqTHgLsj4v7cD5DOc2w6z58AD6dvXUNyJ+t8SccC/wu8t5X1YN2IE78VkzsioiHtHghMknQwyS3tPVuZ5sF0D3yzpOUk5WsX5Y0zM9L6LEqevDSaZA/51ZwmkVuAy/KmIyLmpjVcTiOp9TJL0vER8QLwZUlnpaOOJEnSK4AtbE+uzwCbI6Je0jPpsps8mm4okHQ3ye37NTnvn5a+5qT9/dNlbEv86UbydOAYkuckTJR0dERcmf9ZJJ1N8oCS09ImoX8C7kgPgCB5eIqVASd+Kybrc7r/A3giIs5SUqf8yVamyW1rb6Dl33R7xmlVRKwD7gbultQInCFpGMmG4PiI2CDpSaBPOkl9bD951ti0/IhoVFIhctus8xeV1y/g/0XEb3YSX5AUQpsp6VHgBpInZG2fkXR4OuzEdGNRAayKiLFtfnjrltzGb8VqINvb1i/KYP4vAQekGxWAs1saSdK7tf15p71Immtq0/hWpkl/DMlj8Drq/UqerboHyROW/pr3/h+Ai5uuIpK0n6S98+LbV82fuzo2jS93nEEkRzQXREQdQCQ131+T9Kl0HEk6ohOfwUqQ9/itWP2YpKnn34AHd/fMI2KjpC8CD0taD8xqZdQDgV+nJ4Qr0ljuAnoBn5f0AslGZHonwpiZzmsEMDW3fT+N8RFJbwempc0x64Dzaf6Mhp7ATyXtC2wC6oDP5y3nTJLnI/+2qVkn3dM/L/1s/5bO51aScwbWzflyTitbkvpHxLo0qf8KmB8RE7to2ReRlNS9vCuWZ5bLTT1Wzj6bnux9jqTpps22dLPuwnv8ZmZlxnv8ZmZlxonfzKzMOPGbmZUZJ34zszLjxG9mVmb+P31lXKmBP5WNAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "training_errors = (np.array(training_errors)).reshape(-1,1)\n",
        "testing_errors = (np.array(testing_errors)).reshape(-1,1)\n",
        "\n",
        "plt.plot(sample_sizes, training_errors)\n",
        "plt.title(\"Training MSE vs Training Sample Size\")\n",
        "plt.xlabel(\"Training Sample Size\")\n",
        "plt.ylabel(\"Training MSE\")\n",
        "# plt.legend([\"train\", \"validation\"])\n",
        "plt.show()"
      ],
      "id": "y0s1nHM4DXka"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "3asW2FhcDeHq",
        "outputId": "cdbc17d1-c90c-46cf-a5a8-fc347ae99371"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(sample_sizes, testing_errors_log)\n",
        "plt.title(\"Testing MSE vs Training Sample Size\")\n",
        "plt.xlabel(\"Training Sample Size\")\n",
        "plt.ylabel(\"log10 Testing MSE\")\n",
        "# plt.legend([\"train\", \"validation\"])\n",
        "plt.show()"
      ],
      "id": "3asW2FhcDeHq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2sYsdZw1svN"
      },
      "source": [
        "# Applying PCA"
      ],
      "id": "o2sYsdZw1svN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WPyD72MEMp4"
      },
      "source": [
        "##Initializing PCA Functions"
      ],
      "id": "0WPyD72MEMp4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "463FeRVv1vzZ"
      },
      "outputs": [],
      "source": [
        "# PCA algorithm with Eigenvalue decomposition for Question 1 and Question 2\n",
        "def EVDpca(x, A): # input parameter x should be a numpy array. DO NOT use dataframe for input x\n",
        "    n,k = np.shape(x)\n",
        "    D,P = np.linalg.eig(np.dot(x.T,x))\n",
        "    index = np.argsort(-D) # indices in descending order\n",
        "    p = np.zeros((k,A))\n",
        "    t = np.zeros((n,A))\n",
        "    R2_comp = np.zeros(A)\n",
        "    R2 = np.zeros(A)\n",
        "    for a in range(A):\n",
        "        p[:,a] = P[:,index[a]] \n",
        "        t[:,a] = np.dot(x,p[:,a])\n",
        "        R2_comp[a] = D[index[a]]/sum(D)\n",
        "        R2[a] = np.sum(D[index[0:a+1]])/sum(D)\n",
        "   \n",
        "    return t,p,R2_comp,R2\n",
        "\n",
        "# Function that can be used to plot the loadings for Question 2\n",
        "def loading_plot(p,Dataset):\n",
        "    s = np.arange(p.shape[0])\n",
        "    fig = plt.figure(figsize = (10,6))\n",
        "    ax = fig.add_axes([0,0,1,1])\n",
        "    ax.bar(s + 0.00, P[:,0], color = 'b', width = 0.25)\n",
        "    ax.bar(s + 0.25, P[:,1], color = 'g', width = 0.25)\n",
        "    ax.set_xticks(s)\n",
        "    ax.set_xticklabels( Dataset.columns[:p.shape[0]] )\n",
        "    ax.legend(labels=['p1', 'p2'])\n",
        "\n",
        "# function that can be used to plot the scores with CI for Question 2\n",
        "def scores_plot_CI(t):\n",
        "    import scipy.stats\n",
        "\n",
        "    t1 = t[:,0]\n",
        "    t2 = t[:,1]\n",
        "    plt.figure(figsize=(16, 16))\n",
        "    plt.axis([-2*np.amax(abs(t)), 2*np.amax(abs(t)),-1.5*np.amax(abs(t)),1.5*np.amax(abs(t))])\n",
        "    plt.plot([-np.amax(abs(t)), np.amax(abs(t))], [0, 0],'k-',linewidth = 2)\n",
        "    plt.plot([0, 0], [-np.amax(abs(t)), np.amax(abs(t))],'k-',LineWidth = 2)\n",
        "    plt.xlabel('First component')\n",
        "    plt.ylabel('Second component')\n",
        "    plt.grid()\n",
        "\n",
        "    # Plot the T2 elipses\n",
        "    N = t.shape[0];\n",
        "    a = np.square(np.std(t[:,0],ddof = 1));\n",
        "    b = np.square(np.std(t[:,1],ddof = 1));\n",
        "    # calculate T2 limits\n",
        "    A = 2;\n",
        "    Flim95 = scipy.stats.f.ppf(0.95,A,(N-A));\n",
        "    Flim99 = scipy.stats.f.ppf(0.99,A,(N-A));\n",
        "    T2lim95 = ((N-1)*(N+1)*A*Flim95)/(N*(N-A));\n",
        "    T2lim99 = ((N-1)*(N+1)*A*Flim99)/(N*(N-A));\n",
        "\n",
        "    # calculate elipse distances\n",
        "    theta = np.linspace(0,2*np.pi,50);\n",
        "    x95 = np.sqrt(a*T2lim95)*np.cos(theta);\n",
        "    y95 = np.sqrt(b*T2lim95)*np.sin(theta);\n",
        "    x99 = np.sqrt(a*T2lim99)*np.cos(theta);\n",
        "    y99 = np.sqrt(b*T2lim99)*np.sin(theta);\n",
        "\n",
        "    #  plot elipse\n",
        "    plt.plot(x95, y95, '--r')\n",
        "    plt.plot(x99, y99, '-r')\n",
        "\n",
        "    if max(x99) > max(abs(t1)):\n",
        "        plt.plot([-max(x99)*1.25, max(x99)*1.25], [0, 0],'k-',linewidth = 2)\n",
        "        xlim = ([-max(x99)*1.25, max(x99)*1.25])\n",
        "    else:\n",
        "        plt.plot([-max(abs(t1))*1.25, max(abs(t1))*1.25], [0, 0],'k-',linewidth = 2)\n",
        "        xlim = ([-max(abs(t1))*1.25, max(abs(t1))*1.25])    \n",
        "    \n",
        "    if max(y99) > max(abs(t2)):\n",
        "        plt.plot([0, 0], [-max(y99)*1.25, max(y99)*1.25],'k-',linewidth = 2)\n",
        "        ylim = ([-max(y99)*1.25, max(y99)*1.25])\n",
        "    else:\n",
        "        plt.plot([0, 0],[-max(abs(t2))*1.25, max(abs(t2))*1.25],'k-',linewidth = 2)\n",
        "        ylim = ([-max(abs(t2))*1.25, max(abs(t2))*1.25]) \n",
        "\n",
        "\n",
        "    plt.plot(t[:,0],t[:,1],'ko', markersize = 20)\n",
        "\n",
        "# Function that can be used to plot scores and loadings for Question 2\n",
        "def scores_loading_plot(t,p,Dataset):\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.axis([-np.amax(abs(t)), np.amax(abs(t)),-np.amax(abs(t)),np.amax(abs(t))])\n",
        "    plt.plot([-np.amax(abs(t)), np.amax(abs(t))], [0, 0],'k-',linewidth = 2)\n",
        "    plt.plot([0, 0], [-np.amax(abs(t)), np.amax(abs(t))],'k-',LineWidth = 2)\n",
        "    plt.xlabel('First component')\n",
        "    plt.ylabel('Second component')\n",
        "    plt.grid()\n",
        "\n",
        "\n",
        "    for j in range(p.shape[0]):\n",
        "        plt.quiver(0,0,1.5*p[j,0],1.5*p[j,1],scale=2,color = 'r')\n",
        "        plt.text(10*p[j,0],7*p[j,1],Dataset.columns[j] ,fontsize = 16,color = 'r');\n",
        "    plt.plot(t[:,0],t[:,1],'ko', markersize = 20)\n",
        "\n",
        "# Function that plot the SPE with CI for Question 2\n",
        "def SPEplot(res):\n",
        "    from scipy.stats.distributions import chi2\n",
        "\n",
        "    N = res.shape[0]\n",
        "    SE = np.square(res)\n",
        "    SPE = np.sum(SE,1)\n",
        "\n",
        "    # this part used to compute Confidence Intervel (do not worry about it)\n",
        "    v = np.square(np.std(SPE,ddof = 1))\n",
        "    m = np.mean(SPE)\n",
        "    df = (2*np.square(m))/v\n",
        "\n",
        "    SPE95lim = (v/(2*m)*chi2.ppf(0.95, df=df))\n",
        "    SPE99lim = (v/(2*m)*chi2.ppf(0.99, df=df))\n",
        "\n",
        "    y95 = np.ones((1,N))*SPE95lim\n",
        "    y99 = np.ones((1,N))*SPE99lim\n",
        "    # end of the part used to compute Confidence Intervel\n",
        "\n",
        "    fig = plt.figure()\n",
        "    x = range(N)\n",
        "    plt.plot(x, SPE, 'ko-',label=\"SPE Values\")\n",
        "    plt.plot(x, y95.T, '--r',label=\"95% Limit\")\n",
        "    plt.plot(x, y99.T, '-r',label=\"99% Limit\")\n",
        "    plt.xlabel('Observation')\n",
        "    plt.ylabel('Squared Prediction Error')\n",
        "    plt.legend(loc=\"upper left\")\n",
        "\n",
        "# Function that plot the T2 with CI for Question 2\n",
        "\n",
        "def T2plot(t,A):\n",
        "    import scipy.stats\n",
        "\n",
        "    N = t.shape[0]\n",
        "    tn = t[:,0:A]\n",
        "    t1 = tn/np.std(tn,axis = 0,ddof = 1)\n",
        "    tsq = np.square(t1)\n",
        "    T2 = np.sum(tsq,1)\n",
        "    \n",
        "    # this part used to compute Confidence Intervel (do not worry about it)\n",
        "    Flim95 = scipy.stats.f.ppf(0.95,A,(N-A));\n",
        "    Flim99 = scipy.stats.f.ppf(0.99,A,(N-A));\n",
        "    T2lim95 = ((N-1)*(N+1)*A*Flim95)/(N*(N-A));\n",
        "    T2lim99 = ((N-1)*(N+1)*A*Flim99)/(N*(N-A));\n",
        "\n",
        "    y95 = np.ones((1,N))*T2lim95\n",
        "    y99 = np.ones((1,N))*T2lim99\n",
        "    # end of the part used to compute Confidence Intervel\n",
        "\n",
        "    fig = plt.figure()\n",
        "    x = range(N)\n",
        "    plt.plot(x, T2, 'ko-',label=\"$T^{2}$ Values\")\n",
        "    plt.plot(x, y95.T, '--r',label=\"95% Limit\")\n",
        "    plt.plot(x, y99.T, '-r',label=\"99% Limit\")\n",
        "    plt.xlabel('Observation')\n",
        "    plt.ylabel('Hoteling $T^{2}$')\n",
        "    plt.legend(loc=\"upper left\")"
      ],
      "id": "463FeRVv1vzZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6BLnZy82DPm"
      },
      "source": [
        "## Determining Optimal A-Val"
      ],
      "id": "E6BLnZy82DPm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CxeyuJ6ETVA",
        "outputId": "7da12071-bb7a-45c7-e983-9654f6cb1fcc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# P4: Apply PCA with different number of principal components and plot R2 training and R2 testing versus number of component\n",
        "\n",
        "R2_test_vals = []\n",
        "R2_train_vals = []\n",
        "#A_range = [1, 10, 20, 30, 40, 50, 60, 70, 80]\n",
        "A_range = list(np.arange(1, 26, 1))\n",
        "a_diffs = np.empty(shape=(26,2))\n",
        "\n",
        "for i in range(len(A_range)):\n",
        "    t_test, p_test, R2_comp_test, R2_test = EVDpca(X_test_stand, A_range[i])\n",
        "    R2_test_vals.append(R2_test)\n",
        "    \n",
        "    t_train, p_train, R2_comp_train, R2_train = EVDpca(X_train_stand, A_range[i])\n",
        "    R2_train_vals.append(R2_train)\n",
        "\n",
        "    if i > 0:\n",
        "      a_diffs[i-1,0] = (R2_test_vals[-1][len(R2_test_vals)-1] - R2_test_vals[-1][len(R2_test_vals)-2])*100\n",
        "      a_diffs[i-1,1] = i"
      ],
      "id": "_CxeyuJ6ETVA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiZYX7STGYnS",
        "outputId": "72e55704-e259-4d6b-b7ae-1044a721934d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[20.27329663,  1.        ],\n",
              "       [15.58809577,  2.        ],\n",
              "       [10.20446846,  3.        ],\n",
              "       [ 7.58226283,  4.        ],\n",
              "       [ 6.02737618,  5.        ],\n",
              "       [ 3.63484128,  6.        ],\n",
              "       [ 2.95117104,  7.        ],\n",
              "       [ 2.40038069,  8.        ],\n",
              "       [ 1.83462097,  9.        ],\n",
              "       [ 1.55728198, 10.        ],\n",
              "       [ 1.08923168, 11.        ],\n",
              "       [ 0.95877483, 12.        ],\n",
              "       [ 0.71687067, 13.        ],\n",
              "       [ 0.53567484, 14.        ],\n",
              "       [ 0.45370433, 15.        ],\n",
              "       [ 0.34556014, 16.        ],\n",
              "       [ 0.28396998, 17.        ],\n",
              "       [ 0.21693282, 18.        ],\n",
              "       [ 0.15215263, 19.        ],\n",
              "       [ 0.11873696, 20.        ],\n",
              "       [ 0.1117759 , 21.        ],\n",
              "       [ 0.09363922, 22.        ],\n",
              "       [ 0.07764056, 23.        ],\n",
              "       [ 0.07200239, 24.        ],\n",
              "       [ 0.09999855,  0.06476012],\n",
              "       [ 0.0599647 ,  0.03504814]])"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a_diffs"
      ],
      "id": "aiZYX7STGYnS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raxE6kFrGIto"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "raxE6kFrGIto"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niwB4PGlHiwJ"
      },
      "source": [
        "### 11 Components Optimal"
      ],
      "id": "niwB4PGlHiwJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_50mXAsHnHw"
      },
      "source": [
        "## PCA Linear Regression"
      ],
      "id": "4_50mXAsHnHw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHeQiyIJHvw8"
      },
      "source": [
        "### Re-load data and scale - Reduce Dimensions"
      ],
      "id": "vHeQiyIJHvw8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Umpc4M-THklv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.get_option(\"display.max_columns\")\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_csv('/content/drive/My Drive/4ML3/Project/train.csv')\n",
        "dataset_val = dataset.values\n",
        "\n",
        "x = dataset_val[:,0:(dataset_val.shape[1])-1]\n",
        "y = dataset_val[:,-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=69420)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_x = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "scaler_x.fit(X_train)\n",
        "scaler_y.fit(y_train)\n",
        "\n",
        "X_train_stand = scaler_x.transform(X_train)\n",
        "X_test_stand = scaler_x.transform(X_test)\n",
        "y_train_stand = scaler_y.transform(y_train)\n",
        "y_test_stand = scaler_y.transform(y_test)\n",
        "\n",
        "A_value = 11\n",
        "\n",
        "t_train, p_train, R2_comp_train, R2_train = EVDpca(X_train_stand, 11)\n",
        "t_test, p_test, R2_comp_test, R2_test = EVDpca(X_test_stand, 11)"
      ],
      "id": "Umpc4M-THklv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqTzTE94SJBH",
        "outputId": "15c8e85f-cedd-430f-9e69-cec71422eb30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6379, 11)"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t_test.shape"
      ],
      "id": "YqTzTE94SJBH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXmVrgYeM934"
      },
      "source": [
        "### Train LR with Reduced Dimensions"
      ],
      "id": "RXmVrgYeM934"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6alhHq1VOqR7",
        "outputId": "dd774150-4eca-4839-fa2f-ccc2272613c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Error: [[48.69071435]]\n",
            "Testing Error: [[52.78950471]]\n",
            "LR Intercept: [-2.99366174e-15]\n"
          ]
        }
      ],
      "source": [
        "# Train LR model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(t_train, y_train_stand)\n",
        "\n",
        "# compute errors\n",
        "y_train_pred = lin_reg.predict(t_train)\n",
        "y_test_pred = lin_reg.predict(t_test)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "training_error = np.array(MSE(y_train_stand,y_train_pred)).reshape(-1,1)\n",
        "testing_error = np.array(MSE(y_test_stand,y_test_pred)).reshape(-1,1)\n",
        "print(\"Training Error: {}\".format(scaler_y.inverse_transform(training_error)))\n",
        "print(\"Testing Error: {}\".format(scaler_y.inverse_transform(testing_error)))\n",
        "\n",
        "# Show the weights\n",
        "print(\"LR Intercept: {}\".format(lin_reg.intercept_))\n",
        "#print(\"LR Model Weights: {}\".format(lin_reg.coef_))\n",
        "col_labels = columns=list(dataset.columns)[:-1]\n",
        "df = pd.DataFrame(data=lin_reg.coef_)"
      ],
      "id": "6alhHq1VOqR7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "G7ltlNfuR5zg",
        "outputId": "a48ada39-477d-4e42-e52f-51c1bc091e50"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.117269</td>\n",
              "      <td>-0.057084</td>\n",
              "      <td>-0.070873</td>\n",
              "      <td>0.037332</td>\n",
              "      <td>0.073036</td>\n",
              "      <td>0.103836</td>\n",
              "      <td>-0.000592</td>\n",
              "      <td>-0.016741</td>\n",
              "      <td>-0.052068</td>\n",
              "      <td>0.003653</td>\n",
              "      <td>-0.09146</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...        8         9        10\n",
              "0  0.117269 -0.057084 -0.070873  ... -0.052068  0.003653 -0.09146\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ],
      "id": "G7ltlNfuR5zg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORZxj3OMSZN8"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "ORZxj3OMSZN8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdx1V0fxSfZm"
      },
      "source": [
        "### Time taken test"
      ],
      "id": "vdx1V0fxSfZm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oDucx8sTLZw",
        "outputId": "5e5f2f3e-743f-4405-9c7a-28ed3e2ef422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.1\n",
            "time: 205 µs (started: 2021-12-07 19:09:15 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "id": "9oDucx8sTLZw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQQi8pPnSgZZ",
        "outputId": "469daf38-653a-4da8-e3b0-3aac6d4932d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Error: [[48.69071435]]\n",
            "Testing Error: [[52.78950471]]\n",
            "LR Intercept: [-2.99366174e-15]\n",
            "time: 542 ms (started: 2021-12-07 19:11:17 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.get_option(\"display.max_columns\")\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_csv('/content/drive/My Drive/4ML3/Project/train.csv')\n",
        "dataset_val = dataset.values\n",
        "\n",
        "x = dataset_val[:,0:(dataset_val.shape[1])-1]\n",
        "y = dataset_val[:,-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=69420)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_x = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "scaler_x.fit(X_train)\n",
        "scaler_y.fit(y_train)\n",
        "\n",
        "X_train_stand = scaler_x.transform(X_train)\n",
        "X_test_stand = scaler_x.transform(X_test)\n",
        "y_train_stand = scaler_y.transform(y_train)\n",
        "y_test_stand = scaler_y.transform(y_test)\n",
        "\n",
        "A_value = 11\n",
        "\n",
        "t_train, p_train, R2_comp_train, R2_train = EVDpca(X_train_stand, 11)\n",
        "t_test, p_test, R2_comp_test, R2_test = EVDpca(X_test_stand, 11)\n",
        "\n",
        "# Train LR model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(t_train, y_train_stand)\n",
        "\n",
        "# compute errors\n",
        "y_train_pred = lin_reg.predict(t_train)\n",
        "y_test_pred = lin_reg.predict(t_test)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "training_error = np.array(MSE(y_train_stand,y_train_pred)).reshape(-1,1)\n",
        "testing_error = np.array(MSE(y_test_stand,y_test_pred)).reshape(-1,1)\n",
        "print(\"Training Error: {}\".format(scaler_y.inverse_transform(training_error)))\n",
        "print(\"Testing Error: {}\".format(scaler_y.inverse_transform(testing_error)))\n",
        "\n",
        "# Show the weights\n",
        "print(\"LR Intercept: {}\".format(lin_reg.intercept_))\n",
        "#print(\"LR Model Weights: {}\".format(lin_reg.coef_))\n",
        "col_labels = columns=list(dataset.columns)[:-1]\n",
        "df = pd.DataFrame(data=lin_reg.coef_)"
      ],
      "id": "GQQi8pPnSgZZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erkHFN3OUeQ_"
      },
      "source": [
        "## Apply L2 Regularization\n",
        "\n"
      ],
      "id": "erkHFN3OUeQ_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGY73wpBSqGI",
        "outputId": "50f16ef4-5ebe-4321-a6a7-3070c53047b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Penalty: 0.005\n",
            "Current C_val: 100.0\n",
            "Training Error: [[43.39874476]]\n",
            "Testing Error: [[43.62873436]]\n",
            "\n",
            "Current Penalty: 0.1\n",
            "Current C_val: 5.0\n",
            "Training Error: [[43.39920069]]\n",
            "Testing Error: [[43.62802383]]\n",
            "\n",
            "Current Penalty: 1\n",
            "Current C_val: 0.5\n",
            "Training Error: [[43.41270283]]\n",
            "Testing Error: [[43.63674645]]\n",
            "\n",
            "Current Penalty: 5\n",
            "Current C_val: 0.1\n",
            "Training Error: [[43.45354381]]\n",
            "Testing Error: [[43.66766257]]\n",
            "\n",
            "Current Penalty: 10\n",
            "Current C_val: 0.05\n",
            "Training Error: [[43.48888924]]\n",
            "Testing Error: [[43.69332737]]\n",
            "\n",
            "Current Penalty: 20\n",
            "Current C_val: 0.025\n",
            "Training Error: [[43.55201153]]\n",
            "Testing Error: [[43.74363448]]\n",
            "\n",
            "Current Penalty: 40\n",
            "Current C_val: 0.0125\n",
            "Training Error: [[43.66168707]]\n",
            "Testing Error: [[43.84142151]]\n",
            "\n",
            "Current Penalty: 60\n",
            "Current C_val: 0.008333333333333333\n",
            "Training Error: [[43.75232721]]\n",
            "Testing Error: [[43.9277828]]\n",
            "\n",
            "Current Penalty: 100\n",
            "Current C_val: 0.005\n",
            "Training Error: [[43.89359717]]\n",
            "Testing Error: [[44.06751075]]\n",
            "\n",
            "Current Penalty: 200\n",
            "Current C_val: 0.0025\n",
            "Training Error: [[44.12405442]]\n",
            "Testing Error: [[44.30029128]]\n",
            "\n",
            "Current Penalty: 400\n",
            "Current C_val: 0.00125\n",
            "Training Error: [[44.38899906]]\n",
            "Testing Error: [[44.56523846]]\n",
            "\n",
            "Current Penalty: 800\n",
            "Current C_val: 0.000625\n",
            "Training Error: [[44.70330454]]\n",
            "Testing Error: [[44.87059038]]\n",
            "\n",
            "Current Penalty: 1600\n",
            "Current C_val: 0.0003125\n",
            "Training Error: [[45.12025502]]\n",
            "Testing Error: [[45.27006706]]\n",
            "\n",
            "Current Penalty: 2000\n",
            "Current C_val: 0.00025\n",
            "Training Error: [[45.28712822]]\n",
            "Testing Error: [[45.43041485]]\n",
            "\n",
            "Current Penalty: 2500\n",
            "Current C_val: 0.0002\n",
            "Training Error: [[45.47221122]]\n",
            "Testing Error: [[45.60886454]]\n",
            "\n",
            "Current Penalty: 3000\n",
            "Current C_val: 0.00016666666666666666\n",
            "Training Error: [[45.63695112]]\n",
            "Testing Error: [[45.76818155]]\n",
            "\n",
            "Current Penalty: 4000\n",
            "Current C_val: 0.000125\n",
            "Training Error: [[45.92006119]]\n",
            "Testing Error: [[46.04276333]]\n",
            "\n",
            "Current Penalty: 5000\n",
            "Current C_val: 0.0001\n",
            "Training Error: [[46.15678731]]\n",
            "Testing Error: [[46.27282921]]\n",
            "\n",
            "Current Penalty: 6000\n",
            "Current C_val: 8.333333333333333e-05\n",
            "Training Error: [[46.35935067]]\n",
            "Testing Error: [[46.46983025]]\n",
            "\n",
            "Current Penalty: 7000\n",
            "Current C_val: 7.142857142857143e-05\n",
            "Training Error: [[46.53588491]]\n",
            "Testing Error: [[46.64151089]]\n",
            "\n",
            "Training Error: [[46.53588491]]\n",
            "Testing Error: [[46.64151089]]\n",
            "\n",
            "time: 728 ms (started: 2021-12-07 19:47:43 +00:00)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Ridge # uses L2 regularization by default with C = 1, alpha = 1/2C\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "penalties = [0.005,0.1, 1, 5, 10, 20, 40, 60, 100, 200, 400, 800, 1600, 2000, 2500, 3000, 4000, 5000, 6000, 7000]\n",
        "C_vals = []\n",
        "for penalty in penalties:\n",
        "  C_vals.append(1/(2*penalty))\n",
        "\n",
        "training_errors = []\n",
        "testing_errors = []\n",
        "\n",
        "for i in range(len(penalties)):\n",
        "\n",
        "  # define model\n",
        "  print(\"Current Penalty: {}\".format(penalties[i]))\n",
        "  print(\"Current C_val: {}\".format(C_vals[i]))\n",
        "\n",
        "  model = Ridge(alpha=penalties[i])\n",
        "  model.fit(X_train_stand,y_train_stand)\n",
        "\n",
        "  # compute errors\n",
        "  y_train_pred = model.predict(X_train_stand)\n",
        "  y_test_pred = model.predict(X_test_stand)\n",
        "\n",
        "  # Compute traditional MSE error\n",
        "  training_error = inverse_normalize(MSE(y_train_stand,y_train_pred), \"y\")\n",
        "  testing_error = inverse_normalize(MSE(y_test_stand,y_test_pred), \"y\")\n",
        "  print(\"Training Error: {}\".format(training_error))\n",
        "  print(\"Testing Error: {}\\n\".format(testing_error))\n",
        "\n",
        "  #append scores and store\n",
        "  training_errors.append(training_error)\n",
        "  testing_errors.append(testing_error)\n",
        "\n",
        "print(\"Training Error: {}\".format(training_error))\n",
        "print(\"Testing Error: {}\\n\".format(testing_error))"
      ],
      "id": "aGY73wpBSqGI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfxGuebcb1Ab",
        "outputId": "e1d7ec8b-45fc-43f8-ed27-0eb9c01d1205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The optimal C value: 5.0\n",
            "Testing Error: [[43.62802383]]\n",
            "time: 3.93 ms (started: 2021-12-07 19:47:46 +00:00)\n"
          ]
        }
      ],
      "source": [
        "print(\"The optimal C value: {}\\nTesting Error: {}\".format(C_vals[list(testing_errors).index(min(testing_errors))], testing_errors[list(testing_errors).index(min(testing_errors))]))"
      ],
      "id": "pfxGuebcb1Ab"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A_TkMljZ10l"
      },
      "source": [
        "### Plots"
      ],
      "id": "_A_TkMljZ10l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "3du9HPzKZmsZ",
        "outputId": "46075659-d856-4a84-8905-159d7b644c34"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 219 ms (started: 2021-12-07 19:47:47 +00:00)\n"
          ]
        }
      ],
      "source": [
        "testing_errors = (np.array(testing_errors)).reshape(-1,1)\n",
        "training_errors = (np.array(training_errors)).reshape(-1,1)\n",
        "plt.plot(C_vals, testing_errors)\n",
        "plt.plot(C_vals, training_errors)\n",
        "plt.title(\"Training and Testing MSE vs L2 Penalties\")\n",
        "plt.xlabel(\"L2 C Hyperparameter\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.legend([\"train\", \"test\"])\n",
        "plt.show()"
      ],
      "id": "3du9HPzKZmsZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "c_CM2T0XZn-w",
        "outputId": "a7dc00f0-d437-4a4b-c921-e6e88a584764"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 192 ms (started: 2021-12-07 19:47:49 +00:00)\n"
          ]
        }
      ],
      "source": [
        "testing_errors = (np.array(testing_errors)).reshape(-1,1)\n",
        "training_errors = (np.array(training_errors)).reshape(-1,1)\n",
        "plt.plot(penalties, testing_errors)\n",
        "plt.plot(penalties, training_errors)\n",
        "plt.title(\"Training and Testing MSE vs L2 Penalties\")\n",
        "plt.xlabel(\"L2 Penalties\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.legend([\"train\", \"test\"])\n",
        "plt.show()"
      ],
      "id": "c_CM2T0XZn-w"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__fTUVHldCvt"
      },
      "source": [
        "# Artificial Neural"
      ],
      "id": "__fTUVHldCvt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMgDzUtWfNYR"
      },
      "source": [
        "## Load and scale data"
      ],
      "id": "qMgDzUtWfNYR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azQ1VeeDfPWo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.get_option(\"display.max_columns\")\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_csv('/content/drive/My Drive/4ML3/Project/train.csv')\n",
        "dataset_val = dataset.values\n",
        "\n",
        "x = dataset_val[:,0:(dataset_val.shape[1])-1]\n",
        "y = dataset_val[:,-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=69420)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_x = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "scaler_x.fit(X_train)\n",
        "scaler_y.fit(y_train)\n",
        "\n",
        "X_train_stand = scaler_x.transform(X_train)\n",
        "X_test_stand = scaler_x.transform(X_test)\n",
        "y_train_stand = scaler_y.transform(y_train)\n",
        "y_test_stand = scaler_y.transform(y_test)\n",
        "\n",
        "# method to un-normalize the data back to critical temperatures in Kelvin\n",
        "def inverse_normalize(value, y_or_x):\n",
        "    new_val = np.array(value).reshape(-1,1)\n",
        "    \n",
        "    if y_or_x == \"x\":\n",
        "        scaler = scaler_x\n",
        "    else:\n",
        "        scaler = scaler_y\n",
        "    new_val = scaler.inverse_transform(new_val)\n",
        "    \n",
        "    return new_val"
      ],
      "id": "azQ1VeeDfPWo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPgixdG0fIse"
      },
      "source": [
        "## Setup Model with Dropout Rate Optimization"
      ],
      "id": "GPgixdG0fIse"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9lqbOsSHlHJ"
      },
      "source": [
        "### No Validation"
      ],
      "id": "Z9lqbOsSHlHJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKVLL_MBdEXc",
        "outputId": "f37c43b0-ca82-4e37-b5fd-5673a05b2e8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "466/466 [==============================] - 1s 3ms/step - loss: 0.0749 - mse: 0.0749\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.1175 - mse: 0.1175\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras import Input\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "train_scores = []\n",
        "vald_scores = []\n",
        "\n",
        "dropout_rates = [0, 0.05, 0.1, 0.25, 0.5]\n",
        "\n",
        "# Optimal dropout rate is 0.05\n",
        "model = models.Sequential() # define that we will use the sequential method to build the ANN\n",
        "model.add(Input(shape=(X_train_stand.shape[1],))) # Input layer (has cells equal to number of features)\n",
        "#model.add(layers.Dropout(0.05))\n",
        "model.add(layers.Dense(160, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "#model.add(layers.Dropout(0.05))\n",
        "model.add(layers.Dense(1,activation = 'linear',kernel_initializer='glorot_normal', bias_initializer='zeros')) # Output Layer\n",
        "model.compile(optimizer = Adam(lr=0.001), loss = 'mse', metrics = ['mse'])\n",
        "model.fit(X_train_stand,y_train_stand, epochs = 500, batch_size = 16,verbose = 1)\n",
        "\n",
        "# Evaluate model on training and testing data\n",
        "train_score = model.evaluate(X_train_stand, y_train_stand)\n",
        "vald_score = model.evaluate(X_test_stand, y_test_stand)\n",
        "\n",
        "#train_scores.append(train_score[1] * 100)\n",
        "#vald_scores.append(vald_score[1] * 100)"
      ],
      "id": "nKVLL_MBdEXc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh6uXBNKG2cN"
      },
      "source": [
        "#### Model Evaluation"
      ],
      "id": "sh6uXBNKG2cN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_pKQj__Hqm1",
        "outputId": "598a4b0a-d04d-4e3e-f185-80e8becd378f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Score: [130491.00762068]\n",
            "Test Score: [4479996.95465996]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_score = train_score[1]*100\n",
        "test_score = test_score[1]*100\n",
        "\n",
        "print(\"Train Score: {}\\nTest Score: {}\\n\".format(train_score, test_score))"
      ],
      "id": "0_pKQj__Hqm1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpu3R6b8A0ma"
      },
      "outputs": [],
      "source": [
        "model_loss = pd.DataFrame(model.history.history)\n",
        "model_loss.plot()"
      ],
      "id": "Qpu3R6b8A0ma"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "s2tS_PbDA8rZ",
        "outputId": "8afebc61-e1c3-4e81-c5c5-42e65bd182fe"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ce5ef9f2eac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    947\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mplot_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/plotting/_matplotlib/__init__.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(data, kind, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ax\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left_ax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mplot_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPLOT_CLASSES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/plotting/_matplotlib/core.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_plot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_subplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/plotting/_matplotlib/core.py\u001b[0m in \u001b[0;36m_compute_plot_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# no non-numeric frames or series allowed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no numeric data to plot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;31m# GH25587: cast ExtensionArray of pandas (IntegerArray, etc.) to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: no numeric data to plot"
          ]
        }
      ],
      "source": [
        "model_loss.plot()"
      ],
      "id": "s2tS_PbDA8rZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpE6FY2eG6go",
        "outputId": "444b87a0-431b-49cf-b5c9-6ff10677676a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 160)               13120     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 161       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,281\n",
            "Trainable params: 13,281\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ],
      "id": "dpE6FY2eG6go"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV5jWLAkG-CY"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "rV5jWLAkG-CY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_fOVso1IPBS"
      },
      "source": [
        "### Model with Validation"
      ],
      "id": "x_fOVso1IPBS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyGpNtmDIUHN",
        "outputId": "548de11f-33d6-44b1-b5e5-8a658e2fc50e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.2586 - mse: 0.2586 - val_loss: 0.2130 - val_mse: 0.2130\n",
            "Epoch 2/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.2068 - mse: 0.2068 - val_loss: 0.1972 - val_mse: 0.1972\n",
            "Epoch 3/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1904 - mse: 0.1904 - val_loss: 0.2006 - val_mse: 0.2006\n",
            "Epoch 4/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1853 - mse: 0.1853 - val_loss: 0.1862 - val_mse: 0.1862\n",
            "Epoch 5/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1791 - mse: 0.1791 - val_loss: 0.1847 - val_mse: 0.1847\n",
            "Epoch 6/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1751 - mse: 0.1751 - val_loss: 0.1815 - val_mse: 0.1815\n",
            "Epoch 7/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1698 - mse: 0.1698 - val_loss: 0.1830 - val_mse: 0.1830\n",
            "Epoch 8/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1683 - mse: 0.1683 - val_loss: 0.1825 - val_mse: 0.1825\n",
            "Epoch 9/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1645 - mse: 0.1645 - val_loss: 0.1797 - val_mse: 0.1797\n",
            "Epoch 10/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1646 - mse: 0.1646 - val_loss: 0.1748 - val_mse: 0.1748\n",
            "Epoch 11/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1622 - mse: 0.1622 - val_loss: 0.1697 - val_mse: 0.1697\n",
            "Epoch 12/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1607 - mse: 0.1607 - val_loss: 0.1619 - val_mse: 0.1619\n",
            "Epoch 13/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1570 - mse: 0.1570 - val_loss: 0.1706 - val_mse: 0.1706\n",
            "Epoch 14/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1574 - mse: 0.1574 - val_loss: 0.1638 - val_mse: 0.1638\n",
            "Epoch 15/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1565 - mse: 0.1565 - val_loss: 0.1662 - val_mse: 0.1662\n",
            "Epoch 16/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1525 - mse: 0.1525 - val_loss: 0.1701 - val_mse: 0.1701\n",
            "Epoch 17/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1518 - mse: 0.1518 - val_loss: 0.1782 - val_mse: 0.1782\n",
            "Epoch 18/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1506 - mse: 0.1506 - val_loss: 0.1721 - val_mse: 0.1721\n",
            "Epoch 19/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1505 - mse: 0.1505 - val_loss: 0.1786 - val_mse: 0.1786\n",
            "Epoch 20/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1494 - mse: 0.1494 - val_loss: 0.1648 - val_mse: 0.1648\n",
            "Epoch 21/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1483 - mse: 0.1483 - val_loss: 0.1687 - val_mse: 0.1687\n",
            "Epoch 22/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1474 - mse: 0.1474 - val_loss: 0.1585 - val_mse: 0.1585\n",
            "Epoch 23/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1467 - mse: 0.1467 - val_loss: 0.1565 - val_mse: 0.1565\n",
            "Epoch 24/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1429 - mse: 0.1429 - val_loss: 0.1609 - val_mse: 0.1609\n",
            "Epoch 25/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1428 - mse: 0.1428 - val_loss: 0.1611 - val_mse: 0.1611\n",
            "Epoch 26/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1425 - mse: 0.1425 - val_loss: 0.1515 - val_mse: 0.1515\n",
            "Epoch 27/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1422 - mse: 0.1422 - val_loss: 0.1584 - val_mse: 0.1584\n",
            "Epoch 28/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1396 - mse: 0.1396 - val_loss: 0.1634 - val_mse: 0.1634\n",
            "Epoch 29/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1411 - mse: 0.1411 - val_loss: 0.1520 - val_mse: 0.1520\n",
            "Epoch 30/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1394 - mse: 0.1394 - val_loss: 0.1598 - val_mse: 0.1598\n",
            "Epoch 31/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1363 - mse: 0.1363 - val_loss: 0.1536 - val_mse: 0.1536\n",
            "Epoch 32/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1360 - mse: 0.1360 - val_loss: 0.1563 - val_mse: 0.1563\n",
            "Epoch 33/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1358 - mse: 0.1358 - val_loss: 0.1489 - val_mse: 0.1489\n",
            "Epoch 34/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1350 - mse: 0.1350 - val_loss: 0.1513 - val_mse: 0.1513\n",
            "Epoch 35/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1353 - mse: 0.1353 - val_loss: 0.1540 - val_mse: 0.1540\n",
            "Epoch 36/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1328 - mse: 0.1328 - val_loss: 0.1542 - val_mse: 0.1542\n",
            "Epoch 37/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1342 - mse: 0.1342 - val_loss: 0.1438 - val_mse: 0.1438\n",
            "Epoch 38/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1324 - mse: 0.1324 - val_loss: 0.1698 - val_mse: 0.1698\n",
            "Epoch 39/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1328 - mse: 0.1328 - val_loss: 0.1485 - val_mse: 0.1485\n",
            "Epoch 40/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1313 - mse: 0.1313 - val_loss: 0.1442 - val_mse: 0.1442\n",
            "Epoch 41/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1322 - mse: 0.1322 - val_loss: 0.1446 - val_mse: 0.1446\n",
            "Epoch 42/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1302 - mse: 0.1302 - val_loss: 0.1523 - val_mse: 0.1523\n",
            "Epoch 43/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1295 - mse: 0.1295 - val_loss: 0.1444 - val_mse: 0.1444\n",
            "Epoch 44/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1304 - mse: 0.1304 - val_loss: 0.1402 - val_mse: 0.1402\n",
            "Epoch 45/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1282 - mse: 0.1282 - val_loss: 0.1473 - val_mse: 0.1473\n",
            "Epoch 46/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1286 - mse: 0.1286 - val_loss: 0.1434 - val_mse: 0.1434\n",
            "Epoch 47/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1285 - mse: 0.1285 - val_loss: 0.1424 - val_mse: 0.1424\n",
            "Epoch 48/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1285 - mse: 0.1285 - val_loss: 0.1446 - val_mse: 0.1446\n",
            "Epoch 49/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1273 - mse: 0.1273 - val_loss: 0.1455 - val_mse: 0.1455\n",
            "Epoch 50/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1265 - mse: 0.1265 - val_loss: 0.1487 - val_mse: 0.1487\n",
            "Epoch 51/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1280 - mse: 0.1280 - val_loss: 0.1442 - val_mse: 0.1442\n",
            "Epoch 52/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1262 - mse: 0.1262 - val_loss: 0.1439 - val_mse: 0.1439\n",
            "Epoch 53/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1279 - mse: 0.1279 - val_loss: 0.1437 - val_mse: 0.1437\n",
            "Epoch 54/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1259 - mse: 0.1259 - val_loss: 0.1413 - val_mse: 0.1413\n",
            "Epoch 55/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1263 - mse: 0.1263 - val_loss: 0.1538 - val_mse: 0.1538\n",
            "Epoch 56/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1242 - mse: 0.1242 - val_loss: 0.1490 - val_mse: 0.1490\n",
            "Epoch 57/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1242 - mse: 0.1242 - val_loss: 0.1396 - val_mse: 0.1396\n",
            "Epoch 58/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1230 - mse: 0.1230 - val_loss: 0.1378 - val_mse: 0.1378\n",
            "Epoch 59/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1220 - mse: 0.1220 - val_loss: 0.1442 - val_mse: 0.1442\n",
            "Epoch 60/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1211 - mse: 0.1211 - val_loss: 0.1391 - val_mse: 0.1391\n",
            "Epoch 61/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1221 - mse: 0.1221 - val_loss: 0.1492 - val_mse: 0.1492\n",
            "Epoch 62/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1219 - mse: 0.1219 - val_loss: 0.1486 - val_mse: 0.1486\n",
            "Epoch 63/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1211 - mse: 0.1211 - val_loss: 0.1388 - val_mse: 0.1388\n",
            "Epoch 64/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1218 - mse: 0.1218 - val_loss: 0.1581 - val_mse: 0.1581\n",
            "Epoch 65/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1211 - mse: 0.1211 - val_loss: 0.1440 - val_mse: 0.1440\n",
            "Epoch 66/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1209 - mse: 0.1209 - val_loss: 0.1337 - val_mse: 0.1337\n",
            "Epoch 67/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1199 - mse: 0.1199 - val_loss: 0.1437 - val_mse: 0.1437\n",
            "Epoch 68/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1193 - mse: 0.1193 - val_loss: 0.1313 - val_mse: 0.1313\n",
            "Epoch 69/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1189 - mse: 0.1189 - val_loss: 0.1394 - val_mse: 0.1394\n",
            "Epoch 70/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1184 - mse: 0.1184 - val_loss: 0.1424 - val_mse: 0.1424\n",
            "Epoch 71/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1171 - mse: 0.1171 - val_loss: 0.1471 - val_mse: 0.1471\n",
            "Epoch 72/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1189 - mse: 0.1189 - val_loss: 0.1333 - val_mse: 0.1333\n",
            "Epoch 73/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1187 - mse: 0.1187 - val_loss: 0.1349 - val_mse: 0.1349\n",
            "Epoch 74/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1165 - mse: 0.1165 - val_loss: 0.1305 - val_mse: 0.1305\n",
            "Epoch 75/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1161 - mse: 0.1161 - val_loss: 0.1331 - val_mse: 0.1331\n",
            "Epoch 76/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1179 - mse: 0.1179 - val_loss: 0.1421 - val_mse: 0.1421\n",
            "Epoch 77/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1169 - mse: 0.1169 - val_loss: 0.1422 - val_mse: 0.1422\n",
            "Epoch 78/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1168 - mse: 0.1168 - val_loss: 0.1333 - val_mse: 0.1333\n",
            "Epoch 79/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1163 - mse: 0.1163 - val_loss: 0.1391 - val_mse: 0.1391\n",
            "Epoch 80/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1165 - mse: 0.1165 - val_loss: 0.1315 - val_mse: 0.1315\n",
            "Epoch 81/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1153 - mse: 0.1153 - val_loss: 0.1346 - val_mse: 0.1346\n",
            "Epoch 82/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1158 - mse: 0.1158 - val_loss: 0.1366 - val_mse: 0.1366\n",
            "Epoch 83/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1151 - mse: 0.1151 - val_loss: 0.1363 - val_mse: 0.1363\n",
            "Epoch 84/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1145 - mse: 0.1145 - val_loss: 0.1401 - val_mse: 0.1401\n",
            "Epoch 85/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1141 - mse: 0.1141 - val_loss: 0.1395 - val_mse: 0.1395\n",
            "Epoch 86/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1137 - mse: 0.1137 - val_loss: 0.1389 - val_mse: 0.1389\n",
            "Epoch 87/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1147 - mse: 0.1147 - val_loss: 0.1455 - val_mse: 0.1455\n",
            "Epoch 88/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1149 - mse: 0.1149 - val_loss: 0.1350 - val_mse: 0.1350\n",
            "Epoch 89/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1123 - mse: 0.1123 - val_loss: 0.1332 - val_mse: 0.1332\n",
            "Epoch 90/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1135 - mse: 0.1135 - val_loss: 0.1356 - val_mse: 0.1356\n",
            "Epoch 91/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1140 - mse: 0.1140 - val_loss: 0.1291 - val_mse: 0.1291\n",
            "Epoch 92/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1127 - mse: 0.1127 - val_loss: 0.1316 - val_mse: 0.1316\n",
            "Epoch 93/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1121 - mse: 0.1121 - val_loss: 0.1348 - val_mse: 0.1348\n",
            "Epoch 94/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1119 - mse: 0.1119 - val_loss: 0.1280 - val_mse: 0.1280\n",
            "Epoch 95/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1132 - mse: 0.1132 - val_loss: 0.1331 - val_mse: 0.1331\n",
            "Epoch 96/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1129 - mse: 0.1129 - val_loss: 0.1337 - val_mse: 0.1337\n",
            "Epoch 97/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1115 - mse: 0.1115 - val_loss: 0.1341 - val_mse: 0.1341\n",
            "Epoch 98/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1115 - mse: 0.1115 - val_loss: 0.1287 - val_mse: 0.1287\n",
            "Epoch 99/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1111 - mse: 0.1111 - val_loss: 0.1310 - val_mse: 0.1310\n",
            "Epoch 100/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1122 - mse: 0.1122 - val_loss: 0.1429 - val_mse: 0.1429\n",
            "Epoch 101/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1112 - mse: 0.1112 - val_loss: 0.1302 - val_mse: 0.1302\n",
            "Epoch 102/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1108 - mse: 0.1108 - val_loss: 0.1374 - val_mse: 0.1374\n",
            "Epoch 103/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1117 - mse: 0.1117 - val_loss: 0.1415 - val_mse: 0.1415\n",
            "Epoch 104/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1094 - mse: 0.1094 - val_loss: 0.1536 - val_mse: 0.1536\n",
            "Epoch 105/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1097 - mse: 0.1097 - val_loss: 0.1448 - val_mse: 0.1448\n",
            "Epoch 106/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1101 - mse: 0.1101 - val_loss: 0.1348 - val_mse: 0.1348\n",
            "Epoch 107/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1093 - mse: 0.1093 - val_loss: 0.1382 - val_mse: 0.1382\n",
            "Epoch 108/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1102 - mse: 0.1102 - val_loss: 0.1429 - val_mse: 0.1429\n",
            "Epoch 109/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1083 - mse: 0.1083 - val_loss: 0.1321 - val_mse: 0.1321\n",
            "Epoch 110/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1082 - mse: 0.1082 - val_loss: 0.1465 - val_mse: 0.1465\n",
            "Epoch 111/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1074 - mse: 0.1074 - val_loss: 0.1427 - val_mse: 0.1427\n",
            "Epoch 112/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1106 - mse: 0.1106 - val_loss: 0.1317 - val_mse: 0.1317\n",
            "Epoch 113/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1087 - mse: 0.1087 - val_loss: 0.1347 - val_mse: 0.1347\n",
            "Epoch 114/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1069 - mse: 0.1069 - val_loss: 0.1277 - val_mse: 0.1277\n",
            "Epoch 115/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1081 - mse: 0.1081 - val_loss: 0.1296 - val_mse: 0.1296\n",
            "Epoch 116/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1085 - mse: 0.1085 - val_loss: 0.1287 - val_mse: 0.1287\n",
            "Epoch 117/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1070 - mse: 0.1070 - val_loss: 0.1319 - val_mse: 0.1319\n",
            "Epoch 118/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1069 - mse: 0.1069 - val_loss: 0.1311 - val_mse: 0.1311\n",
            "Epoch 119/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1066 - mse: 0.1066 - val_loss: 0.1348 - val_mse: 0.1348\n",
            "Epoch 120/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1077 - mse: 0.1077 - val_loss: 0.1455 - val_mse: 0.1455\n",
            "Epoch 121/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1085 - mse: 0.1085 - val_loss: 0.1308 - val_mse: 0.1308\n",
            "Epoch 122/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1064 - mse: 0.1064 - val_loss: 0.1250 - val_mse: 0.1250\n",
            "Epoch 123/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1069 - mse: 0.1069 - val_loss: 0.1399 - val_mse: 0.1399\n",
            "Epoch 124/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1072 - mse: 0.1072 - val_loss: 0.1250 - val_mse: 0.1250\n",
            "Epoch 125/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1068 - mse: 0.1068 - val_loss: 0.1310 - val_mse: 0.1310\n",
            "Epoch 126/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1062 - mse: 0.1062 - val_loss: 0.1301 - val_mse: 0.1301\n",
            "Epoch 127/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1058 - mse: 0.1058 - val_loss: 0.1367 - val_mse: 0.1367\n",
            "Epoch 128/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1056 - mse: 0.1056 - val_loss: 0.1363 - val_mse: 0.1363\n",
            "Epoch 129/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1063 - mse: 0.1063 - val_loss: 0.1298 - val_mse: 0.1298\n",
            "Epoch 130/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1048 - mse: 0.1048 - val_loss: 0.1270 - val_mse: 0.1270\n",
            "Epoch 131/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1068 - mse: 0.1068 - val_loss: 0.1304 - val_mse: 0.1304\n",
            "Epoch 132/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1062 - mse: 0.1062 - val_loss: 0.1520 - val_mse: 0.1520\n",
            "Epoch 133/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1055 - mse: 0.1055 - val_loss: 0.1382 - val_mse: 0.1382\n",
            "Epoch 134/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1043 - mse: 0.1043 - val_loss: 0.1339 - val_mse: 0.1339\n",
            "Epoch 135/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1044 - mse: 0.1044 - val_loss: 0.1340 - val_mse: 0.1340\n",
            "Epoch 136/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1043 - mse: 0.1043 - val_loss: 0.1320 - val_mse: 0.1320\n",
            "Epoch 137/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1059 - mse: 0.1059 - val_loss: 0.1366 - val_mse: 0.1366\n",
            "Epoch 138/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1047 - mse: 0.1047 - val_loss: 0.1248 - val_mse: 0.1248\n",
            "Epoch 139/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1043 - mse: 0.1043 - val_loss: 0.1303 - val_mse: 0.1303\n",
            "Epoch 140/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1041 - mse: 0.1041 - val_loss: 0.1344 - val_mse: 0.1344\n",
            "Epoch 141/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1045 - mse: 0.1045 - val_loss: 0.1290 - val_mse: 0.1290\n",
            "Epoch 142/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1031 - mse: 0.1031 - val_loss: 0.1366 - val_mse: 0.1366\n",
            "Epoch 143/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1054 - mse: 0.1054 - val_loss: 0.1300 - val_mse: 0.1300\n",
            "Epoch 144/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1048 - mse: 0.1048 - val_loss: 0.1260 - val_mse: 0.1260\n",
            "Epoch 145/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1028 - mse: 0.1028 - val_loss: 0.1361 - val_mse: 0.1361\n",
            "Epoch 146/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1027 - mse: 0.1027 - val_loss: 0.1274 - val_mse: 0.1274\n",
            "Epoch 147/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1037 - mse: 0.1037 - val_loss: 0.1295 - val_mse: 0.1295\n",
            "Epoch 148/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1039 - mse: 0.1039 - val_loss: 0.1315 - val_mse: 0.1315\n",
            "Epoch 149/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1043 - mse: 0.1043 - val_loss: 0.1271 - val_mse: 0.1271\n",
            "Epoch 150/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1011 - mse: 0.1011 - val_loss: 0.1313 - val_mse: 0.1313\n",
            "Epoch 151/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1028 - mse: 0.1028 - val_loss: 0.1396 - val_mse: 0.1396\n",
            "Epoch 152/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1020 - mse: 0.1020 - val_loss: 0.1215 - val_mse: 0.1215\n",
            "Epoch 153/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1024 - mse: 0.1024 - val_loss: 0.1270 - val_mse: 0.1270\n",
            "Epoch 154/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1018 - mse: 0.1018 - val_loss: 0.1281 - val_mse: 0.1281\n",
            "Epoch 155/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1021 - mse: 0.1021 - val_loss: 0.1225 - val_mse: 0.1225\n",
            "Epoch 156/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1031 - mse: 0.1031 - val_loss: 0.1260 - val_mse: 0.1260\n",
            "Epoch 157/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1003 - mse: 0.1003 - val_loss: 0.1244 - val_mse: 0.1244\n",
            "Epoch 158/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1024 - mse: 0.1024 - val_loss: 0.1347 - val_mse: 0.1347\n",
            "Epoch 159/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1023 - mse: 0.1023 - val_loss: 0.1250 - val_mse: 0.1250\n",
            "Epoch 160/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1021 - mse: 0.1021 - val_loss: 0.1265 - val_mse: 0.1265\n",
            "Epoch 161/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.1013 - mse: 0.1013 - val_loss: 0.1298 - val_mse: 0.1298\n",
            "Epoch 162/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1021 - mse: 0.1021 - val_loss: 0.1256 - val_mse: 0.1256\n",
            "Epoch 163/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1028 - mse: 0.1028 - val_loss: 0.1342 - val_mse: 0.1342\n",
            "Epoch 164/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1014 - mse: 0.1014 - val_loss: 0.1362 - val_mse: 0.1362\n",
            "Epoch 165/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1005 - mse: 0.1005 - val_loss: 0.1307 - val_mse: 0.1307\n",
            "Epoch 166/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1012 - mse: 0.1012 - val_loss: 0.1245 - val_mse: 0.1245\n",
            "Epoch 167/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1014 - mse: 0.1014 - val_loss: 0.1250 - val_mse: 0.1250\n",
            "Epoch 168/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1012 - mse: 0.1012 - val_loss: 0.1282 - val_mse: 0.1282\n",
            "Epoch 169/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1002 - mse: 0.1002 - val_loss: 0.1236 - val_mse: 0.1236\n",
            "Epoch 170/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1002 - mse: 0.1002 - val_loss: 0.1314 - val_mse: 0.1314\n",
            "Epoch 171/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1023 - mse: 0.1023 - val_loss: 0.1372 - val_mse: 0.1372\n",
            "Epoch 172/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0997 - mse: 0.0997 - val_loss: 0.1317 - val_mse: 0.1317\n",
            "Epoch 173/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0995 - mse: 0.0995 - val_loss: 0.1288 - val_mse: 0.1288\n",
            "Epoch 174/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0999 - mse: 0.0999 - val_loss: 0.1265 - val_mse: 0.1265\n",
            "Epoch 175/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.0995 - mse: 0.0995 - val_loss: 0.1337 - val_mse: 0.1337\n",
            "Epoch 176/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0991 - mse: 0.0991 - val_loss: 0.1210 - val_mse: 0.1210\n",
            "Epoch 177/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1002 - mse: 0.1002 - val_loss: 0.1344 - val_mse: 0.1344\n",
            "Epoch 178/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1006 - mse: 0.1006 - val_loss: 0.1310 - val_mse: 0.1310\n",
            "Epoch 179/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1009 - mse: 0.1009 - val_loss: 0.1317 - val_mse: 0.1317\n",
            "Epoch 180/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0992 - mse: 0.0992 - val_loss: 0.1273 - val_mse: 0.1273\n",
            "Epoch 181/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.1000 - mse: 0.1000 - val_loss: 0.1356 - val_mse: 0.1356\n",
            "Epoch 182/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0985 - mse: 0.0985 - val_loss: 0.1369 - val_mse: 0.1369\n",
            "Epoch 183/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0984 - mse: 0.0984 - val_loss: 0.1382 - val_mse: 0.1382\n",
            "Epoch 184/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.1004 - mse: 0.1004 - val_loss: 0.1312 - val_mse: 0.1312\n",
            "Epoch 185/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0970 - mse: 0.0970 - val_loss: 0.1308 - val_mse: 0.1308\n",
            "Epoch 186/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.1279 - val_mse: 0.1279\n",
            "Epoch 187/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.1250 - val_mse: 0.1250\n",
            "Epoch 188/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.1327 - val_mse: 0.1327\n",
            "Epoch 189/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0992 - mse: 0.0992 - val_loss: 0.1234 - val_mse: 0.1234\n",
            "Epoch 190/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.0989 - mse: 0.0989 - val_loss: 0.1206 - val_mse: 0.1206\n",
            "Epoch 191/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0971 - mse: 0.0971 - val_loss: 0.1267 - val_mse: 0.1267\n",
            "Epoch 192/500\n",
            "931/931 [==============================] - 4s 4ms/step - loss: 0.0984 - mse: 0.0984 - val_loss: 0.1255 - val_mse: 0.1255\n",
            "Epoch 193/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0972 - mse: 0.0972 - val_loss: 0.1322 - val_mse: 0.1322\n",
            "Epoch 194/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0973 - mse: 0.0973 - val_loss: 0.1304 - val_mse: 0.1304\n",
            "Epoch 195/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0985 - mse: 0.0985 - val_loss: 0.1263 - val_mse: 0.1263\n",
            "Epoch 196/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0980 - mse: 0.0980 - val_loss: 0.1237 - val_mse: 0.1237\n",
            "Epoch 197/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 0.1282 - val_mse: 0.1282\n",
            "Epoch 198/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0965 - mse: 0.0965 - val_loss: 0.1260 - val_mse: 0.1260\n",
            "Epoch 199/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 0.1327 - val_mse: 0.1327\n",
            "Epoch 200/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.1279 - val_mse: 0.1279\n",
            "Epoch 201/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0972 - mse: 0.0972 - val_loss: 0.1276 - val_mse: 0.1276\n",
            "Epoch 202/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0967 - mse: 0.0967 - val_loss: 0.1227 - val_mse: 0.1227\n",
            "Epoch 203/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0966 - mse: 0.0966 - val_loss: 0.1207 - val_mse: 0.1207\n",
            "Epoch 204/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0957 - mse: 0.0957 - val_loss: 0.1269 - val_mse: 0.1269\n",
            "Epoch 205/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0962 - mse: 0.0962 - val_loss: 0.1222 - val_mse: 0.1222\n",
            "Epoch 206/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0956 - mse: 0.0956 - val_loss: 0.1340 - val_mse: 0.1340\n",
            "Epoch 207/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0966 - mse: 0.0966 - val_loss: 0.1221 - val_mse: 0.1221\n",
            "Epoch 208/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0956 - mse: 0.0956 - val_loss: 0.1339 - val_mse: 0.1339\n",
            "Epoch 209/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0969 - mse: 0.0969 - val_loss: 0.1243 - val_mse: 0.1243\n",
            "Epoch 210/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0971 - mse: 0.0971 - val_loss: 0.1265 - val_mse: 0.1265\n",
            "Epoch 211/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0949 - mse: 0.0949 - val_loss: 0.1289 - val_mse: 0.1289\n",
            "Epoch 212/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0955 - mse: 0.0955 - val_loss: 0.1230 - val_mse: 0.1230\n",
            "Epoch 213/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0957 - mse: 0.0957 - val_loss: 0.1236 - val_mse: 0.1236\n",
            "Epoch 214/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0950 - mse: 0.0950 - val_loss: 0.1304 - val_mse: 0.1304\n",
            "Epoch 215/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0963 - mse: 0.0963 - val_loss: 0.1309 - val_mse: 0.1309\n",
            "Epoch 216/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0961 - mse: 0.0961 - val_loss: 0.1342 - val_mse: 0.1342\n",
            "Epoch 217/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0967 - mse: 0.0967 - val_loss: 0.1339 - val_mse: 0.1339\n",
            "Epoch 218/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0959 - mse: 0.0959 - val_loss: 0.1214 - val_mse: 0.1214\n",
            "Epoch 219/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0950 - mse: 0.0950 - val_loss: 0.1283 - val_mse: 0.1283\n",
            "Epoch 220/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0955 - mse: 0.0955 - val_loss: 0.1262 - val_mse: 0.1262\n",
            "Epoch 221/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0959 - mse: 0.0959 - val_loss: 0.1289 - val_mse: 0.1289\n",
            "Epoch 222/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0960 - mse: 0.0960 - val_loss: 0.1276 - val_mse: 0.1276\n",
            "Epoch 223/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0958 - mse: 0.0958 - val_loss: 0.1375 - val_mse: 0.1375\n",
            "Epoch 224/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0959 - mse: 0.0959 - val_loss: 0.1236 - val_mse: 0.1236\n",
            "Epoch 225/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0953 - mse: 0.0953 - val_loss: 0.1254 - val_mse: 0.1254\n",
            "Epoch 226/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0947 - mse: 0.0947 - val_loss: 0.1233 - val_mse: 0.1233\n",
            "Epoch 227/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0943 - mse: 0.0943 - val_loss: 0.1329 - val_mse: 0.1329\n",
            "Epoch 228/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0953 - mse: 0.0953 - val_loss: 0.1202 - val_mse: 0.1202\n",
            "Epoch 229/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0950 - mse: 0.0950 - val_loss: 0.1267 - val_mse: 0.1267\n",
            "Epoch 230/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0941 - mse: 0.0941 - val_loss: 0.1236 - val_mse: 0.1236\n",
            "Epoch 231/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0946 - mse: 0.0946 - val_loss: 0.1212 - val_mse: 0.1212\n",
            "Epoch 232/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0943 - mse: 0.0943 - val_loss: 0.1272 - val_mse: 0.1272\n",
            "Epoch 233/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0944 - mse: 0.0944 - val_loss: 0.1251 - val_mse: 0.1251\n",
            "Epoch 234/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0942 - mse: 0.0942 - val_loss: 0.1306 - val_mse: 0.1306\n",
            "Epoch 235/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0936 - mse: 0.0936 - val_loss: 0.1211 - val_mse: 0.1211\n",
            "Epoch 236/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0935 - mse: 0.0935 - val_loss: 0.1295 - val_mse: 0.1295\n",
            "Epoch 237/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.1190 - val_mse: 0.1190\n",
            "Epoch 238/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0935 - mse: 0.0935 - val_loss: 0.1320 - val_mse: 0.1320\n",
            "Epoch 239/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0945 - mse: 0.0945 - val_loss: 0.1280 - val_mse: 0.1280\n",
            "Epoch 240/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0937 - mse: 0.0937 - val_loss: 0.1282 - val_mse: 0.1282\n",
            "Epoch 241/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0941 - mse: 0.0941 - val_loss: 0.1272 - val_mse: 0.1272\n",
            "Epoch 242/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0933 - mse: 0.0933 - val_loss: 0.1287 - val_mse: 0.1287\n",
            "Epoch 243/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0947 - mse: 0.0947 - val_loss: 0.1259 - val_mse: 0.1259\n",
            "Epoch 244/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1239 - val_mse: 0.1239\n",
            "Epoch 245/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0938 - mse: 0.0938 - val_loss: 0.1365 - val_mse: 0.1365\n",
            "Epoch 246/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0929 - mse: 0.0929 - val_loss: 0.1297 - val_mse: 0.1297\n",
            "Epoch 247/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0927 - mse: 0.0927 - val_loss: 0.1233 - val_mse: 0.1233\n",
            "Epoch 248/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0929 - mse: 0.0929 - val_loss: 0.1357 - val_mse: 0.1357\n",
            "Epoch 249/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0935 - mse: 0.0935 - val_loss: 0.1217 - val_mse: 0.1217\n",
            "Epoch 250/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0925 - mse: 0.0925 - val_loss: 0.1263 - val_mse: 0.1263\n",
            "Epoch 251/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0933 - mse: 0.0933 - val_loss: 0.1247 - val_mse: 0.1247\n",
            "Epoch 252/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0932 - mse: 0.0932 - val_loss: 0.1282 - val_mse: 0.1282\n",
            "Epoch 253/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0922 - mse: 0.0922 - val_loss: 0.1230 - val_mse: 0.1230\n",
            "Epoch 254/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0935 - mse: 0.0935 - val_loss: 0.1212 - val_mse: 0.1212\n",
            "Epoch 255/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0938 - mse: 0.0938 - val_loss: 0.1272 - val_mse: 0.1272\n",
            "Epoch 256/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0924 - mse: 0.0924 - val_loss: 0.1306 - val_mse: 0.1306\n",
            "Epoch 257/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0925 - mse: 0.0925 - val_loss: 0.1345 - val_mse: 0.1345\n",
            "Epoch 258/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0924 - mse: 0.0924 - val_loss: 0.1221 - val_mse: 0.1221\n",
            "Epoch 259/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0921 - mse: 0.0921 - val_loss: 0.1228 - val_mse: 0.1228\n",
            "Epoch 260/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0924 - mse: 0.0924 - val_loss: 0.1375 - val_mse: 0.1375\n",
            "Epoch 261/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0922 - mse: 0.0922 - val_loss: 0.1221 - val_mse: 0.1221\n",
            "Epoch 262/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0926 - mse: 0.0926 - val_loss: 0.1257 - val_mse: 0.1257\n",
            "Epoch 263/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0926 - mse: 0.0926 - val_loss: 0.1237 - val_mse: 0.1237\n",
            "Epoch 264/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0930 - mse: 0.0930 - val_loss: 0.1279 - val_mse: 0.1279\n",
            "Epoch 265/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.1259 - val_mse: 0.1259\n",
            "Epoch 266/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.1219 - val_mse: 0.1219\n",
            "Epoch 267/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0921 - mse: 0.0921 - val_loss: 0.1213 - val_mse: 0.1213\n",
            "Epoch 268/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0916 - mse: 0.0916 - val_loss: 0.1221 - val_mse: 0.1221\n",
            "Epoch 269/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0917 - mse: 0.0917 - val_loss: 0.1278 - val_mse: 0.1278\n",
            "Epoch 270/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0918 - mse: 0.0918 - val_loss: 0.1186 - val_mse: 0.1186\n",
            "Epoch 271/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0916 - mse: 0.0916 - val_loss: 0.1273 - val_mse: 0.1273\n",
            "Epoch 272/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0924 - mse: 0.0924 - val_loss: 0.1287 - val_mse: 0.1287\n",
            "Epoch 273/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.1190 - val_mse: 0.1190\n",
            "Epoch 274/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0899 - mse: 0.0899 - val_loss: 0.1300 - val_mse: 0.1300\n",
            "Epoch 275/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0910 - mse: 0.0910 - val_loss: 0.1262 - val_mse: 0.1262\n",
            "Epoch 276/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0926 - mse: 0.0926 - val_loss: 0.1372 - val_mse: 0.1372\n",
            "Epoch 277/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0916 - mse: 0.0916 - val_loss: 0.1240 - val_mse: 0.1240\n",
            "Epoch 278/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0925 - mse: 0.0925 - val_loss: 0.1250 - val_mse: 0.1250\n",
            "Epoch 279/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0914 - mse: 0.0914 - val_loss: 0.1221 - val_mse: 0.1221\n",
            "Epoch 280/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1236 - val_mse: 0.1236\n",
            "Epoch 281/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.1193 - val_mse: 0.1193\n",
            "Epoch 282/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0918 - mse: 0.0918 - val_loss: 0.1280 - val_mse: 0.1280\n",
            "Epoch 283/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0910 - mse: 0.0910 - val_loss: 0.1208 - val_mse: 0.1208\n",
            "Epoch 284/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0923 - mse: 0.0923 - val_loss: 0.1282 - val_mse: 0.1282\n",
            "Epoch 285/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1282 - val_mse: 0.1282\n",
            "Epoch 286/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0918 - mse: 0.0918 - val_loss: 0.1218 - val_mse: 0.1218\n",
            "Epoch 287/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1252 - val_mse: 0.1252\n",
            "Epoch 288/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0908 - mse: 0.0908 - val_loss: 0.1386 - val_mse: 0.1386\n",
            "Epoch 289/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0908 - mse: 0.0908 - val_loss: 0.1247 - val_mse: 0.1247\n",
            "Epoch 290/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0895 - mse: 0.0895 - val_loss: 0.1354 - val_mse: 0.1354\n",
            "Epoch 291/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1234 - val_mse: 0.1234\n",
            "Epoch 292/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1205 - val_mse: 0.1205\n",
            "Epoch 293/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0899 - mse: 0.0899 - val_loss: 0.1241 - val_mse: 0.1241\n",
            "Epoch 294/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0893 - mse: 0.0893 - val_loss: 0.1280 - val_mse: 0.1280\n",
            "Epoch 295/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0893 - mse: 0.0893 - val_loss: 0.1342 - val_mse: 0.1342\n",
            "Epoch 296/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0908 - mse: 0.0908 - val_loss: 0.1236 - val_mse: 0.1236\n",
            "Epoch 297/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0912 - mse: 0.0912 - val_loss: 0.1217 - val_mse: 0.1217\n",
            "Epoch 298/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.1253 - val_mse: 0.1253\n",
            "Epoch 299/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1258 - val_mse: 0.1258\n",
            "Epoch 300/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0894 - mse: 0.0894 - val_loss: 0.1216 - val_mse: 0.1216\n",
            "Epoch 301/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0899 - mse: 0.0899 - val_loss: 0.1261 - val_mse: 0.1261\n",
            "Epoch 302/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0898 - mse: 0.0898 - val_loss: 0.1224 - val_mse: 0.1224\n",
            "Epoch 303/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.1192 - val_mse: 0.1192\n",
            "Epoch 304/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1224 - val_mse: 0.1224\n",
            "Epoch 305/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.1195 - val_mse: 0.1195\n",
            "Epoch 306/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0892 - mse: 0.0892 - val_loss: 0.1317 - val_mse: 0.1317\n",
            "Epoch 307/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0894 - mse: 0.0894 - val_loss: 0.1211 - val_mse: 0.1211\n",
            "Epoch 308/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1318 - val_mse: 0.1318\n",
            "Epoch 309/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0898 - mse: 0.0898 - val_loss: 0.1214 - val_mse: 0.1214\n",
            "Epoch 310/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0898 - mse: 0.0898 - val_loss: 0.1190 - val_mse: 0.1190\n",
            "Epoch 311/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0895 - mse: 0.0895 - val_loss: 0.1301 - val_mse: 0.1301\n",
            "Epoch 312/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0893 - mse: 0.0893 - val_loss: 0.1197 - val_mse: 0.1197\n",
            "Epoch 313/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0894 - mse: 0.0894 - val_loss: 0.1442 - val_mse: 0.1442\n",
            "Epoch 314/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1284 - val_mse: 0.1284\n",
            "Epoch 315/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.1222 - val_mse: 0.1222\n",
            "Epoch 316/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0890 - mse: 0.0890 - val_loss: 0.1272 - val_mse: 0.1272\n",
            "Epoch 317/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.1297 - val_mse: 0.1297\n",
            "Epoch 318/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1229 - val_mse: 0.1229\n",
            "Epoch 319/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0892 - mse: 0.0892 - val_loss: 0.1288 - val_mse: 0.1288\n",
            "Epoch 320/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1198 - val_mse: 0.1198\n",
            "Epoch 321/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1317 - val_mse: 0.1317\n",
            "Epoch 322/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0898 - mse: 0.0898 - val_loss: 0.1276 - val_mse: 0.1276\n",
            "Epoch 323/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.1257 - val_mse: 0.1257\n",
            "Epoch 324/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0893 - mse: 0.0893 - val_loss: 0.1303 - val_mse: 0.1303\n",
            "Epoch 325/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1231 - val_mse: 0.1231\n",
            "Epoch 326/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.1234 - val_mse: 0.1234\n",
            "Epoch 327/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0899 - mse: 0.0899 - val_loss: 0.1245 - val_mse: 0.1245\n",
            "Epoch 328/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0890 - mse: 0.0890 - val_loss: 0.1227 - val_mse: 0.1227\n",
            "Epoch 329/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0892 - mse: 0.0892 - val_loss: 0.1282 - val_mse: 0.1282\n",
            "Epoch 330/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1214 - val_mse: 0.1214\n",
            "Epoch 331/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0881 - mse: 0.0881 - val_loss: 0.1332 - val_mse: 0.1332\n",
            "Epoch 332/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0891 - mse: 0.0891 - val_loss: 0.1316 - val_mse: 0.1316\n",
            "Epoch 333/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0885 - mse: 0.0885 - val_loss: 0.1293 - val_mse: 0.1293\n",
            "Epoch 334/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0891 - mse: 0.0891 - val_loss: 0.1270 - val_mse: 0.1270\n",
            "Epoch 335/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.1320 - val_mse: 0.1320\n",
            "Epoch 336/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0891 - mse: 0.0891 - val_loss: 0.1280 - val_mse: 0.1280\n",
            "Epoch 337/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0890 - mse: 0.0890 - val_loss: 0.1225 - val_mse: 0.1225\n",
            "Epoch 338/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0880 - mse: 0.0880 - val_loss: 0.1189 - val_mse: 0.1189\n",
            "Epoch 339/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0868 - mse: 0.0868 - val_loss: 0.1236 - val_mse: 0.1236\n",
            "Epoch 340/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0890 - mse: 0.0890 - val_loss: 0.1252 - val_mse: 0.1252\n",
            "Epoch 341/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0885 - mse: 0.0885 - val_loss: 0.1222 - val_mse: 0.1222\n",
            "Epoch 342/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.1273 - val_mse: 0.1273\n",
            "Epoch 343/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0874 - mse: 0.0874 - val_loss: 0.1264 - val_mse: 0.1264\n",
            "Epoch 344/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1305 - val_mse: 0.1305\n",
            "Epoch 345/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0877 - mse: 0.0877 - val_loss: 0.1298 - val_mse: 0.1298\n",
            "Epoch 346/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1253 - val_mse: 0.1253\n",
            "Epoch 347/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0875 - mse: 0.0875 - val_loss: 0.1246 - val_mse: 0.1246\n",
            "Epoch 348/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1262 - val_mse: 0.1262\n",
            "Epoch 349/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.1277 - val_mse: 0.1277\n",
            "Epoch 350/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0885 - mse: 0.0885 - val_loss: 0.1241 - val_mse: 0.1241\n",
            "Epoch 351/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0885 - mse: 0.0885 - val_loss: 0.1210 - val_mse: 0.1210\n",
            "Epoch 352/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0878 - mse: 0.0878 - val_loss: 0.1168 - val_mse: 0.1168\n",
            "Epoch 353/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0871 - mse: 0.0871 - val_loss: 0.1257 - val_mse: 0.1257\n",
            "Epoch 354/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0875 - mse: 0.0875 - val_loss: 0.1183 - val_mse: 0.1183\n",
            "Epoch 355/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0872 - mse: 0.0872 - val_loss: 0.1265 - val_mse: 0.1265\n",
            "Epoch 356/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.1243 - val_mse: 0.1243\n",
            "Epoch 357/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0879 - mse: 0.0879 - val_loss: 0.1187 - val_mse: 0.1187\n",
            "Epoch 358/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.1197 - val_mse: 0.1197\n",
            "Epoch 359/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.1209 - val_mse: 0.1209\n",
            "Epoch 360/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0874 - mse: 0.0874 - val_loss: 0.1277 - val_mse: 0.1277\n",
            "Epoch 361/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.1226 - val_mse: 0.1226\n",
            "Epoch 362/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0878 - mse: 0.0878 - val_loss: 0.1224 - val_mse: 0.1224\n",
            "Epoch 363/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0869 - mse: 0.0869 - val_loss: 0.1218 - val_mse: 0.1218\n",
            "Epoch 364/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0875 - mse: 0.0875 - val_loss: 0.1215 - val_mse: 0.1215\n",
            "Epoch 365/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0864 - mse: 0.0864 - val_loss: 0.1195 - val_mse: 0.1195\n",
            "Epoch 366/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.1263 - val_mse: 0.1263\n",
            "Epoch 367/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0869 - mse: 0.0869 - val_loss: 0.1238 - val_mse: 0.1238\n",
            "Epoch 368/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.1228 - val_mse: 0.1228\n",
            "Epoch 369/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0880 - mse: 0.0880 - val_loss: 0.1170 - val_mse: 0.1170\n",
            "Epoch 370/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0866 - mse: 0.0866 - val_loss: 0.1190 - val_mse: 0.1190\n",
            "Epoch 371/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0864 - mse: 0.0864 - val_loss: 0.1282 - val_mse: 0.1282\n",
            "Epoch 372/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0877 - mse: 0.0877 - val_loss: 0.1208 - val_mse: 0.1208\n",
            "Epoch 373/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1288 - val_mse: 0.1288\n",
            "Epoch 374/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0869 - mse: 0.0869 - val_loss: 0.1226 - val_mse: 0.1226\n",
            "Epoch 375/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.1305 - val_mse: 0.1305\n",
            "Epoch 376/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.1189 - val_mse: 0.1189\n",
            "Epoch 377/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.1207 - val_mse: 0.1207\n",
            "Epoch 378/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.1195 - val_mse: 0.1195\n",
            "Epoch 379/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0871 - mse: 0.0871 - val_loss: 0.1259 - val_mse: 0.1259\n",
            "Epoch 380/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0863 - mse: 0.0863 - val_loss: 0.1229 - val_mse: 0.1229\n",
            "Epoch 381/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0865 - mse: 0.0865 - val_loss: 0.1207 - val_mse: 0.1207\n",
            "Epoch 382/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0865 - mse: 0.0865 - val_loss: 0.1220 - val_mse: 0.1220\n",
            "Epoch 383/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.1225 - val_mse: 0.1225\n",
            "Epoch 384/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.1260 - val_mse: 0.1260\n",
            "Epoch 385/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0855 - mse: 0.0855 - val_loss: 0.1251 - val_mse: 0.1251\n",
            "Epoch 386/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.1184 - val_mse: 0.1184\n",
            "Epoch 387/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0862 - mse: 0.0862 - val_loss: 0.1217 - val_mse: 0.1217\n",
            "Epoch 388/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0869 - mse: 0.0869 - val_loss: 0.1223 - val_mse: 0.1223\n",
            "Epoch 389/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0856 - mse: 0.0856 - val_loss: 0.1189 - val_mse: 0.1189\n",
            "Epoch 390/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.1220 - val_mse: 0.1220\n",
            "Epoch 391/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.1254 - val_mse: 0.1254\n",
            "Epoch 392/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0866 - mse: 0.0866 - val_loss: 0.1245 - val_mse: 0.1245\n",
            "Epoch 393/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.1200 - val_mse: 0.1200\n",
            "Epoch 394/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0868 - mse: 0.0868 - val_loss: 0.1186 - val_mse: 0.1186\n",
            "Epoch 395/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.1213 - val_mse: 0.1213\n",
            "Epoch 396/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.1241 - val_mse: 0.1241\n",
            "Epoch 397/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0858 - mse: 0.0858 - val_loss: 0.1286 - val_mse: 0.1286\n",
            "Epoch 398/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.1228 - val_mse: 0.1228\n",
            "Epoch 399/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0869 - mse: 0.0869 - val_loss: 0.1246 - val_mse: 0.1246\n",
            "Epoch 400/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0856 - mse: 0.0856 - val_loss: 0.1197 - val_mse: 0.1197\n",
            "Epoch 401/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0863 - mse: 0.0863 - val_loss: 0.1216 - val_mse: 0.1216\n",
            "Epoch 402/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.1248 - val_mse: 0.1248\n",
            "Epoch 403/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.1192 - val_mse: 0.1192\n",
            "Epoch 404/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0854 - mse: 0.0854 - val_loss: 0.1316 - val_mse: 0.1316\n",
            "Epoch 405/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.1294 - val_mse: 0.1294\n",
            "Epoch 406/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0866 - mse: 0.0866 - val_loss: 0.1227 - val_mse: 0.1227\n",
            "Epoch 407/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0852 - mse: 0.0852 - val_loss: 0.1186 - val_mse: 0.1186\n",
            "Epoch 408/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.1392 - val_mse: 0.1392\n",
            "Epoch 409/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0854 - mse: 0.0854 - val_loss: 0.1198 - val_mse: 0.1198\n",
            "Epoch 410/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0858 - mse: 0.0858 - val_loss: 0.1281 - val_mse: 0.1281\n",
            "Epoch 411/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.1187 - val_mse: 0.1187\n",
            "Epoch 412/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0863 - mse: 0.0863 - val_loss: 0.1219 - val_mse: 0.1219\n",
            "Epoch 413/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0850 - mse: 0.0850 - val_loss: 0.1212 - val_mse: 0.1212\n",
            "Epoch 414/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0858 - mse: 0.0858 - val_loss: 0.1575 - val_mse: 0.1575\n",
            "Epoch 415/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0855 - mse: 0.0855 - val_loss: 0.1270 - val_mse: 0.1270\n",
            "Epoch 416/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0856 - mse: 0.0856 - val_loss: 0.1198 - val_mse: 0.1198\n",
            "Epoch 417/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0851 - mse: 0.0851 - val_loss: 0.1257 - val_mse: 0.1257\n",
            "Epoch 418/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0854 - mse: 0.0854 - val_loss: 0.1242 - val_mse: 0.1242\n",
            "Epoch 419/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0858 - mse: 0.0858 - val_loss: 0.1197 - val_mse: 0.1197\n",
            "Epoch 420/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0858 - mse: 0.0858 - val_loss: 0.1169 - val_mse: 0.1169\n",
            "Epoch 421/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0849 - mse: 0.0849 - val_loss: 0.1237 - val_mse: 0.1237\n",
            "Epoch 422/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0847 - mse: 0.0847 - val_loss: 0.1220 - val_mse: 0.1220\n",
            "Epoch 423/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0868 - mse: 0.0868 - val_loss: 0.1189 - val_mse: 0.1189\n",
            "Epoch 424/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0850 - mse: 0.0850 - val_loss: 0.1243 - val_mse: 0.1243\n",
            "Epoch 425/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0846 - mse: 0.0846 - val_loss: 0.1191 - val_mse: 0.1191\n",
            "Epoch 426/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0864 - mse: 0.0864 - val_loss: 0.1313 - val_mse: 0.1313\n",
            "Epoch 427/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0851 - mse: 0.0851 - val_loss: 0.1204 - val_mse: 0.1204\n",
            "Epoch 428/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0862 - mse: 0.0862 - val_loss: 0.1273 - val_mse: 0.1273\n",
            "Epoch 429/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0849 - mse: 0.0849 - val_loss: 0.1237 - val_mse: 0.1237\n",
            "Epoch 430/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.1219 - val_mse: 0.1219\n",
            "Epoch 431/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0843 - mse: 0.0843 - val_loss: 0.1187 - val_mse: 0.1187\n",
            "Epoch 432/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0846 - mse: 0.0846 - val_loss: 0.1317 - val_mse: 0.1317\n",
            "Epoch 433/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0854 - mse: 0.0854 - val_loss: 0.1231 - val_mse: 0.1231\n",
            "Epoch 434/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0843 - mse: 0.0843 - val_loss: 0.1241 - val_mse: 0.1241\n",
            "Epoch 435/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0847 - mse: 0.0847 - val_loss: 0.1193 - val_mse: 0.1193\n",
            "Epoch 436/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0850 - mse: 0.0850 - val_loss: 0.1255 - val_mse: 0.1255\n",
            "Epoch 437/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0845 - mse: 0.0845 - val_loss: 0.1258 - val_mse: 0.1258\n",
            "Epoch 438/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0851 - mse: 0.0851 - val_loss: 0.1211 - val_mse: 0.1211\n",
            "Epoch 439/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0849 - mse: 0.0849 - val_loss: 0.1223 - val_mse: 0.1223\n",
            "Epoch 440/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0837 - mse: 0.0837 - val_loss: 0.1233 - val_mse: 0.1233\n",
            "Epoch 441/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0847 - mse: 0.0847 - val_loss: 0.1196 - val_mse: 0.1196\n",
            "Epoch 442/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1223 - val_mse: 0.1223\n",
            "Epoch 443/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0843 - mse: 0.0843 - val_loss: 0.1220 - val_mse: 0.1220\n",
            "Epoch 444/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0842 - mse: 0.0842 - val_loss: 0.1190 - val_mse: 0.1190\n",
            "Epoch 445/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0851 - mse: 0.0851 - val_loss: 0.1301 - val_mse: 0.1301\n",
            "Epoch 446/500\n",
            "931/931 [==============================] - 4s 5ms/step - loss: 0.0837 - mse: 0.0837 - val_loss: 0.1184 - val_mse: 0.1184\n",
            "Epoch 447/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0846 - mse: 0.0846 - val_loss: 0.1215 - val_mse: 0.1215\n",
            "Epoch 448/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0853 - mse: 0.0853 - val_loss: 0.1173 - val_mse: 0.1173\n",
            "Epoch 449/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0856 - mse: 0.0856 - val_loss: 0.1218 - val_mse: 0.1218\n",
            "Epoch 450/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0852 - mse: 0.0852 - val_loss: 0.1155 - val_mse: 0.1155\n",
            "Epoch 451/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0834 - mse: 0.0834 - val_loss: 0.1167 - val_mse: 0.1167\n",
            "Epoch 452/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1258 - val_mse: 0.1258\n",
            "Epoch 453/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0846 - mse: 0.0846 - val_loss: 0.1201 - val_mse: 0.1201\n",
            "Epoch 454/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0853 - mse: 0.0853 - val_loss: 0.1468 - val_mse: 0.1468\n",
            "Epoch 455/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.1226 - val_mse: 0.1226\n",
            "Epoch 456/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0842 - mse: 0.0842 - val_loss: 0.1216 - val_mse: 0.1216\n",
            "Epoch 457/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0842 - mse: 0.0842 - val_loss: 0.1157 - val_mse: 0.1157\n",
            "Epoch 458/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1291 - val_mse: 0.1291\n",
            "Epoch 459/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0855 - mse: 0.0855 - val_loss: 0.1219 - val_mse: 0.1219\n",
            "Epoch 460/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0834 - mse: 0.0834 - val_loss: 0.1278 - val_mse: 0.1278\n",
            "Epoch 461/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0827 - mse: 0.0827 - val_loss: 0.1195 - val_mse: 0.1195\n",
            "Epoch 462/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0868 - mse: 0.0868 - val_loss: 0.1177 - val_mse: 0.1177\n",
            "Epoch 463/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0842 - mse: 0.0842 - val_loss: 0.1236 - val_mse: 0.1236\n",
            "Epoch 464/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0833 - mse: 0.0833 - val_loss: 0.1242 - val_mse: 0.1242\n",
            "Epoch 465/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0841 - mse: 0.0841 - val_loss: 0.1363 - val_mse: 0.1363\n",
            "Epoch 466/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0839 - mse: 0.0839 - val_loss: 0.1283 - val_mse: 0.1283\n",
            "Epoch 467/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0853 - mse: 0.0853 - val_loss: 0.1251 - val_mse: 0.1251\n",
            "Epoch 468/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0839 - mse: 0.0839 - val_loss: 0.1270 - val_mse: 0.1270\n",
            "Epoch 469/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0834 - mse: 0.0834 - val_loss: 0.1244 - val_mse: 0.1244\n",
            "Epoch 470/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0841 - mse: 0.0841 - val_loss: 0.1175 - val_mse: 0.1175\n",
            "Epoch 471/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0846 - mse: 0.0846 - val_loss: 0.1171 - val_mse: 0.1171\n",
            "Epoch 472/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0836 - mse: 0.0836 - val_loss: 0.1248 - val_mse: 0.1248\n",
            "Epoch 473/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0834 - mse: 0.0834 - val_loss: 0.1224 - val_mse: 0.1224\n",
            "Epoch 474/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1191 - val_mse: 0.1191\n",
            "Epoch 475/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0834 - mse: 0.0834 - val_loss: 0.1228 - val_mse: 0.1228\n",
            "Epoch 476/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0832 - mse: 0.0832 - val_loss: 0.1254 - val_mse: 0.1254\n",
            "Epoch 477/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0845 - mse: 0.0845 - val_loss: 0.1279 - val_mse: 0.1279\n",
            "Epoch 478/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0837 - mse: 0.0837 - val_loss: 0.1198 - val_mse: 0.1198\n",
            "Epoch 479/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0837 - mse: 0.0837 - val_loss: 0.1224 - val_mse: 0.1224\n",
            "Epoch 480/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.1162 - val_mse: 0.1162\n",
            "Epoch 481/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0832 - mse: 0.0832 - val_loss: 0.1225 - val_mse: 0.1225\n",
            "Epoch 482/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.1183 - val_mse: 0.1183\n",
            "Epoch 483/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0840 - mse: 0.0840 - val_loss: 0.1193 - val_mse: 0.1193\n",
            "Epoch 484/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1143 - val_mse: 0.1143\n",
            "Epoch 485/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0815 - mse: 0.0815 - val_loss: 0.1254 - val_mse: 0.1254\n",
            "Epoch 486/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0828 - mse: 0.0828 - val_loss: 0.1245 - val_mse: 0.1245\n",
            "Epoch 487/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0835 - mse: 0.0835 - val_loss: 0.1237 - val_mse: 0.1237\n",
            "Epoch 488/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0839 - mse: 0.0839 - val_loss: 0.1294 - val_mse: 0.1294\n",
            "Epoch 489/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0835 - mse: 0.0835 - val_loss: 0.1306 - val_mse: 0.1306\n",
            "Epoch 490/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1381 - val_mse: 0.1381\n",
            "Epoch 491/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1171 - val_mse: 0.1171\n",
            "Epoch 492/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0826 - mse: 0.0826 - val_loss: 0.1312 - val_mse: 0.1312\n",
            "Epoch 493/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0829 - mse: 0.0829 - val_loss: 0.1145 - val_mse: 0.1145\n",
            "Epoch 494/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0829 - mse: 0.0829 - val_loss: 0.1228 - val_mse: 0.1228\n",
            "Epoch 495/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0823 - mse: 0.0823 - val_loss: 0.1208 - val_mse: 0.1208\n",
            "Epoch 496/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0836 - mse: 0.0836 - val_loss: 0.1181 - val_mse: 0.1181\n",
            "Epoch 497/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0829 - mse: 0.0829 - val_loss: 0.1176 - val_mse: 0.1176\n",
            "Epoch 498/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0835 - mse: 0.0835 - val_loss: 0.1201 - val_mse: 0.1201\n",
            "Epoch 499/500\n",
            "931/931 [==============================] - 5s 5ms/step - loss: 0.0832 - mse: 0.0832 - val_loss: 0.1194 - val_mse: 0.1194\n",
            "Epoch 500/500\n",
            "931/931 [==============================] - 5s 6ms/step - loss: 0.0831 - mse: 0.0831 - val_loss: 0.1236 - val_mse: 0.1236\n",
            "466/466 [==============================] - 1s 3ms/step - loss: 0.0825 - mse: 0.0825\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.1236 - mse: 0.1236\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras import Input\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "train_scores = []\n",
        "vald_scores = []\n",
        "\n",
        "dropout_rates = [0, 0.05, 0.1, 0.25, 0.5]\n",
        "\n",
        "# Optimal dropout rate is 0.05\n",
        "model = models.Sequential() # define that we will use the sequential method to build the ANN\n",
        "model.add(Input(shape=(X_train_stand.shape[1],))) # Input layer (has cells equal to number of features)\n",
        "#model.add(layers.Dropout(0.05))\n",
        "model.add(layers.Dense(160, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "#model.add(layers.Dropout(0.05))\n",
        "model.add(layers.Dense(1,activation = 'linear',kernel_initializer='glorot_normal', bias_initializer='zeros')) # Output Layer\n",
        "model.compile(optimizer = Adam(lr=0.001), loss = 'mse', metrics = ['mse'])\n",
        "model.fit(X_train_stand,y_train_stand, epochs = 500, batch_size = 16,verbose = 1, validation_data=(X_test_stand, y_test_stand))\n",
        "\n",
        "# Evaluate model on training and testing data\n",
        "train_score = model.evaluate(X_train_stand, y_train_stand)\n",
        "vald_score = model.evaluate(X_test_stand, y_test_stand)"
      ],
      "id": "hyGpNtmDIUHN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCt1Xgf1RXsD"
      },
      "source": [
        "### Model with Validation and Dropout (model)"
      ],
      "id": "xCt1Xgf1RXsD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPzzhXTaI2mE",
        "outputId": "555c2b0a-d645-4285-c56a-fe1cd9e27f31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "931/931 [==============================] - 2s 2ms/step - loss: 0.1785 - mse: 0.1785 - val_loss: 0.1699 - val_mse: 0.1699\n",
            "Epoch 37/500\n",
            "931/931 [==============================] - 2s 2ms/step - loss: 0.1795 - mse: 0.1795 - val_loss: 0.1677 - val_mse: 0.1677\n",
            "Epoch 38/500\n",
            "931/931 [==============================] - 2s 2ms/step - loss: 0.1780 - mse: 0.1780 - val_loss: 0.1700 - val_mse: 0.1700\n",
            "Epoch 39/500\n",
            "931/931 [==============================] - 2s 2ms/step - loss: 0.1748 - mse: 0.1748 - val_loss: 0.1684 - val_mse: 0.1684\n",
            "Epoch 40/500\n",
            "931/931 [==============================] - 2s 2ms/step - loss: 0.1764 - mse: 0.1764 - val_loss: 0.1809 - val_mse: 0.1809\n",
            "Epoch 41/500\n",
            "931/931 [==============================] - 2s 2ms/step - loss: 0.1766 - mse: 0.1766 - val_loss: 0.1721 - val_mse: 0.1721\n",
            "Epoch 42/500\n",
            "931/931 [==============================] - 2s 2ms/step - loss: 0.1763 - mse: 0.1763 - val_loss: 0.1618 - val_mse: 0.1618\n",
            "Epoch 43/500\n",
            "347/931 [==========>...................] - ETA: 0s - loss: 0.1781 - mse: 0.1781"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras import Input\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "train_scores = []\n",
        "vald_scores = []\n",
        "\n",
        "# Optimal dropout rate is 0.05\n",
        "model = models.Sequential() # define that we will use the sequential method to build the ANN\n",
        "model.add(Input(shape=(X_train_stand.shape[1],))) # Input layer (has cells equal to number of features)\n",
        "model.add(layers.Dropout(0.05))\n",
        "model.add(layers.Dense(160, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "model.add(layers.Dropout(0.05))\n",
        "model.add(layers.Dense(1,activation = 'linear',kernel_initializer='glorot_normal', bias_initializer='zeros')) # Output Layer\n",
        "model.compile(optimizer = Adam(lr=0.001), loss = 'mse', metrics = ['mse'])\n",
        "fitted_model = model.fit(X_train_stand,y_train_stand, epochs = 500, batch_size = 16,verbose = 1, validation_data=(X_test_stand, y_test_stand))\n",
        "\n",
        "# Evaluate model on training and testing data\n",
        "train_score = model.evaluate(X_train_stand, y_train_stand)\n",
        "vald_score = model.evaluate(X_test_stand, y_test_stand)"
      ],
      "id": "TPzzhXTaI2mE"
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlMzo6LLY_5T",
        "outputId": "6c59133a-d111-4fba-ce2d-64e14744a7c4"
      },
      "id": "UlMzo6LLY_5T",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7f4e9b637650>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_loss = pd.DataFrame(model.history.history.keys())"
      ],
      "metadata": {
        "id": "oPi_qcryZDIQ"
      },
      "id": "oPi_qcryZDIQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_loss.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "ccDGjH1ZZEQI",
        "outputId": "0b5f26ea-4218-4782-f8ba-b1ebae543fbb"
      },
      "id": "ccDGjH1ZZEQI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-ce5ef9f2eac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    947\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mplot_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/plotting/_matplotlib/__init__.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(data, kind, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ax\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left_ax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mplot_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPLOT_CLASSES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/plotting/_matplotlib/core.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_plot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_subplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/plotting/_matplotlib/core.py\u001b[0m in \u001b[0;36m_compute_plot_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# no non-numeric frames or series allowed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no numeric data to plot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;31m# GH25587: cast ExtensionArray of pandas (IntegerArray, etc.) to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: no numeric data to plot"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "QjJNHvWzd3jm",
        "outputId": "124eeeb6-0c61-4bd1-d79b-b6aa356b5f0d"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-dd94a6926206>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    947\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mplot_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/plotting/_matplotlib/__init__.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(data, kind, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ax\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left_ax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mplot_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPLOT_CLASSES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/plotting/_matplotlib/core.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_plot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_subplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/plotting/_matplotlib/core.py\u001b[0m in \u001b[0;36m_compute_plot_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# no non-numeric frames or series allowed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no numeric data to plot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;31m# GH25587: cast ExtensionArray of pandas (IntegerArray, etc.) to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: no numeric data to plot"
          ]
        }
      ],
      "source": [
        "model_loss = pd.DataFrame(model.history.history)\n",
        "model_loss.plot()"
      ],
      "id": "QjJNHvWzd3jm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_rgMEsXR_74"
      },
      "source": [
        "## ANN Architecture"
      ],
      "id": "3_rgMEsXR_74"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C1s8stmSDZT"
      },
      "source": [
        "### Medium Big Brain (mb_model)"
      ],
      "id": "-C1s8stmSDZT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "av16tjFYRXik",
        "outputId": "8c2ae0e8-1e20-4a34-8dd2-cec3afcd60c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "931/931 [==============================] - 4s 3ms/step - loss: 0.2656 - mse: 0.2656 - val_loss: 0.2317 - val_mse: 0.2317\n",
            "Epoch 2/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.2187 - mse: 0.2187 - val_loss: 0.2002 - val_mse: 0.2002\n",
            "Epoch 3/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.2023 - mse: 0.2023 - val_loss: 0.2092 - val_mse: 0.2092\n",
            "Epoch 4/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1951 - mse: 0.1951 - val_loss: 0.1869 - val_mse: 0.1869\n",
            "Epoch 5/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1906 - mse: 0.1906 - val_loss: 0.1949 - val_mse: 0.1949\n",
            "Epoch 6/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1860 - mse: 0.1860 - val_loss: 0.1955 - val_mse: 0.1955\n",
            "Epoch 7/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1768 - mse: 0.1768 - val_loss: 0.1659 - val_mse: 0.1659\n",
            "Epoch 8/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1747 - mse: 0.1747 - val_loss: 0.1570 - val_mse: 0.1570\n",
            "Epoch 9/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1700 - mse: 0.1700 - val_loss: 0.1608 - val_mse: 0.1608\n",
            "Epoch 10/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1676 - mse: 0.1676 - val_loss: 0.1649 - val_mse: 0.1649\n",
            "Epoch 11/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1660 - mse: 0.1660 - val_loss: 0.1782 - val_mse: 0.1782\n",
            "Epoch 12/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1645 - mse: 0.1645 - val_loss: 0.1508 - val_mse: 0.1508\n",
            "Epoch 13/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1590 - mse: 0.1590 - val_loss: 0.1461 - val_mse: 0.1461\n",
            "Epoch 14/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1556 - mse: 0.1556 - val_loss: 0.1716 - val_mse: 0.1716\n",
            "Epoch 15/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1559 - mse: 0.1559 - val_loss: 0.1491 - val_mse: 0.1491\n",
            "Epoch 16/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1500 - mse: 0.1500 - val_loss: 0.1409 - val_mse: 0.1409\n",
            "Epoch 17/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1474 - mse: 0.1474 - val_loss: 0.1412 - val_mse: 0.1412\n",
            "Epoch 18/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1477 - mse: 0.1477 - val_loss: 0.1494 - val_mse: 0.1494\n",
            "Epoch 19/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1498 - mse: 0.1498 - val_loss: 0.1400 - val_mse: 0.1400\n",
            "Epoch 20/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1422 - mse: 0.1422 - val_loss: 0.1364 - val_mse: 0.1364\n",
            "Epoch 21/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1416 - mse: 0.1416 - val_loss: 0.1307 - val_mse: 0.1307\n",
            "Epoch 22/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1417 - mse: 0.1417 - val_loss: 0.1335 - val_mse: 0.1335\n",
            "Epoch 23/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1374 - mse: 0.1374 - val_loss: 0.1273 - val_mse: 0.1273\n",
            "Epoch 24/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1395 - mse: 0.1395 - val_loss: 0.1327 - val_mse: 0.1327\n",
            "Epoch 25/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1398 - mse: 0.1398 - val_loss: 0.1328 - val_mse: 0.1328\n",
            "Epoch 26/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1375 - mse: 0.1375 - val_loss: 0.1456 - val_mse: 0.1456\n",
            "Epoch 27/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1358 - mse: 0.1358 - val_loss: 0.1439 - val_mse: 0.1439\n",
            "Epoch 28/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1350 - mse: 0.1350 - val_loss: 0.1259 - val_mse: 0.1259\n",
            "Epoch 29/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1378 - mse: 0.1378 - val_loss: 0.1351 - val_mse: 0.1351\n",
            "Epoch 30/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1350 - mse: 0.1350 - val_loss: 0.1294 - val_mse: 0.1294\n",
            "Epoch 31/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1317 - mse: 0.1317 - val_loss: 0.1311 - val_mse: 0.1311\n",
            "Epoch 32/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1327 - mse: 0.1327 - val_loss: 0.1268 - val_mse: 0.1268\n",
            "Epoch 33/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1311 - mse: 0.1311 - val_loss: 0.1350 - val_mse: 0.1350\n",
            "Epoch 34/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1318 - mse: 0.1318 - val_loss: 0.1275 - val_mse: 0.1275\n",
            "Epoch 35/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1288 - mse: 0.1288 - val_loss: 0.1320 - val_mse: 0.1320\n",
            "Epoch 36/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1275 - mse: 0.1275 - val_loss: 0.1304 - val_mse: 0.1304\n",
            "Epoch 37/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1293 - mse: 0.1293 - val_loss: 0.1213 - val_mse: 0.1213\n",
            "Epoch 38/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1280 - mse: 0.1280 - val_loss: 0.1242 - val_mse: 0.1242\n",
            "Epoch 39/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1297 - mse: 0.1297 - val_loss: 0.1242 - val_mse: 0.1242\n",
            "Epoch 40/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1273 - mse: 0.1273 - val_loss: 0.1226 - val_mse: 0.1226\n",
            "Epoch 41/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1256 - mse: 0.1256 - val_loss: 0.1228 - val_mse: 0.1228\n",
            "Epoch 42/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1273 - mse: 0.1273 - val_loss: 0.1275 - val_mse: 0.1275\n",
            "Epoch 43/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1203 - mse: 0.1203 - val_loss: 0.1271 - val_mse: 0.1271\n",
            "Epoch 44/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1232 - mse: 0.1232 - val_loss: 0.1286 - val_mse: 0.1286\n",
            "Epoch 45/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1236 - mse: 0.1236 - val_loss: 0.1212 - val_mse: 0.1212\n",
            "Epoch 46/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1217 - mse: 0.1217 - val_loss: 0.1202 - val_mse: 0.1202\n",
            "Epoch 47/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1226 - mse: 0.1226 - val_loss: 0.1230 - val_mse: 0.1230\n",
            "Epoch 48/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1218 - mse: 0.1218 - val_loss: 0.1209 - val_mse: 0.1209\n",
            "Epoch 49/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1212 - mse: 0.1212 - val_loss: 0.1235 - val_mse: 0.1235\n",
            "Epoch 50/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1223 - mse: 0.1223 - val_loss: 0.1190 - val_mse: 0.1190\n",
            "Epoch 51/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1208 - mse: 0.1208 - val_loss: 0.1323 - val_mse: 0.1323\n",
            "Epoch 52/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1197 - mse: 0.1197 - val_loss: 0.1245 - val_mse: 0.1245\n",
            "Epoch 53/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1181 - mse: 0.1181 - val_loss: 0.1204 - val_mse: 0.1204\n",
            "Epoch 54/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1176 - mse: 0.1176 - val_loss: 0.1184 - val_mse: 0.1184\n",
            "Epoch 55/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1171 - mse: 0.1171 - val_loss: 0.1175 - val_mse: 0.1175\n",
            "Epoch 56/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1216 - mse: 0.1216 - val_loss: 0.1171 - val_mse: 0.1171\n",
            "Epoch 57/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1175 - mse: 0.1175 - val_loss: 0.1220 - val_mse: 0.1220\n",
            "Epoch 58/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1188 - mse: 0.1188 - val_loss: 0.1223 - val_mse: 0.1223\n",
            "Epoch 59/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1162 - mse: 0.1162 - val_loss: 0.1222 - val_mse: 0.1222\n",
            "Epoch 60/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1157 - mse: 0.1157 - val_loss: 0.1194 - val_mse: 0.1194\n",
            "Epoch 61/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1192 - mse: 0.1192 - val_loss: 0.1202 - val_mse: 0.1202\n",
            "Epoch 62/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1164 - mse: 0.1164 - val_loss: 0.1194 - val_mse: 0.1194\n",
            "Epoch 63/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1159 - mse: 0.1159 - val_loss: 0.1152 - val_mse: 0.1152\n",
            "Epoch 64/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1160 - mse: 0.1160 - val_loss: 0.1168 - val_mse: 0.1168\n",
            "Epoch 65/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1115 - mse: 0.1115 - val_loss: 0.1245 - val_mse: 0.1245\n",
            "Epoch 66/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1132 - mse: 0.1132 - val_loss: 0.1162 - val_mse: 0.1162\n",
            "Epoch 67/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1142 - mse: 0.1142 - val_loss: 0.1174 - val_mse: 0.1174\n",
            "Epoch 68/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1119 - mse: 0.1119 - val_loss: 0.1180 - val_mse: 0.1180\n",
            "Epoch 69/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1146 - mse: 0.1146 - val_loss: 0.1167 - val_mse: 0.1167\n",
            "Epoch 70/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1123 - mse: 0.1123 - val_loss: 0.1128 - val_mse: 0.1128\n",
            "Epoch 71/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1123 - mse: 0.1123 - val_loss: 0.1132 - val_mse: 0.1132\n",
            "Epoch 72/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1080 - mse: 0.1080 - val_loss: 0.1142 - val_mse: 0.1142\n",
            "Epoch 73/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1125 - mse: 0.1125 - val_loss: 0.1228 - val_mse: 0.1228\n",
            "Epoch 74/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1131 - mse: 0.1131 - val_loss: 0.1186 - val_mse: 0.1186\n",
            "Epoch 75/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1110 - mse: 0.1110 - val_loss: 0.1131 - val_mse: 0.1131\n",
            "Epoch 76/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1130 - mse: 0.1130 - val_loss: 0.1151 - val_mse: 0.1151\n",
            "Epoch 77/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1082 - mse: 0.1082 - val_loss: 0.1205 - val_mse: 0.1205\n",
            "Epoch 78/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1118 - mse: 0.1118 - val_loss: 0.1162 - val_mse: 0.1162\n",
            "Epoch 79/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1130 - mse: 0.1130 - val_loss: 0.1147 - val_mse: 0.1147\n",
            "Epoch 80/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1096 - mse: 0.1096 - val_loss: 0.1159 - val_mse: 0.1159\n",
            "Epoch 81/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1112 - mse: 0.1112 - val_loss: 0.1174 - val_mse: 0.1174\n",
            "Epoch 82/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1123 - mse: 0.1123 - val_loss: 0.1132 - val_mse: 0.1132\n",
            "Epoch 83/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1070 - mse: 0.1070 - val_loss: 0.1171 - val_mse: 0.1171\n",
            "Epoch 84/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1055 - mse: 0.1055 - val_loss: 0.1156 - val_mse: 0.1156\n",
            "Epoch 85/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1085 - mse: 0.1085 - val_loss: 0.1155 - val_mse: 0.1155\n",
            "Epoch 86/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1084 - mse: 0.1084 - val_loss: 0.1215 - val_mse: 0.1215\n",
            "Epoch 87/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1093 - mse: 0.1093 - val_loss: 0.1113 - val_mse: 0.1113\n",
            "Epoch 88/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1072 - mse: 0.1072 - val_loss: 0.1152 - val_mse: 0.1152\n",
            "Epoch 89/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1084 - mse: 0.1084 - val_loss: 0.1112 - val_mse: 0.1112\n",
            "Epoch 90/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1067 - mse: 0.1067 - val_loss: 0.1120 - val_mse: 0.1120\n",
            "Epoch 91/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1092 - mse: 0.1092 - val_loss: 0.1178 - val_mse: 0.1178\n",
            "Epoch 92/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1075 - mse: 0.1075 - val_loss: 0.1176 - val_mse: 0.1176\n",
            "Epoch 93/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1076 - mse: 0.1076 - val_loss: 0.1115 - val_mse: 0.1115\n",
            "Epoch 94/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1065 - mse: 0.1065 - val_loss: 0.1131 - val_mse: 0.1131\n",
            "Epoch 95/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1080 - mse: 0.1080 - val_loss: 0.1088 - val_mse: 0.1088\n",
            "Epoch 96/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1056 - mse: 0.1056 - val_loss: 0.1165 - val_mse: 0.1165\n",
            "Epoch 97/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1053 - mse: 0.1053 - val_loss: 0.1128 - val_mse: 0.1128\n",
            "Epoch 98/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1072 - mse: 0.1072 - val_loss: 0.1170 - val_mse: 0.1170\n",
            "Epoch 99/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1042 - mse: 0.1042 - val_loss: 0.1134 - val_mse: 0.1134\n",
            "Epoch 100/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1061 - mse: 0.1061 - val_loss: 0.1093 - val_mse: 0.1093\n",
            "Epoch 101/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1041 - mse: 0.1041 - val_loss: 0.1170 - val_mse: 0.1170\n",
            "Epoch 102/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1058 - mse: 0.1058 - val_loss: 0.1152 - val_mse: 0.1152\n",
            "Epoch 103/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1052 - mse: 0.1052 - val_loss: 0.1277 - val_mse: 0.1277\n",
            "Epoch 104/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1060 - mse: 0.1060 - val_loss: 0.1134 - val_mse: 0.1134\n",
            "Epoch 105/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1048 - mse: 0.1048 - val_loss: 0.1186 - val_mse: 0.1186\n",
            "Epoch 106/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1027 - mse: 0.1027 - val_loss: 0.1095 - val_mse: 0.1095\n",
            "Epoch 107/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1026 - mse: 0.1026 - val_loss: 0.1126 - val_mse: 0.1126\n",
            "Epoch 108/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1053 - mse: 0.1053 - val_loss: 0.1142 - val_mse: 0.1142\n",
            "Epoch 109/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1027 - mse: 0.1027 - val_loss: 0.1118 - val_mse: 0.1118\n",
            "Epoch 110/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1047 - mse: 0.1047 - val_loss: 0.1154 - val_mse: 0.1154\n",
            "Epoch 111/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1057 - mse: 0.1057 - val_loss: 0.1141 - val_mse: 0.1141\n",
            "Epoch 112/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1025 - mse: 0.1025 - val_loss: 0.1145 - val_mse: 0.1145\n",
            "Epoch 113/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1060 - mse: 0.1060 - val_loss: 0.1090 - val_mse: 0.1090\n",
            "Epoch 114/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1025 - mse: 0.1025 - val_loss: 0.1123 - val_mse: 0.1123\n",
            "Epoch 115/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1027 - mse: 0.1027 - val_loss: 0.1092 - val_mse: 0.1092\n",
            "Epoch 116/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1009 - mse: 0.1009 - val_loss: 0.1083 - val_mse: 0.1083\n",
            "Epoch 117/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1034 - mse: 0.1034 - val_loss: 0.1156 - val_mse: 0.1156\n",
            "Epoch 118/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1038 - mse: 0.1038 - val_loss: 0.1167 - val_mse: 0.1167\n",
            "Epoch 119/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1023 - mse: 0.1023 - val_loss: 0.1098 - val_mse: 0.1098\n",
            "Epoch 120/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1036 - mse: 0.1036 - val_loss: 0.1145 - val_mse: 0.1145\n",
            "Epoch 121/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1019 - mse: 0.1019 - val_loss: 0.1095 - val_mse: 0.1095\n",
            "Epoch 122/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1024 - mse: 0.1024 - val_loss: 0.1195 - val_mse: 0.1195\n",
            "Epoch 123/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1032 - mse: 0.1032 - val_loss: 0.1102 - val_mse: 0.1102\n",
            "Epoch 124/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1007 - mse: 0.1007 - val_loss: 0.1104 - val_mse: 0.1104\n",
            "Epoch 125/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1018 - mse: 0.1018 - val_loss: 0.1116 - val_mse: 0.1116\n",
            "Epoch 126/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1009 - mse: 0.1009 - val_loss: 0.1096 - val_mse: 0.1096\n",
            "Epoch 127/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1025 - mse: 0.1025 - val_loss: 0.1090 - val_mse: 0.1090\n",
            "Epoch 128/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1030 - mse: 0.1030 - val_loss: 0.1104 - val_mse: 0.1104\n",
            "Epoch 129/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1023 - mse: 0.1023 - val_loss: 0.1184 - val_mse: 0.1184\n",
            "Epoch 130/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1026 - mse: 0.1026 - val_loss: 0.1089 - val_mse: 0.1089\n",
            "Epoch 131/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0998 - mse: 0.0998 - val_loss: 0.1082 - val_mse: 0.1082\n",
            "Epoch 132/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1004 - mse: 0.1004 - val_loss: 0.1073 - val_mse: 0.1073\n",
            "Epoch 133/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1013 - mse: 0.1013 - val_loss: 0.1101 - val_mse: 0.1101\n",
            "Epoch 134/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1002 - mse: 0.1002 - val_loss: 0.1091 - val_mse: 0.1091\n",
            "Epoch 135/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1016 - mse: 0.1016 - val_loss: 0.1098 - val_mse: 0.1098\n",
            "Epoch 136/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0997 - mse: 0.0997 - val_loss: 0.1098 - val_mse: 0.1098\n",
            "Epoch 137/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0999 - mse: 0.0999 - val_loss: 0.1128 - val_mse: 0.1128\n",
            "Epoch 138/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1018 - mse: 0.1018 - val_loss: 0.1135 - val_mse: 0.1135\n",
            "Epoch 139/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1004 - mse: 0.1004 - val_loss: 0.1088 - val_mse: 0.1088\n",
            "Epoch 140/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.1118 - val_mse: 0.1118\n",
            "Epoch 141/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1001 - mse: 0.1001 - val_loss: 0.1099 - val_mse: 0.1099\n",
            "Epoch 142/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1016 - mse: 0.1016 - val_loss: 0.1103 - val_mse: 0.1103\n",
            "Epoch 143/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1011 - mse: 0.1011 - val_loss: 0.1132 - val_mse: 0.1132\n",
            "Epoch 144/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1015 - mse: 0.1015 - val_loss: 0.1083 - val_mse: 0.1083\n",
            "Epoch 145/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.1128 - val_mse: 0.1128\n",
            "Epoch 146/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0998 - mse: 0.0998 - val_loss: 0.1147 - val_mse: 0.1147\n",
            "Epoch 147/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1001 - mse: 0.1001 - val_loss: 0.1102 - val_mse: 0.1102\n",
            "Epoch 148/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0981 - mse: 0.0981 - val_loss: 0.1073 - val_mse: 0.1073\n",
            "Epoch 149/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1001 - mse: 0.1001 - val_loss: 0.1118 - val_mse: 0.1118\n",
            "Epoch 150/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0981 - mse: 0.0981 - val_loss: 0.1105 - val_mse: 0.1105\n",
            "Epoch 151/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.1141 - val_mse: 0.1141\n",
            "Epoch 152/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.1057 - val_mse: 0.1057\n",
            "Epoch 153/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0973 - mse: 0.0973 - val_loss: 0.1078 - val_mse: 0.1078\n",
            "Epoch 154/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0971 - mse: 0.0971 - val_loss: 0.1078 - val_mse: 0.1078\n",
            "Epoch 155/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.1216 - val_mse: 0.1216\n",
            "Epoch 156/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.1119 - val_mse: 0.1119\n",
            "Epoch 157/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.1149 - val_mse: 0.1149\n",
            "Epoch 158/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0991 - mse: 0.0991 - val_loss: 0.1083 - val_mse: 0.1083\n",
            "Epoch 159/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0962 - mse: 0.0962 - val_loss: 0.1114 - val_mse: 0.1114\n",
            "Epoch 160/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0975 - mse: 0.0975 - val_loss: 0.1138 - val_mse: 0.1138\n",
            "Epoch 161/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.1052 - val_mse: 0.1052\n",
            "Epoch 162/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.1139 - val_mse: 0.1139\n",
            "Epoch 163/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.1026 - mse: 0.1026 - val_loss: 0.1106 - val_mse: 0.1106\n",
            "Epoch 164/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0972 - mse: 0.0972 - val_loss: 0.1065 - val_mse: 0.1065\n",
            "Epoch 165/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0985 - mse: 0.0985 - val_loss: 0.1075 - val_mse: 0.1075\n",
            "Epoch 166/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0958 - mse: 0.0958 - val_loss: 0.1075 - val_mse: 0.1075\n",
            "Epoch 167/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0956 - mse: 0.0956 - val_loss: 0.1077 - val_mse: 0.1077\n",
            "Epoch 168/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0996 - mse: 0.0996 - val_loss: 0.1101 - val_mse: 0.1101\n",
            "Epoch 169/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0964 - mse: 0.0964 - val_loss: 0.1085 - val_mse: 0.1085\n",
            "Epoch 170/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.1079 - val_mse: 0.1079\n",
            "Epoch 171/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0971 - mse: 0.0971 - val_loss: 0.1141 - val_mse: 0.1141\n",
            "Epoch 172/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.1108 - val_mse: 0.1108\n",
            "Epoch 173/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0927 - mse: 0.0927 - val_loss: 0.1140 - val_mse: 0.1140\n",
            "Epoch 174/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0975 - mse: 0.0975 - val_loss: 0.1051 - val_mse: 0.1051\n",
            "Epoch 175/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.1178 - val_mse: 0.1178\n",
            "Epoch 176/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0962 - mse: 0.0962 - val_loss: 0.1093 - val_mse: 0.1093\n",
            "Epoch 177/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0970 - mse: 0.0970 - val_loss: 0.1117 - val_mse: 0.1117\n",
            "Epoch 178/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.1041 - val_mse: 0.1041\n",
            "Epoch 179/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0961 - mse: 0.0961 - val_loss: 0.1050 - val_mse: 0.1050\n",
            "Epoch 180/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0931 - mse: 0.0931 - val_loss: 0.1092 - val_mse: 0.1092\n",
            "Epoch 181/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0957 - mse: 0.0957 - val_loss: 0.1077 - val_mse: 0.1077\n",
            "Epoch 182/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0981 - mse: 0.0981 - val_loss: 0.1031 - val_mse: 0.1031\n",
            "Epoch 183/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.1096 - val_mse: 0.1096\n",
            "Epoch 184/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0972 - mse: 0.0972 - val_loss: 0.1066 - val_mse: 0.1066\n",
            "Epoch 185/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0985 - mse: 0.0985 - val_loss: 0.1108 - val_mse: 0.1108\n",
            "Epoch 186/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.1069 - val_mse: 0.1069\n",
            "Epoch 187/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0943 - mse: 0.0943 - val_loss: 0.1064 - val_mse: 0.1064\n",
            "Epoch 188/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0955 - mse: 0.0955 - val_loss: 0.1076 - val_mse: 0.1076\n",
            "Epoch 189/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0945 - mse: 0.0945 - val_loss: 0.1083 - val_mse: 0.1083\n",
            "Epoch 190/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0963 - mse: 0.0963 - val_loss: 0.1070 - val_mse: 0.1070\n",
            "Epoch 191/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0952 - mse: 0.0952 - val_loss: 0.1060 - val_mse: 0.1060\n",
            "Epoch 192/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0952 - mse: 0.0952 - val_loss: 0.1083 - val_mse: 0.1083\n",
            "Epoch 193/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.1073 - val_mse: 0.1073\n",
            "Epoch 194/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0967 - mse: 0.0967 - val_loss: 0.1077 - val_mse: 0.1077\n",
            "Epoch 195/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0945 - mse: 0.0945 - val_loss: 0.1058 - val_mse: 0.1058\n",
            "Epoch 196/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0960 - mse: 0.0960 - val_loss: 0.1072 - val_mse: 0.1072\n",
            "Epoch 197/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0943 - mse: 0.0943 - val_loss: 0.1048 - val_mse: 0.1048\n",
            "Epoch 198/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0971 - mse: 0.0971 - val_loss: 0.1056 - val_mse: 0.1056\n",
            "Epoch 199/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0947 - mse: 0.0947 - val_loss: 0.1092 - val_mse: 0.1092\n",
            "Epoch 200/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0930 - mse: 0.0930 - val_loss: 0.1081 - val_mse: 0.1081\n",
            "Epoch 201/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0961 - mse: 0.0961 - val_loss: 0.1087 - val_mse: 0.1087\n",
            "Epoch 202/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0966 - mse: 0.0966 - val_loss: 0.1068 - val_mse: 0.1068\n",
            "Epoch 203/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0962 - mse: 0.0962 - val_loss: 0.1085 - val_mse: 0.1085\n",
            "Epoch 204/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0950 - mse: 0.0950 - val_loss: 0.1076 - val_mse: 0.1076\n",
            "Epoch 205/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0930 - mse: 0.0930 - val_loss: 0.1095 - val_mse: 0.1095\n",
            "Epoch 206/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0961 - mse: 0.0961 - val_loss: 0.1058 - val_mse: 0.1058\n",
            "Epoch 207/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0921 - mse: 0.0921 - val_loss: 0.1047 - val_mse: 0.1047\n",
            "Epoch 208/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0926 - mse: 0.0926 - val_loss: 0.1062 - val_mse: 0.1062\n",
            "Epoch 209/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0938 - mse: 0.0938 - val_loss: 0.1061 - val_mse: 0.1061\n",
            "Epoch 210/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0963 - mse: 0.0963 - val_loss: 0.1066 - val_mse: 0.1066\n",
            "Epoch 211/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0965 - mse: 0.0965 - val_loss: 0.1055 - val_mse: 0.1055\n",
            "Epoch 212/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0939 - mse: 0.0939 - val_loss: 0.1100 - val_mse: 0.1100\n",
            "Epoch 213/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1055 - val_mse: 0.1055\n",
            "Epoch 214/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0937 - mse: 0.0937 - val_loss: 0.1087 - val_mse: 0.1087\n",
            "Epoch 215/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.1039 - val_mse: 0.1039\n",
            "Epoch 216/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0947 - mse: 0.0947 - val_loss: 0.1094 - val_mse: 0.1094\n",
            "Epoch 217/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0939 - mse: 0.0939 - val_loss: 0.1093 - val_mse: 0.1093\n",
            "Epoch 218/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0931 - mse: 0.0931 - val_loss: 0.1074 - val_mse: 0.1074\n",
            "Epoch 219/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0949 - mse: 0.0949 - val_loss: 0.1100 - val_mse: 0.1100\n",
            "Epoch 220/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0946 - mse: 0.0946 - val_loss: 0.1107 - val_mse: 0.1107\n",
            "Epoch 221/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0914 - mse: 0.0914 - val_loss: 0.1057 - val_mse: 0.1057\n",
            "Epoch 222/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0999 - mse: 0.0999 - val_loss: 0.1045 - val_mse: 0.1045\n",
            "Epoch 223/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1012 - val_mse: 0.1012\n",
            "Epoch 224/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0936 - mse: 0.0936 - val_loss: 0.1061 - val_mse: 0.1061\n",
            "Epoch 225/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0919 - mse: 0.0919 - val_loss: 0.1025 - val_mse: 0.1025\n",
            "Epoch 226/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0924 - mse: 0.0924 - val_loss: 0.1097 - val_mse: 0.1097\n",
            "Epoch 227/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.1070 - val_mse: 0.1070\n",
            "Epoch 228/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0935 - mse: 0.0935 - val_loss: 0.1086 - val_mse: 0.1086\n",
            "Epoch 229/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0948 - mse: 0.0948 - val_loss: 0.1073 - val_mse: 0.1073\n",
            "Epoch 230/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0927 - mse: 0.0927 - val_loss: 0.1085 - val_mse: 0.1085\n",
            "Epoch 231/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0955 - mse: 0.0955 - val_loss: 0.1087 - val_mse: 0.1087\n",
            "Epoch 232/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.1156 - val_mse: 0.1156\n",
            "Epoch 233/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0934 - mse: 0.0934 - val_loss: 0.1077 - val_mse: 0.1077\n",
            "Epoch 234/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0938 - mse: 0.0938 - val_loss: 0.1148 - val_mse: 0.1148\n",
            "Epoch 235/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0919 - mse: 0.0919 - val_loss: 0.1016 - val_mse: 0.1016\n",
            "Epoch 236/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0924 - mse: 0.0924 - val_loss: 0.1109 - val_mse: 0.1109\n",
            "Epoch 237/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0923 - mse: 0.0923 - val_loss: 0.1108 - val_mse: 0.1108\n",
            "Epoch 238/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0966 - mse: 0.0966 - val_loss: 0.1063 - val_mse: 0.1063\n",
            "Epoch 239/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.1062 - val_mse: 0.1062\n",
            "Epoch 240/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0945 - mse: 0.0945 - val_loss: 0.1073 - val_mse: 0.1073\n",
            "Epoch 241/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0927 - mse: 0.0927 - val_loss: 0.1090 - val_mse: 0.1090\n",
            "Epoch 242/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0950 - mse: 0.0950 - val_loss: 0.1073 - val_mse: 0.1073\n",
            "Epoch 243/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0918 - mse: 0.0918 - val_loss: 0.1057 - val_mse: 0.1057\n",
            "Epoch 244/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0922 - mse: 0.0922 - val_loss: 0.1086 - val_mse: 0.1086\n",
            "Epoch 245/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0925 - mse: 0.0925 - val_loss: 0.1067 - val_mse: 0.1067\n",
            "Epoch 246/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0941 - mse: 0.0941 - val_loss: 0.1084 - val_mse: 0.1084\n",
            "Epoch 247/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0918 - mse: 0.0918 - val_loss: 0.1040 - val_mse: 0.1040\n",
            "Epoch 248/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0925 - mse: 0.0925 - val_loss: 0.1027 - val_mse: 0.1027\n",
            "Epoch 249/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0933 - mse: 0.0933 - val_loss: 0.1094 - val_mse: 0.1094\n",
            "Epoch 250/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.1102 - val_mse: 0.1102\n",
            "Epoch 251/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0922 - mse: 0.0922 - val_loss: 0.1072 - val_mse: 0.1072\n",
            "Epoch 252/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0907 - mse: 0.0907 - val_loss: 0.1049 - val_mse: 0.1049\n",
            "Epoch 253/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1068 - val_mse: 0.1068\n",
            "Epoch 254/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0941 - mse: 0.0941 - val_loss: 0.1077 - val_mse: 0.1077\n",
            "Epoch 255/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.1129 - val_mse: 0.1129\n",
            "Epoch 256/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0926 - mse: 0.0926 - val_loss: 0.1086 - val_mse: 0.1086\n",
            "Epoch 257/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0912 - mse: 0.0912 - val_loss: 0.1084 - val_mse: 0.1084\n",
            "Epoch 258/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1087 - val_mse: 0.1087\n",
            "Epoch 259/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0923 - mse: 0.0923 - val_loss: 0.1060 - val_mse: 0.1060\n",
            "Epoch 260/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0924 - mse: 0.0924 - val_loss: 0.1092 - val_mse: 0.1092\n",
            "Epoch 261/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0925 - mse: 0.0925 - val_loss: 0.1053 - val_mse: 0.1053\n",
            "Epoch 262/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0948 - mse: 0.0948 - val_loss: 0.1046 - val_mse: 0.1046\n",
            "Epoch 263/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.1067 - val_mse: 0.1067\n",
            "Epoch 264/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.1087 - val_mse: 0.1087\n",
            "Epoch 265/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.1019 - val_mse: 0.1019\n",
            "Epoch 266/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.1050 - val_mse: 0.1050\n",
            "Epoch 267/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0914 - mse: 0.0914 - val_loss: 0.1118 - val_mse: 0.1118\n",
            "Epoch 268/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.1061 - val_mse: 0.1061\n",
            "Epoch 269/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0912 - mse: 0.0912 - val_loss: 0.1031 - val_mse: 0.1031\n",
            "Epoch 270/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0921 - mse: 0.0921 - val_loss: 0.1040 - val_mse: 0.1040\n",
            "Epoch 271/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0938 - mse: 0.0938 - val_loss: 0.1075 - val_mse: 0.1075\n",
            "Epoch 272/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.1107 - val_mse: 0.1107\n",
            "Epoch 273/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0929 - mse: 0.0929 - val_loss: 0.1131 - val_mse: 0.1131\n",
            "Epoch 274/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0892 - mse: 0.0892 - val_loss: 0.1141 - val_mse: 0.1141\n",
            "Epoch 275/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0931 - mse: 0.0931 - val_loss: 0.1058 - val_mse: 0.1058\n",
            "Epoch 276/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0890 - mse: 0.0890 - val_loss: 0.1046 - val_mse: 0.1046\n",
            "Epoch 277/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0922 - mse: 0.0922 - val_loss: 0.1038 - val_mse: 0.1038\n",
            "Epoch 278/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.1056 - val_mse: 0.1056\n",
            "Epoch 279/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0911 - mse: 0.0911 - val_loss: 0.1096 - val_mse: 0.1096\n",
            "Epoch 280/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1050 - val_mse: 0.1050\n",
            "Epoch 281/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0871 - mse: 0.0871 - val_loss: 0.1080 - val_mse: 0.1080\n",
            "Epoch 282/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0924 - mse: 0.0924 - val_loss: 0.1116 - val_mse: 0.1116\n",
            "Epoch 283/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0917 - mse: 0.0917 - val_loss: 0.1036 - val_mse: 0.1036\n",
            "Epoch 284/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0912 - mse: 0.0912 - val_loss: 0.1082 - val_mse: 0.1082\n",
            "Epoch 285/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0918 - mse: 0.0918 - val_loss: 0.1048 - val_mse: 0.1048\n",
            "Epoch 286/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0898 - mse: 0.0898 - val_loss: 0.1097 - val_mse: 0.1097\n",
            "Epoch 287/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0891 - mse: 0.0891 - val_loss: 0.1038 - val_mse: 0.1038\n",
            "Epoch 288/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0918 - mse: 0.0918 - val_loss: 0.1075 - val_mse: 0.1075\n",
            "Epoch 289/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0911 - mse: 0.0911 - val_loss: 0.1117 - val_mse: 0.1117\n",
            "Epoch 290/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0892 - mse: 0.0892 - val_loss: 0.1066 - val_mse: 0.1066\n",
            "Epoch 291/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.1064 - val_mse: 0.1064\n",
            "Epoch 292/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0902 - mse: 0.0902 - val_loss: 0.1083 - val_mse: 0.1083\n",
            "Epoch 293/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0895 - mse: 0.0895 - val_loss: 0.1054 - val_mse: 0.1054\n",
            "Epoch 294/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0945 - mse: 0.0945 - val_loss: 0.1102 - val_mse: 0.1102\n",
            "Epoch 295/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0895 - mse: 0.0895 - val_loss: 0.1095 - val_mse: 0.1095\n",
            "Epoch 296/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0926 - mse: 0.0926 - val_loss: 0.1134 - val_mse: 0.1134\n",
            "Epoch 297/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0895 - mse: 0.0895 - val_loss: 0.1039 - val_mse: 0.1039\n",
            "Epoch 298/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1052 - val_mse: 0.1052\n",
            "Epoch 299/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0910 - mse: 0.0910 - val_loss: 0.1083 - val_mse: 0.1083\n",
            "Epoch 300/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0917 - mse: 0.0917 - val_loss: 0.1051 - val_mse: 0.1051\n",
            "Epoch 301/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.1044 - val_mse: 0.1044\n",
            "Epoch 302/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0907 - mse: 0.0907 - val_loss: 0.1069 - val_mse: 0.1069\n",
            "Epoch 303/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0890 - mse: 0.0890 - val_loss: 0.1100 - val_mse: 0.1100\n",
            "Epoch 304/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0922 - mse: 0.0922 - val_loss: 0.1034 - val_mse: 0.1034\n",
            "Epoch 305/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0918 - mse: 0.0918 - val_loss: 0.1039 - val_mse: 0.1039\n",
            "Epoch 306/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1102 - val_mse: 0.1102\n",
            "Epoch 307/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.1065 - val_mse: 0.1065\n",
            "Epoch 308/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1039 - val_mse: 0.1039\n",
            "Epoch 309/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0907 - mse: 0.0907 - val_loss: 0.1051 - val_mse: 0.1051\n",
            "Epoch 310/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0894 - mse: 0.0894 - val_loss: 0.1054 - val_mse: 0.1054\n",
            "Epoch 311/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0899 - mse: 0.0899 - val_loss: 0.1059 - val_mse: 0.1059\n",
            "Epoch 312/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0898 - mse: 0.0898 - val_loss: 0.1082 - val_mse: 0.1082\n",
            "Epoch 313/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0910 - mse: 0.0910 - val_loss: 0.1061 - val_mse: 0.1061\n",
            "Epoch 314/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.1081 - val_mse: 0.1081\n",
            "Epoch 315/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.1097 - val_mse: 0.1097\n",
            "Epoch 316/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.1031 - val_mse: 0.1031\n",
            "Epoch 317/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0913 - mse: 0.0913 - val_loss: 0.1042 - val_mse: 0.1042\n",
            "Epoch 318/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.1109 - val_mse: 0.1109\n",
            "Epoch 319/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1060 - val_mse: 0.1060\n",
            "Epoch 320/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1066 - val_mse: 0.1066\n",
            "Epoch 321/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1064 - val_mse: 0.1064\n",
            "Epoch 322/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.1088 - val_mse: 0.1088\n",
            "Epoch 323/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0904 - mse: 0.0904 - val_loss: 0.1022 - val_mse: 0.1022\n",
            "Epoch 324/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0917 - mse: 0.0917 - val_loss: 0.1079 - val_mse: 0.1079\n",
            "Epoch 325/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1043 - val_mse: 0.1043\n",
            "Epoch 326/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0899 - mse: 0.0899 - val_loss: 0.1071 - val_mse: 0.1071\n",
            "Epoch 327/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0894 - mse: 0.0894 - val_loss: 0.1053 - val_mse: 0.1053\n",
            "Epoch 328/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.1022 - val_mse: 0.1022\n",
            "Epoch 329/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0914 - mse: 0.0914 - val_loss: 0.0989 - val_mse: 0.0989\n",
            "Epoch 330/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0918 - mse: 0.0918 - val_loss: 0.1054 - val_mse: 0.1054\n",
            "Epoch 331/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0894 - mse: 0.0894 - val_loss: 0.1086 - val_mse: 0.1086\n",
            "Epoch 332/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0898 - mse: 0.0898 - val_loss: 0.1026 - val_mse: 0.1026\n",
            "Epoch 333/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.1072 - val_mse: 0.1072\n",
            "Epoch 334/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0892 - mse: 0.0892 - val_loss: 0.1068 - val_mse: 0.1068\n",
            "Epoch 335/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0878 - mse: 0.0878 - val_loss: 0.1009 - val_mse: 0.1009\n",
            "Epoch 336/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0878 - mse: 0.0878 - val_loss: 0.1028 - val_mse: 0.1028\n",
            "Epoch 337/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0891 - mse: 0.0891 - val_loss: 0.1060 - val_mse: 0.1060\n",
            "Epoch 338/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0893 - mse: 0.0893 - val_loss: 0.1056 - val_mse: 0.1056\n",
            "Epoch 339/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0856 - mse: 0.0856 - val_loss: 0.1064 - val_mse: 0.1064\n",
            "Epoch 340/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.1073 - val_mse: 0.1073\n",
            "Epoch 341/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0880 - mse: 0.0880 - val_loss: 0.1035 - val_mse: 0.1035\n",
            "Epoch 342/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0882 - mse: 0.0882 - val_loss: 0.1039 - val_mse: 0.1039\n",
            "Epoch 343/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.1061 - val_mse: 0.1061\n",
            "Epoch 344/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.1086 - val_mse: 0.1086\n",
            "Epoch 345/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.1057 - val_mse: 0.1057\n",
            "Epoch 346/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.1042 - val_mse: 0.1042\n",
            "Epoch 347/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0890 - mse: 0.0890 - val_loss: 0.1111 - val_mse: 0.1111\n",
            "Epoch 348/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1038 - val_mse: 0.1038\n",
            "Epoch 349/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0866 - mse: 0.0866 - val_loss: 0.1106 - val_mse: 0.1106\n",
            "Epoch 350/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1057 - val_mse: 0.1057\n",
            "Epoch 351/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.1103 - val_mse: 0.1103\n",
            "Epoch 352/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0891 - mse: 0.0891 - val_loss: 0.1080 - val_mse: 0.1080\n",
            "Epoch 353/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0913 - mse: 0.0913 - val_loss: 0.1056 - val_mse: 0.1056\n",
            "Epoch 354/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0879 - mse: 0.0879 - val_loss: 0.1024 - val_mse: 0.1024\n",
            "Epoch 355/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0870 - mse: 0.0870 - val_loss: 0.1020 - val_mse: 0.1020\n",
            "Epoch 356/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.1054 - val_mse: 0.1054\n",
            "Epoch 357/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1087 - val_mse: 0.1087\n",
            "Epoch 358/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0877 - mse: 0.0877 - val_loss: 0.1022 - val_mse: 0.1022\n",
            "Epoch 359/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1037 - val_mse: 0.1037\n",
            "Epoch 360/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0898 - mse: 0.0898 - val_loss: 0.1037 - val_mse: 0.1037\n",
            "Epoch 361/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0866 - mse: 0.0866 - val_loss: 0.1027 - val_mse: 0.1027\n",
            "Epoch 362/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0874 - mse: 0.0874 - val_loss: 0.1040 - val_mse: 0.1040\n",
            "Epoch 363/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0875 - mse: 0.0875 - val_loss: 0.1048 - val_mse: 0.1048\n",
            "Epoch 364/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0870 - mse: 0.0870 - val_loss: 0.1022 - val_mse: 0.1022\n",
            "Epoch 365/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0891 - mse: 0.0891 - val_loss: 0.1121 - val_mse: 0.1121\n",
            "Epoch 366/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.1002 - val_mse: 0.1002\n",
            "Epoch 367/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0866 - mse: 0.0866 - val_loss: 0.1047 - val_mse: 0.1047\n",
            "Epoch 368/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1019 - val_mse: 0.1019\n",
            "Epoch 369/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0872 - mse: 0.0872 - val_loss: 0.1018 - val_mse: 0.1018\n",
            "Epoch 370/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.1037 - val_mse: 0.1037\n",
            "Epoch 371/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0868 - mse: 0.0868 - val_loss: 0.1027 - val_mse: 0.1027\n",
            "Epoch 372/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0898 - mse: 0.0898 - val_loss: 0.1157 - val_mse: 0.1157\n",
            "Epoch 373/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0892 - mse: 0.0892 - val_loss: 0.1073 - val_mse: 0.1073\n",
            "Epoch 374/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0881 - mse: 0.0881 - val_loss: 0.1039 - val_mse: 0.1039\n",
            "Epoch 375/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1035 - val_mse: 0.1035\n",
            "Epoch 376/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.1067 - val_mse: 0.1067\n",
            "Epoch 377/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0899 - mse: 0.0899 - val_loss: 0.1034 - val_mse: 0.1034\n",
            "Epoch 378/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.1099 - val_mse: 0.1099\n",
            "Epoch 379/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0879 - mse: 0.0879 - val_loss: 0.1054 - val_mse: 0.1054\n",
            "Epoch 380/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0874 - mse: 0.0874 - val_loss: 0.1055 - val_mse: 0.1055\n",
            "Epoch 381/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.1035 - val_mse: 0.1035\n",
            "Epoch 382/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.1038 - val_mse: 0.1038\n",
            "Epoch 383/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.1037 - val_mse: 0.1037\n",
            "Epoch 384/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0893 - mse: 0.0893 - val_loss: 0.1037 - val_mse: 0.1037\n",
            "Epoch 385/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.1029 - val_mse: 0.1029\n",
            "Epoch 386/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.1051 - val_mse: 0.1051\n",
            "Epoch 387/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.1060 - val_mse: 0.1060\n",
            "Epoch 388/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0868 - mse: 0.0868 - val_loss: 0.1077 - val_mse: 0.1077\n",
            "Epoch 389/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0865 - mse: 0.0865 - val_loss: 0.1036 - val_mse: 0.1036\n",
            "Epoch 390/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0868 - mse: 0.0868 - val_loss: 0.1059 - val_mse: 0.1059\n",
            "Epoch 391/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1037 - val_mse: 0.1037\n",
            "Epoch 392/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.1055 - val_mse: 0.1055\n",
            "Epoch 393/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0878 - mse: 0.0878 - val_loss: 0.1059 - val_mse: 0.1059\n",
            "Epoch 394/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0871 - mse: 0.0871 - val_loss: 0.1039 - val_mse: 0.1039\n",
            "Epoch 395/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0880 - mse: 0.0880 - val_loss: 0.1031 - val_mse: 0.1031\n",
            "Epoch 396/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0864 - mse: 0.0864 - val_loss: 0.1076 - val_mse: 0.1076\n",
            "Epoch 397/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0865 - mse: 0.0865 - val_loss: 0.1057 - val_mse: 0.1057\n",
            "Epoch 398/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0862 - mse: 0.0862 - val_loss: 0.1023 - val_mse: 0.1023\n",
            "Epoch 399/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0865 - mse: 0.0865 - val_loss: 0.1058 - val_mse: 0.1058\n",
            "Epoch 400/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0864 - mse: 0.0864 - val_loss: 0.1066 - val_mse: 0.1066\n",
            "Epoch 401/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0893 - mse: 0.0893 - val_loss: 0.1008 - val_mse: 0.1008\n",
            "Epoch 402/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.1104 - val_mse: 0.1104\n",
            "Epoch 403/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.1012 - val_mse: 0.1012\n",
            "Epoch 404/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.1021 - val_mse: 0.1021\n",
            "Epoch 405/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.1124 - val_mse: 0.1124\n",
            "Epoch 406/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0880 - mse: 0.0880 - val_loss: 0.1036 - val_mse: 0.1036\n",
            "Epoch 407/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.1043 - val_mse: 0.1043\n",
            "Epoch 408/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0864 - mse: 0.0864 - val_loss: 0.1048 - val_mse: 0.1048\n",
            "Epoch 409/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.1054 - val_mse: 0.1054\n",
            "Epoch 410/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.1008 - val_mse: 0.1008\n",
            "Epoch 411/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.1011 - val_mse: 0.1011\n",
            "Epoch 412/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1033 - val_mse: 0.1033\n",
            "Epoch 413/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0855 - mse: 0.0855 - val_loss: 0.1046 - val_mse: 0.1046\n",
            "Epoch 414/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.1047 - val_mse: 0.1047\n",
            "Epoch 415/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0864 - mse: 0.0864 - val_loss: 0.1016 - val_mse: 0.1016\n",
            "Epoch 416/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0862 - mse: 0.0862 - val_loss: 0.1055 - val_mse: 0.1055\n",
            "Epoch 417/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.1034 - val_mse: 0.1034\n",
            "Epoch 418/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0868 - mse: 0.0868 - val_loss: 0.1052 - val_mse: 0.1052\n",
            "Epoch 419/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1020 - val_mse: 0.1020\n",
            "Epoch 420/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0871 - mse: 0.0871 - val_loss: 0.1039 - val_mse: 0.1039\n",
            "Epoch 421/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0863 - mse: 0.0863 - val_loss: 0.1048 - val_mse: 0.1048\n",
            "Epoch 422/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.1039 - val_mse: 0.1039\n",
            "Epoch 423/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.1061 - val_mse: 0.1061\n",
            "Epoch 424/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.1048 - val_mse: 0.1048\n",
            "Epoch 425/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0880 - mse: 0.0880 - val_loss: 0.1064 - val_mse: 0.1064\n",
            "Epoch 426/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0872 - mse: 0.0872 - val_loss: 0.1030 - val_mse: 0.1030\n",
            "Epoch 427/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0877 - mse: 0.0877 - val_loss: 0.1046 - val_mse: 0.1046\n",
            "Epoch 428/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0879 - mse: 0.0879 - val_loss: 0.1022 - val_mse: 0.1022\n",
            "Epoch 429/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0874 - mse: 0.0874 - val_loss: 0.1072 - val_mse: 0.1072\n",
            "Epoch 430/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0853 - mse: 0.0853 - val_loss: 0.1022 - val_mse: 0.1022\n",
            "Epoch 431/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0862 - mse: 0.0862 - val_loss: 0.1026 - val_mse: 0.1026\n",
            "Epoch 432/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0855 - mse: 0.0855 - val_loss: 0.1052 - val_mse: 0.1052\n",
            "Epoch 433/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0854 - mse: 0.0854 - val_loss: 0.1022 - val_mse: 0.1022\n",
            "Epoch 434/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0891 - mse: 0.0891 - val_loss: 0.1065 - val_mse: 0.1065\n",
            "Epoch 435/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0868 - mse: 0.0868 - val_loss: 0.1008 - val_mse: 0.1008\n",
            "Epoch 436/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.1086 - val_mse: 0.1086\n",
            "Epoch 437/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0984 - val_mse: 0.0984\n",
            "Epoch 438/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0890 - mse: 0.0890 - val_loss: 0.1028 - val_mse: 0.1028\n",
            "Epoch 439/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.1018 - val_mse: 0.1018\n",
            "Epoch 440/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.1012 - val_mse: 0.1012\n",
            "Epoch 441/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0878 - mse: 0.0878 - val_loss: 0.1081 - val_mse: 0.1081\n",
            "Epoch 442/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0874 - mse: 0.0874 - val_loss: 0.1005 - val_mse: 0.1005\n",
            "Epoch 443/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0875 - mse: 0.0875 - val_loss: 0.1102 - val_mse: 0.1102\n",
            "Epoch 444/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0866 - mse: 0.0866 - val_loss: 0.1062 - val_mse: 0.1062\n",
            "Epoch 445/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0852 - mse: 0.0852 - val_loss: 0.1038 - val_mse: 0.1038\n",
            "Epoch 446/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0872 - mse: 0.0872 - val_loss: 0.1043 - val_mse: 0.1043\n",
            "Epoch 447/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0865 - mse: 0.0865 - val_loss: 0.1036 - val_mse: 0.1036\n",
            "Epoch 448/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.1017 - val_mse: 0.1017\n",
            "Epoch 449/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0871 - mse: 0.0871 - val_loss: 0.1037 - val_mse: 0.1037\n",
            "Epoch 450/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1033 - val_mse: 0.1033\n",
            "Epoch 451/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0847 - mse: 0.0847 - val_loss: 0.1045 - val_mse: 0.1045\n",
            "Epoch 452/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0881 - mse: 0.0881 - val_loss: 0.1062 - val_mse: 0.1062\n",
            "Epoch 453/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0845 - mse: 0.0845 - val_loss: 0.1069 - val_mse: 0.1069\n",
            "Epoch 454/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.1043 - val_mse: 0.1043\n",
            "Epoch 455/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.1021 - val_mse: 0.1021\n",
            "Epoch 456/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0877 - mse: 0.0877 - val_loss: 0.1026 - val_mse: 0.1026\n",
            "Epoch 457/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0865 - mse: 0.0865 - val_loss: 0.1070 - val_mse: 0.1070\n",
            "Epoch 458/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.1018 - val_mse: 0.1018\n",
            "Epoch 459/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0862 - mse: 0.0862 - val_loss: 0.1018 - val_mse: 0.1018\n",
            "Epoch 460/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1052 - val_mse: 0.1052\n",
            "Epoch 461/500\n",
            "931/931 [==============================] - 3s 4ms/step - loss: 0.0870 - mse: 0.0870 - val_loss: 0.1168 - val_mse: 0.1168\n",
            "Epoch 462/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0870 - mse: 0.0870 - val_loss: 0.1078 - val_mse: 0.1078\n",
            "Epoch 463/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.1038 - val_mse: 0.1038\n",
            "Epoch 464/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.1060 - val_mse: 0.1060\n",
            "Epoch 465/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.1033 - val_mse: 0.1033\n",
            "Epoch 466/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0891 - mse: 0.0891 - val_loss: 0.1039 - val_mse: 0.1039\n",
            "Epoch 467/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0849 - mse: 0.0849 - val_loss: 0.1002 - val_mse: 0.1002\n",
            "Epoch 468/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.1049 - val_mse: 0.1049\n",
            "Epoch 469/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0902 - mse: 0.0902 - val_loss: 0.1040 - val_mse: 0.1040\n",
            "Epoch 470/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1043 - val_mse: 0.1043\n",
            "Epoch 471/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0855 - mse: 0.0855 - val_loss: 0.1029 - val_mse: 0.1029\n",
            "Epoch 472/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0880 - mse: 0.0880 - val_loss: 0.1009 - val_mse: 0.1009\n",
            "Epoch 473/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0846 - mse: 0.0846 - val_loss: 0.1016 - val_mse: 0.1016\n",
            "Epoch 474/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0847 - mse: 0.0847 - val_loss: 0.1061 - val_mse: 0.1061\n",
            "Epoch 475/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.1022 - val_mse: 0.1022\n",
            "Epoch 476/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0849 - mse: 0.0849 - val_loss: 0.1139 - val_mse: 0.1139\n",
            "Epoch 477/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0875 - mse: 0.0875 - val_loss: 0.1031 - val_mse: 0.1031\n",
            "Epoch 478/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0841 - mse: 0.0841 - val_loss: 0.1014 - val_mse: 0.1014\n",
            "Epoch 479/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0858 - mse: 0.0858 - val_loss: 0.1023 - val_mse: 0.1023\n",
            "Epoch 480/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0849 - mse: 0.0849 - val_loss: 0.1025 - val_mse: 0.1025\n",
            "Epoch 481/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0852 - mse: 0.0852 - val_loss: 0.1030 - val_mse: 0.1030\n",
            "Epoch 482/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0877 - mse: 0.0877 - val_loss: 0.1009 - val_mse: 0.1009\n",
            "Epoch 483/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0879 - mse: 0.0879 - val_loss: 0.1025 - val_mse: 0.1025\n",
            "Epoch 484/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0988 - val_mse: 0.0988\n",
            "Epoch 485/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0862 - mse: 0.0862 - val_loss: 0.1014 - val_mse: 0.1014\n",
            "Epoch 486/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.1052 - val_mse: 0.1052\n",
            "Epoch 487/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0869 - mse: 0.0869 - val_loss: 0.1022 - val_mse: 0.1022\n",
            "Epoch 488/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1013 - val_mse: 0.1013\n",
            "Epoch 489/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0877 - mse: 0.0877 - val_loss: 0.1014 - val_mse: 0.1014\n",
            "Epoch 490/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0854 - mse: 0.0854 - val_loss: 0.1025 - val_mse: 0.1025\n",
            "Epoch 491/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0858 - mse: 0.0858 - val_loss: 0.1033 - val_mse: 0.1033\n",
            "Epoch 492/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0856 - mse: 0.0856 - val_loss: 0.1023 - val_mse: 0.1023\n",
            "Epoch 493/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0848 - mse: 0.0848 - val_loss: 0.1021 - val_mse: 0.1021\n",
            "Epoch 494/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.1001 - val_mse: 0.1001\n",
            "Epoch 495/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0869 - mse: 0.0869 - val_loss: 0.0988 - val_mse: 0.0988\n",
            "Epoch 496/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0871 - mse: 0.0871 - val_loss: 0.1060 - val_mse: 0.1060\n",
            "Epoch 497/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0853 - mse: 0.0853 - val_loss: 0.1029 - val_mse: 0.1029\n",
            "Epoch 498/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.1043 - val_mse: 0.1043\n",
            "Epoch 499/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.1077 - val_mse: 0.1077\n",
            "Epoch 500/500\n",
            "931/931 [==============================] - 3s 3ms/step - loss: 0.0864 - mse: 0.0864 - val_loss: 0.1013 - val_mse: 0.1013\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b4135ae3258c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Evaluate model on training and testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmb_train_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_stand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_stand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mmb_vald_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_stand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_stand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras import Input\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "mb_train_scores = []\n",
        "mb_vald_scores = []\n",
        "\n",
        "# Optimal dropout rate is 0.05\n",
        "mb_model = models.Sequential() # define that we will use the sequential method to build the ANN\n",
        "mb_model.add(Input(shape=(X_train_stand.shape[1],))) # Input layer (has cells equal to number of features)\n",
        "\n",
        "# hidden layers\n",
        "mb_model.add(layers.Dropout(0.05))\n",
        "mb_model.add(layers.Dense(120, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "mb_model.add(layers.Dropout(0.05))\n",
        "mb_model.add(layers.Dense(160, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "mb_model.add(layers.Dropout(0.05))\n",
        "mb_model.add(layers.Dense(120, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "mb_model.add(layers.Dropout(0.05))\n",
        "\n",
        "# output layer\n",
        "mb_model.add(layers.Dense(1,activation = 'linear',kernel_initializer='glorot_normal', bias_initializer='zeros')) # Output Layer\n",
        "mb_model.compile(optimizer = Adam(lr=0.001), loss = 'mse', metrics = ['mse'])\n",
        "mb_model.fit(X_train_stand,y_train_stand, epochs = 500, batch_size = 16,verbose = 1, validation_data=(X_test_stand, y_test_stand))\n",
        "\n",
        "# Evaluate model on training and testing data\n",
        "mb_train_score = model.evaluate(X_train_stand, y_train_stand)\n",
        "mb_vald_score = model.evaluate(X_test_stand, y_test_stand)"
      ],
      "id": "av16tjFYRXik"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAzk3MMzd5dt"
      },
      "outputs": [],
      "source": [
        "mb_model_loss = pd.DataFrame(mb_model.history.history)\n",
        "mb_model_loss.plot()"
      ],
      "id": "nAzk3MMzd5dt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y14Fcy6RSgx0"
      },
      "source": [
        "### Big Brain (bb_mode)"
      ],
      "id": "y14Fcy6RSgx0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r5463FGFSkap"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import Input\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "bb_train_scores = []\n",
        "bb_vald_scores = []\n",
        "\n",
        "# Optimal dropout rate is 0.05\n",
        "bb_model = models.Sequential() # define that we will use the sequential method to build the ANN\n",
        "bb_model.add(Input(shape=(X_train_stand.shape[1],))) # Input layer (has cells equal to number of features)\n",
        "bb_model.add(layers.Dropout(0.05))\n",
        "bb_model.add(layers.Dense(100, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "bb_model.add(layers.Dropout(0.05))\n",
        "bb_model.add(layers.Dense(120, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "bb_model.add(layers.Dropout(0.05))\n",
        "bb_model.add(layers.Dense(140, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "bb_model.add(layers.Dropout(0.05))\n",
        "bb_model.add(layers.Dense(160, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "bb_model.add(layers.Dropout(0.05))\n",
        "bb_model.add(layers.Dense(140, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "bb_model.add(layers.Dropout(0.05))\n",
        "\n",
        "#output layer\n",
        "bb_model.add(layers.Dense(1,activation = 'linear',kernel_initializer='glorot_normal', bias_initializer='zeros')) # Output Layer\n",
        "bb_model.compile(optimizer = Adam(lr=0.001), loss = 'mse', metrics = ['mse'])\n",
        "bb_model.fit(X_train_stand,y_train_stand, epochs = 500, batch_size = 16,verbose = 1, validation_data=(X_test_stand, y_test_stand))\n",
        "\n",
        "# Evaluate model on training and testing data\n",
        "bb_train_score = model.evaluate(X_train_stand, y_train_stand)\n",
        "bb_vald_score = model.evaluate(X_test_stand, y_test_stand)"
      ],
      "id": "r5463FGFSkap"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgQ8QlgceIh7"
      },
      "outputs": [],
      "source": [
        "bb_model_loss = pd.DataFrame(bb_model.history.history)\n",
        "bb_model_loss.plot()"
      ],
      "id": "vgQ8QlgceIh7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij0ccnPXTHpL"
      },
      "source": [
        "### Autoencode Architecture (ae_simple_model)"
      ],
      "id": "ij0ccnPXTHpL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCOUvnLYT3Qb"
      },
      "source": [
        ""
      ],
      "id": "GCOUvnLYT3Qb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3pPv7lf6TJmY"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import Input\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "ae_simple_train_scores = []\n",
        "ae_simple_vald_scores = []\n",
        "\n",
        "# Optimal dropout rate is 0.05\n",
        "ae_simple_model = models.Sequential() # define that we will use the sequential method to build the ANN\n",
        "ae_simple_model.add(Input(shape=(X_train_stand.shape[1],))) # Input layer (has cells equal to number of features)\n",
        "\n",
        "# hidden layers\n",
        "ae_simple_model.add(layers.Dropout(0.05))\n",
        "ae_simple_model.add(layers.Dense(60, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ae_simple_model.add(layers.Dropout(0.05))\n",
        "ae_simple_model.add(layers.Dense(30, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ae_simple_model.add(layers.Dropout(0.05))\n",
        "ae_simple_model.add(layers.Dense(11, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ae_simple_model.add(layers.Dropout(0.05))\n",
        "ae_simple_model.add(layers.Dense(30, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ae_simple_model.add(layers.Dropout(0.05))\n",
        "ae_simple_model.add(layers.Dense(60, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ae_simple_model.add(layers.Dropout(0.05))\n",
        "\n",
        "#output layer\n",
        "ae_simple_model.add(layers.Dense(1,activation = 'linear',kernel_initializer='glorot_normal', bias_initializer='zeros')) # Output Layer\n",
        "ae_simple_model.compile(optimizer = Adam(lr=0.001), loss = 'mse', metrics = ['mse'])\n",
        "ae_simple_model.fit(X_train_stand,y_train_stand, epochs = 500, batch_size = 16,verbose = 1, validation_data=(X_test_stand, y_test_stand))\n",
        "\n",
        "# Evaluate model on training and testing data\n",
        "ae_simple_train_score = model.evaluate(X_train_stand, y_train_stand)\n",
        "ae_simple_vald_score = model.evaluate(X_test_stand, y_test_stand)"
      ],
      "id": "3pPv7lf6TJmY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjwSREGAeM6V"
      },
      "outputs": [],
      "source": [
        "ae_simple_model_loss = pd.DataFrame(ae_simple_model.history.history)\n",
        "ae_simple_model_loss.plot()"
      ],
      "id": "WjwSREGAeM6V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyzw3Q5oT4G-"
      },
      "source": [
        "### Omegabrain (ob_model)"
      ],
      "id": "gyzw3Q5oT4G-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VjoQ8_mT-ZM"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import Input\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "ob_train_scores = []\n",
        "ob_vald_scores = []\n",
        "\n",
        "# Optimal dropout rate is 0.05\n",
        "ob_model = models.Sequential() # define that we will use the sequential method to build the ANN\n",
        "ob_model.add(Input(shape=(X_train_stand.shape[1],))) # Input layer (has cells equal to number of features)\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(100, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(120, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(140, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(160, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(180, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(200, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(220, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(240, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(260, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(240, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(220, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(200, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(180, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(160, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(140, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(120, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(100, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "ob_model.add(layers.Dense(80, activation='relu', kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
        "ob_model.add(layers.Dropout(0.05))\n",
        "\n",
        "\n",
        "#output layer\n",
        "ob_model.add(layers.Dense(1,activation = 'linear',kernel_initializer='glorot_normal', bias_initializer='zeros')) # Output Layer\n",
        "ob_model.compile(optimizer = Adam(lr=0.001), loss = 'mse', metrics = ['mse'])\n",
        "ob_model.fit(X_train_stand,y_train_stand, epochs = 500, batch_size = 16,verbose = 1, validation_data=(X_test_stand, y_test_stand))\n",
        "\n",
        "# Evaluate model on training and testing data\n",
        "ob_train_score = model.evaluate(X_train_stand, y_train_stand)\n",
        "ob_vald_score = model.evaluate(X_test_stand, y_test_stand)"
      ],
      "id": "7VjoQ8_mT-ZM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRoDlgTCVDwB"
      },
      "outputs": [],
      "source": [
        "ob_model_loss = pd.DataFrame(ob_model.history.history)\n",
        "ob_model_loss.plot()"
      ],
      "id": "TRoDlgTCVDwB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEBKRmGdecm8"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "TEBKRmGdecm8"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Superconductor Code.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}